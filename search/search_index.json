{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"document.title = \"Daffy Home\"; [dir=\"ltr\"] .md-sidebar--primary:not([hidden]) ~ .md-content > .md-content__inner { margin-left: 0;} :root >* { --md-default-bg-color: #161616; /* background */ --md-primary-bg-color: #fff; /* Title bar text */ --md-typeset-a-color: #aaa; /* Additional header text */ --md-typeset-color: #fff; /* nav text normal */ --md-accent-fg-color: #392fa4; /* text hover + highlight*/ --md-default-fg-color--lighter: #33f; /* Nav scroll bar */ --md-primary-bg-color--light: #fff; /* Search bar text */ --md-default-fg-color: #fff; /* Search result box section header */ --md-default-fg-color--light: #eee; /* Search box result text */ } .mainPageLeftColumn { width: 20%; } .mainPageRightColumn { width: 80%; } div.md-source-file {color: black; margin-left: 1rem;} Process New User Experienced User Daffy is D eployment A utomation F ramework F or Y ou. A tool to do all the heavy lifting of the OpenShift and IBM Cloud Pak installs. The National Market Top Team created Daffy to assist the technical sales teams with the progression of IBM Cloud Pak opportunities. The goal is to provide the technical sales with a set of (easy to use) scripts that will aid in the installation of OpenShift and the IBM Cloud Pak's. Current Version v2023-03-24a (Release Notes) Fit for purpose Using Daffy IBMers, business partners and customers are onboarded to IBM Cloud Paks in less than a few hours, removing challenges that previously existed when setting up OpenShift. Important Daffy scripts were designed to help pre-sales (CTP/BP) with POC deployments. If you choose to use this in a production environment, you may, but it will be the installer's responsibility to support that installation. IBM can not give support for Daffy itself. As it relates to OpenShift and Cloud pak deployments, you can open a ticket with IBM Support. The installer/business partner would need to verify that the environment meets all HA, best practices, management aspects, and security requirements. As this is a scripting engine, you have full access to the logic/code and have ability to make any changes you feel fit. If you do make any changes to the Daffy engine outside of your cluster environment file, you are on your own, and we will not be able to assist with that environment. Please refer to the Production Deployment Guides for the recommended approach when advising customers on how to deploy a Production Ready Environment. \u00b6","title":"Home"},{"location":"#daffy-scripts-were-designed-to-help-pre-sales-ctpbp-with-poc-deployments-if-you-choose-to-use-this-in-a-production-environment-you-may-but-it-will-be-the-installers-responsibility-to-support-that-installation-ibm-can-not-give-support-for-daffy-itself-as-it-relates-to-openshift-and-cloud-pak-deployments-you-can-open-a-ticket-with-ibm-support-the-installerbusiness-partner-would-need-to-verify-that-the-environment-meets-all-ha-best-practices-management-aspects-and-security-requirements-as-this-is-a-scripting-engine-you-have-full-access-to-the-logiccode-and-have-ability-to-make-any-changes-you-feel-fit-if-you-do-make-any-changes-to-the-daffy-engine-outside-of-your-cluster-environment-file-you-are-on-your-own-and-we-will-not-be-able-to-assist-with-that-environment-please-refer-to-the-production-deployment-guides-for-the-recommended-approach-when-advising-customers-on-how-to-deploy-a-production-ready-environment","text":"","title":"Daffy scripts were designed to help pre-sales (CTP/BP) with POC deployments. If you choose to use this in a production environment, you may, but it will be the installer's responsibility to support that installation. IBM can not give support for Daffy itself. As it relates to OpenShift and Cloud pak deployments, you can open a ticket with IBM Support. The installer/business partner would need to verify that the environment meets all HA, best practices, management aspects, and security requirements. As this is a scripting engine, you have full access to the logic/code and have ability to make any changes you feel fit. If you do make any changes to the Daffy engine outside of your cluster environment file, you are on your own, and we will not be able to assist with that environment. Please refer to the Production Deployment Guides for the recommended approach when advising customers on how to deploy a Production Ready Environment."},{"location":"Markets/","text":"document.title = \"Daffy Markets\"; .market-grid { display: grid; grid-template-columns: repeat(auto-fill, minmax(270px, 1fr)); /* grid-template-rows: 50%; */ /* grid-auto-rows: 1fr; */ grid-gap: 20px; align-items: stretch; } .market-card { border: 1px solid #ccc; box-shadow: 2px 2px 6px 0px rgba(0,0,0,0.3); } .market-grid img { width: 100%; height: 55%; } .market-inner-container { padding: 0 15px 20px; } What Are Market Leads? \u00b6 Market leads are your main contact for Daffy demos, engagement requests, proof of concepts, and troubleshooting. Our team can help with demos and presentations to explain Daffy to customers, business partners, or internal IBMers. We also provide support for upcoming customer proof of concepts that require Daffy personnel. For troubleshooting support, please run /data/daffy/tools.sh --mustGather to gather your logs in a Tar file. Post a message explaining the issue to the #daffy-user-group Slack channel in the Americas Technical Sales workspace and make sure to include @market_lead_name and attach the Tar file. You can find the point of contact for your market in the following list. Alternatively you can click on this link to fill out a request form , and our team will contact you. Financial Market Leads \u00b6 Michael Schapira Senior Data & AI Technical Specialist michael.schapira@us.ibm.com Khory Kotowski Client Technical Specialist: Analytics khory.kotowski@ibm.com Pub Fed Leads \u00b6 Bob Reno Principal Brand Technical Specialist rbreno@us.ibm.com Paul Betts Principal Brand Technical Specialist bettsp@us.ibm.com Canada Leads \u00b6 Rodrigo de la Parra Senior Automation Technical Specialist rodrigo@ca.ibm.com National Market Leads \u00b6 Kyle Dawson Principal Automation Technical Specialist kyle.dawson@us.ibm.com David Krier Principal Automation Technical Specialist dakrier@us.ibm.com Jeff Imholz Principal Automation Technical Specialist jimholz@us.ibm.com","title":"Market Leaders"},{"location":"Markets/#what-are-market-leads","text":"Market leads are your main contact for Daffy demos, engagement requests, proof of concepts, and troubleshooting. Our team can help with demos and presentations to explain Daffy to customers, business partners, or internal IBMers. We also provide support for upcoming customer proof of concepts that require Daffy personnel. For troubleshooting support, please run /data/daffy/tools.sh --mustGather to gather your logs in a Tar file. Post a message explaining the issue to the #daffy-user-group Slack channel in the Americas Technical Sales workspace and make sure to include @market_lead_name and attach the Tar file. You can find the point of contact for your market in the following list. Alternatively you can click on this link to fill out a request form , and our team will contact you.","title":"What Are Market Leads?"},{"location":"Markets/#financial-market-leads","text":"","title":"Financial Market Leads"},{"location":"Markets/#pub-fed-leads","text":"","title":"Pub Fed Leads"},{"location":"Markets/#canada-leads","text":"","title":"Canada Leads"},{"location":"Markets/#national-market-leads","text":"","title":"National Market Leads"},{"location":"Meet-the-Team/","text":"document.title = \"Meet the Team\"; Creators \u00b6 Kyle Dawson Principal Automation Technical Specialist, IBM Technology, US National Market David Krier Principal Integration Technical Specialist, IBM Technology, US National Market Jeff Imholz Principal Automation Technical Specialist, IBM Technology, US National Market Sunil S. Patel Principal Technical Sales Manager, Data-AI-Automation Blackbelt team, IBM Technology, US National Market Daffy the Mascot OpenShift Expert, IBM Technology, US National Market Global Sales - Software Sales Contributors \u00b6 Toby Liu Cloud Engineer, Client Engineering, Global Sales, US National Market Greta Holland Associate AI Applications Technical Specialist, IBM Technology, US National Market Global Sales - Software Sales Taner Avci Brand Technical Sales Specialist Intern Daniel Schroedl Brand Technical Sales Specialist Intern Gold Star Beta Testers \u00b6 Mark Hickok CP4D SME North America Team William James CP4D SME US National Market Dan Kikuchi OpenShift and CP4D SME North America Team Hector Diaz Lopez CPBA SME US Financial Services Market Stuart Jones CPBA SME US Financial Services Market Darren D'Amato CPBA SME US National Market Matt Koscak CP4D SME North America Team","title":"Meet the Team"},{"location":"Meet-the-Team/#creators","text":"","title":"Creators"},{"location":"Meet-the-Team/#contributors","text":"Toby Liu Cloud Engineer, Client Engineering, Global Sales, US National Market Greta Holland Associate AI Applications Technical Specialist, IBM Technology, US National Market Global Sales - Software Sales Taner Avci Brand Technical Sales Specialist Intern Daniel Schroedl Brand Technical Sales Specialist Intern","title":"Contributors"},{"location":"Meet-the-Team/#gold-star-beta-testers","text":"Mark Hickok CP4D SME North America Team William James CP4D SME US National Market Dan Kikuchi OpenShift and CP4D SME North America Team Hector Diaz Lopez CPBA SME US Financial Services Market Stuart Jones CPBA SME US Financial Services Market Darren D'Amato CPBA SME US National Market Matt Koscak CP4D SME North America Team","title":"Gold Star Beta Testers"},{"location":"hidden/","text":"MKDOCS Install & Configuration \u00b6 This site uses MKDocs for the publishing of the \"GitHub Pages\" site You MUST install both MKDocs and the Material Theme. Here is how you setup your machine to edit the documentation hosted on the Github Pages Site. Details of these steps are below. Outline of the steps you will take Install MKdocs Install the MKdocs Theme (Material) Get a Personal Access Token from GitHub Clone this repository Make your changes (Feel free to reach out to Dave Krier or Kyle Dawson for help on how to use MKDocs) Deploy your changes Install MKDocs \u00b6 Here is the installation documentation for MKDocs Install the MKDocs theme Install guidance is on the Material theme page. Material Theme - Quick Start Material Theme Documentation \u00b6 pip3 install mkdocs-material You can check the version of Material you currently have installed with this command. python3 -m pip list If you have any issues with the pip install, you may need to perform an uninstall and reinstall. pip3 uninstall mkdocs-material pip3 install mkdocs-material pip3 uninstall mkdocs pip3 install mkdocs Link to the Material Theme Documentation You need a \"Personal Access Token\" from Github. (This is super simple). \u00b6 Why do I need this? You need this because you need to be able to push changes back into the github repository. Here is a Github doc page you can look at for help if the steps below do not make sense. Login to Github. Click on your user icon in the upper right hand corner Select Settings --> Developer Settings --> Personal Access Tokens Generate a new token with only these prevliages Repo:Status Repo-Deployment Public-Repo Take note of the access key (It's the only time you will see it) Important You will use this Access Token as your password when using the gh-deploy mkdocs command. Clone this repo in Github. \u00b6 I assume you will all know how to do that!! MKDocs User Guide - Modifying the site \u00b6 MKDocs Documentation Deploying your changes. Navigate to your cloned repo directory in a terminal window. You must be inside the /daffydoc directory to run the following commands. Build the files using the build command mkdocs build Deploy the newly built files to the Github Pages site Note This is where you will use the access token from Github. Use the access token instead of your password. mkdocs gh-deploy","title":"MKDOCS Install & Configuration"},{"location":"hidden/#mkdocs-install-configuration","text":"This site uses MKDocs for the publishing of the \"GitHub Pages\" site You MUST install both MKDocs and the Material Theme. Here is how you setup your machine to edit the documentation hosted on the Github Pages Site. Details of these steps are below. Outline of the steps you will take Install MKdocs Install the MKdocs Theme (Material) Get a Personal Access Token from GitHub Clone this repository Make your changes (Feel free to reach out to Dave Krier or Kyle Dawson for help on how to use MKDocs) Deploy your changes","title":"MKDOCS Install &amp; Configuration"},{"location":"hidden/#install-mkdocs","text":"Here is the installation documentation for MKDocs Install the MKDocs theme Install guidance is on the Material theme page. Material Theme - Quick Start","title":"Install MKDocs"},{"location":"hidden/#material-theme-documentation","text":"pip3 install mkdocs-material You can check the version of Material you currently have installed with this command. python3 -m pip list If you have any issues with the pip install, you may need to perform an uninstall and reinstall. pip3 uninstall mkdocs-material pip3 install mkdocs-material pip3 uninstall mkdocs pip3 install mkdocs Link to the Material Theme Documentation","title":"Material Theme Documentation"},{"location":"hidden/#you-need-a-personal-access-token-from-github-this-is-super-simple","text":"Why do I need this? You need this because you need to be able to push changes back into the github repository. Here is a Github doc page you can look at for help if the steps below do not make sense. Login to Github. Click on your user icon in the upper right hand corner Select Settings --> Developer Settings --> Personal Access Tokens Generate a new token with only these prevliages Repo:Status Repo-Deployment Public-Repo Take note of the access key (It's the only time you will see it) Important You will use this Access Token as your password when using the gh-deploy mkdocs command.","title":"You need a \"Personal Access Token\" from Github. (This is super simple)."},{"location":"hidden/#clone-this-repo-in-github","text":"I assume you will all know how to do that!!","title":"Clone this repo in Github."},{"location":"hidden/#mkdocs-user-guide-modifying-the-site","text":"MKDocs Documentation Deploying your changes. Navigate to your cloned repo directory in a terminal window. You must be inside the /daffydoc directory to run the following commands. Build the files using the build command mkdocs build Deploy the newly built files to the Github Pages site Note This is where you will use the access token from Github. Use the access token instead of your password. mkdocs gh-deploy","title":"MKDocs User Guide - Modifying the site"},{"location":"release/","text":"v2023-03-24a \u00b6 Cloud Pak for Business Automation SSL certificates allowed as Alt name *.apps.${CLUSTER}.${BASE_DOMAIN} OpenShift SSL certificates allowed as Alt name *.apps.${CLUSTER}.${BASE_DOMAIN} and api.${CLUSTER}.${BASE_DOMAIN} v2023-03-24 \u00b6 Cloud Pak for Security Removed entire cloud pak Cloud Pak for Business Automation Support for OpenShift 4.12 Removed Support for 22.0.1 Removed Support for OPS CLoud Pak for WAIOPS Removed Support for 3.4.0 and 3.4.1 Added Support for 3.6.1 and 3.6.2 Cloud Pak for Data Removed Support for 4.5.1 and 4.5.2 Cloud Pak for Integration Added Support for OpenShift 4.12 OpenShift Removed Support for 4.8 and 4.9 Misc Added basic info during the setup of Daffy. Install location, version, cert folder, samples, etc Removed daffy fixpak feature Removed support for running daffy on mac desktop v2023-03-09 \u00b6 Tested OpenShift with Current Daffy Release GCP 4.10.36/4.12.2(OCP Only) AWS 4.10.36/4.12.2(OCP Only) Azure 4.10.36/4.12.2(OCP Only) ROKS 4.10.47/4.12.3(OCP Only) VSphere 4.10.36/4.12.2(OCP Only) KVM 4.10.36/4.12.2(OCP Only) IBM IPI 4.10.36/4.12.2(OCP Only) ROSA 4.10.36/4.12.1(OCP Only) ARO 4.10.40 *** Known issue with OCP 4.10.39 and above. Operators fail to install sporadically on all Cloud Paks Turbonomic Added Support 8.7.0 & 8.7.5 Added Support for KubeTurbo to OCP Build Must have TURBO_KUBE_MONITORING=true in the env file along with the KubeTurbo necessary vars Airgap Added new option to mirror OpenShift Catalog as stand only command line flag Instana Added support for Instana agent to be used in all Cloud Paks (must have an instana download key) INSTANA_MONITORING=\"true\" Cloud Pak for Business Automation Added Support for 22.0.2 IF001 and IF002 Added bring your own SSL certificates to CP4BA(cp-console and cpd) Download SSL certificates from IBM Secrets Manager(Optionally) Removed Support for OPS(Open Prediction Service) Cloud Pak for Integration Added bring your own SSL certificates to CP4I Added support for Event Endpoint Management by default when deploying API Connect Support for cluster logging and monitoring Cloud Pak for Data Support for 4.6.1,4.6.2 and 4.6.3 Support for new service AI FactSheets Support for new service Data Replication Added flags for Manta with WKC OpenShift Support for Azure ARO Cluster install Cloud Paks Supported - CP4BA, CP4I and WSA Support for Ingress Certs install *.apps and api URLs. If stored in /data/daffy/certs/${CLUSTER_NAME} Support for OpenShift 4.12 IBM,AWS,Azure,GCP,KVM,VSphere,Rosa,ROKS AWS Added support during cleanup to remove S3 Buckets that were part of cluster (Default this is off AWS_S3_CLEANUP_CLUSTER_BUCKETS_ON_DESTROY=false) IBM IPI added improvements to cluster cleanup(Retry logic and improved timing and geting IBM Cloud support to fix their backend) IBM Support to download Certs from IBM Cloud Secrets Manager and stored in /data/daffy/certs/${CLUSTER_NAME} Db2 Added new precheck for running as root user Misc Added cleanup logic to remove older log files. Default is 30 days. LOG_DIR_RETENTION=30 Blink messages that are long, will now show a count down to how much longer it will wait for certain functions plus total time v2023-01-11 \u00b6 Tested OpenShift with Current Daffy Release GCP 4.8.51 / 4.10.36 AWS 4.8.51 / 4.10.36 Azure 4.8.51 / 4.10.36 ROKS 4.8.51 / 4.10.43 VSphere 4.8.51 / 4.10.36 KVM 4.8.51 / 4.10.36 IBM IPI 4.10.36 ROSA 4.10.36 *** Known issue with OCP 4.10.39 and above. Operators fail to install sporadically on all Cloud Paks Cloud Pak for Integration Added Support for 2022.4.1 WebSphere Automation Added Support for 1.5 Cloud Pak for Business Automation Added Support for 22.0.2 Cloud Pak for Watson AIOPS Added support for 3.5.1 Added support for 3.6.0 Removed support for all 3.3.x versions OpenShift Support for AWS Rosa(Install OCP and Cloud Paks) Support for EFS for storage(will setup EFS Storage on AWS/ROSA) -Build Cluster and CP4I and CP4BA support Create EFS Provider and Setup in Cluster Bug fix - Proxy install on RHEL, convert NO_PROXY to lower case no_proxy. Lower case works on both Ubuntu and RHEL VSphere New Flag to skip building vsphere folders. If large VSphere network, the checking of folders can take a long time (10 min) Default : VSPHERE_CREATE_FOLDERS=\"true\" Support for full path of Network Path VSPHERE_NETWORK1=\"itzna-itz-wdc04-private/itz-550005mqws-arcqrk30-segment\" AirGap Support for OCP 4.9 and 4.10 and ODF(Support from dwakeman@us.ibm.com) OpenShift Data Foundation(ODF) Changed core logic for ODF to build new machine sets for Infra Storage nodes Separate from standard worker machine sets aws-ipi, vsphere-ipi, azure-ipi, ibm-ipi, gcp-ipi v2022-12-02a \u00b6 Cloud Pak for Business Automation Added Support for 22.0.1 IF005 Added new Service docprocessing(Automation Document Processing - ADP) Cloud Pak for Data Updated default value for CRIO PIDS tunning to 16384 Bug fix for SPSS and WS - version flag change Misc New logic to install podman on Ubuntu - Issue with opensuse.org cert (Solution from green@techd.com) v2022-12-02 \u00b6 Tested OpenShift with Current Daffy Release GCP 4.8.51 / 4.10.39 AWS 4.8.51 / 4.10.39 Azure 4.8.51 / 4.10.39 ROKS 4.8.51 / 4.10.39 VSphere 4.8.51 / 4.10.39 KVM 4.8.51 / 4.10.39 IBM IPI 4.10.39 Airgap Ability to build local repo on internet facing bastion, Loads openshift catalog into local Repository Cloud Pak for Data Support for 4.6.0 Added Watson Pipelines (Only supported 4.6.0) Websphere Automation Added support for NFS Storge defaults AppStore Added CP4D Backup and Restore(DR Usecase) TechZone Tiles Cloud Pak for Data Added support for CP4D 4.6.0 Added Watson Pipelines Added Watson OpenScale v2022-11-10b \u00b6 Airgap Bug fix for testing internet access when firewall blocks but does not drop connections Replace podman default auth file (/run/user/0/containers/auth.json) with new podman command override REGISTRY_AUTH_FILE Ability to build local repo on internet facing bastion, no need to export and move to airgap mirror-registry precheck - tool requires hostname be in /etc/hosts or mirror-registry crashes OpenShift Updated versions for CoreOS(VSphere Airgap) and logic to download correct versions WebSpher Automation Bug fix - missing step to prepare input files is service.sh(step 3) AppStore Added new CP4D Backup and Restore(Use case for DR) v2022-11-10a \u00b6 Cloud Pak for Data Added Watson OpenScale capability Misc Cleanup of logs for services v2022-11-10 \u00b6 Tested OpenShift with Current Daffy Release GCP 4.8.51 / 4.10.36 AWS 4.8.51 / 4.10.36 Azure 4.8.51 / 4.10.36 ROKS 4.8.51 / 4.10.36 VSphere 4.8.51 / 4.10.36 KVM 4.8.51 / 4.10.36 IBM IPI 4.10.36 Cloud Pak for Business Automation Added Support for 22.0.1 IF004 Added ADS to Production Decisions Service and Starter Services decisions and content-decisions Added support for Roks vpn-gen2 for step 2 and step 3(Cloud Pak and Services) RPA Added support for ROKS Cloud Pak for Data Removed Storage Vendor tag (not needed for 4.5.x) Cloud Pak for Integration Fixed bug with APIC that would fail when apply CR if webhook wasn't finalized Updated for latest releases of 2022.4.1 services OpenShift Added new precheck for VSPHERE_USERNAME, it cannot contain \\ Misc Added new Override for KVM Build options (KVM_VM_OPTION) and new default is to not enable VNC Added new Override for Airgap (OCP_REGISTRY_IMAGE_EXPORT_FILE) to enable export of registry. Default is true Added better error message for ROKS when logged into wrong IBM Cloud Account Updated refresh.sh to allow menu choice for version v2022-10-18 \u00b6 Tested OpenShift with Current Daffy Release GCP 4.8.49 / 4.10.32 AWS 4.8.49 / 4.10.32 Azure 4.8.49 / 4.10.32 ROKS 4.8.49 / 4.10.32 VSphere 4.8.49 / 4.10.32 KVM 4.8.49 / 4.10.32 IBM IPI 4.10.32 Cloud Pak for Business Automation Added Support for 22.0.1 IF003 Removed Support for 21.0.3 and all IFIXs Removed Support for 22.0.1 without IFIX Added Logic for Production Decisions to automate the LDAP addition to Zen RPA Added support for 21.0.4 and 21.0.5 Removed support for 21.0.2 and 21.0.3 Cloud Pak for Data Added NEW CP4DCTL Install to Tools Support for 4.5.3 (ROKS issue fixed with WKC) Removed support for 4.0.x Cloud Pak for WAIOps Added support for V3.5 AppStore Created new /data/daffy/appstore.sh to install appstore utilities New Daffy CLI environment configurator Misc Updated all samples to have false value and removed the comment character Added new Deployment Type - PostSale via DAFFY_DEPLOYMENT_TYPE Added install of openshift-install via tools.sh Fixed bug with image registry route for vsphere-ipi v2022-09-29 \u00b6 Tested OpenShift with Current Daffy Release GCP 4.8.49 / 4.10.32 AWS 4.8.49 / 4.10.32 Azure 4.8.49 / 4.10.32 ROKS 4.8.49 / 4.10.32 VSphere 4.8.49 / 4.10.32 KVM 4.8.49 / 4.10.32 IBM IPI 4.10.32 Cloud Pak for Business Automation Added Support for 22.0.1 IF002 Added Support for Decision Production Service(Step 3 - Build DB and LDAP assets and Deploy Services) Added new starter services all (removed old samples) OpenShift Added support for all IPI installs to enable Masters true (OCP_MASTER_NODES_SCHEDULABLE=true) Added support to auto login to IBM Account for ROKS install (IBMCLOUD_ACCOUNT_ID=\"\") Added ability to increase wait time for VSPhere UPI reboot for (VSPHERE_IGNITION_FILES_DEPLOYMENT_WAIT_TIME=\"500\") Added Support to override VSphere UPI network device (VSPHERE_NETWORK_ADAPTER=\"\") Added Red Hat pull secret validation precheck Upgraded RHACM to 2.6 Upgraded Mirror Registry to 1.2.6 Added support for 4.11 Added precheck to IBM ipi for CIS instance and domain name Added support for 4.11(OCP only) Added support for Manual credentials mode on GCP & AWS Added prechecks for bring your own VPC for GCP, AWS, & Azure Cloud Pak for Data Added support for 4.5.2 Removed support for 4.0.2-4.0.5 Cloud Pak for Integration Removed support for 2021.3.1 & 2021.2.1 Cloud Pak for WAIOps Added support for 3.4.1 & 3.4.2 Added deployment of Infrastructure Automation component (CP4WAIOPS_DEPLOY_IA=true) Reconfigured scripts to align with Daffy deployment pattern. Infrastructure Automation and Event Manager will be installed with the service.sh script. AI Manager gets deployed with the base of the cloud pak. IBM IPI Added precheck CIS and DNS domain Added VPC quota precheck that builds VPC, subnet, and instances to test for quota limits (Can be toggled off IBM_ALLOW_VPC_PRECHECK=false) AppStore New AppStore feature New App - IBM Sterling B2B Install tool Misc Remove support for OCP 4.6 and 4.7 - https://access.redhat.com/support/policy/updates/openshift Updated IDS LDAP install and --console info to include search filters on output During cleanup for ROKS, logout from ibmcloud Added ROKS and ibm-ipi to security-cleanup tool Added OpenShift Ceph Tools into any installation of OCS/ODF. Added install of dos2unix util for prepare host Removed install of nmon for prepare host Added ROKS and IBM IPI to security-cleanup.sh Added OpenShift Ceph Tools into any installation of OCS/ODF Updated CloudCTL and added support to install stand alone via tools.sh v2022-08-17b \u00b6 Cloud Pak for Integration Fixed Daffy code issue with Platform Navigator not finishing operator install v2022-08-17a \u00b6 Cloud Pak for Data Fixed issue with operators for services stuck in Upgrade Pending (Known CP4D issue with release of 4.5.2, breaking previous releases of 4.5.0 and 4.5.1. Would occur with Daffy, CP4D-cli, or running manually) v2022-08-17 \u00b6 Tested OpenShift with Current Daffy Release GCP 4.8.46 / 4.10.22 AWS 4.8.46 / 4.10.22 Azure 4.8.46 / 4.10.22 ROKS 4.8.46 / 4.10.22 VSphere 4.8.46 / 4.10.22 KVM 4.8.46 / 4.10.22 IBM IPI 4.10.22 Cloud Pak for Business Automation Added Support for 22.0.1 IF001 Added Support for Decision Production Service(Step 2 - Namespace and base Operators) Added Support for Content Production Service(Step 2 - Namespace and base Operators) Added Support for Workflow Production Service(Step 2 - Namespace and base Operators) Addd New RPA Server MSSQL Server project and Database configured OpenLDAP for RPA configured Added 21.0.3 Added 21.0.2 IF005 Cloud Pak for Integration Add support for each service to be in its own namespace Cloud Pak for Data Added Support for 4.5.0 and 4.5.1 Added Support to specify different storage class for each service Added new service Match 360 Added new service Open OpenPages Added new service Analytics Engine powered by Apache Spark Added new service Db2 Warehouse Added new service Data Privacy Added new service Cognos Analytics Added new service Db2 OLTP Turbonomics Added Platform Operator Install Added Kubeturbo Operator Metrics Collector Watsion WAIOps Added support for 3.4.0 IBM IPI Added support for new platform type of IBM Cloud (ibm-ipi) OCP 4.10+ Misc Added ability to install older version of daffy with refresh.sh (--list) Fixed bug for KVM precheck on Host that has Memory > 1 TB Added precheck to validate the number of cluster nodes matches the number of nodes in environment file v2022-07-14 \u00b6 Tested OpenShift with Current Daffy Release GCP 4.8.42 / 4.10.17 AWS 4.8.42 / 4.10.17 Azure 4.8.42 / 4.10.17 ROKS 4.8.42 / 4.10.17 VSphere 4.8.42 / 4.10.17 KVM 4.8.42 / 4.10.17 Cloud Pak for Business Automation Added 21.0.3 IF008 and IF009 Added 22.0.1 New Samples ocp-starter-ocs-all-22.0.1 and roks-starter-ibm-all-22.0.1 Cloud Pak for Integration Updated to install via single operator per product Added support for 2022.2.1 Cloud Pak for Data Added 4.0.8 and 4.09 WebSphere Automation Added support for v1.4 Cloud Pak for Security Added support for v1.10 OpenShift Added ability to enable masters as worker nodes (UPI Only) - OCP_MASTER_NODES_SCHEDULABLE=true(Default is false) Added support for mirror-registry to support airgap and building of own registry(Quay mirror-registry) Added support Red Hat Advanced Cluster Management (RHACM) Added support for ODF overview page in OpenShift Console Misc Added new icons to output messages to help users with types of messages being displayed Added new logging to capture all info displayed to console for each process and list log location Added support to create new disk for NFS Added precheck to validate cluster version matches environment file OCP version Added precheck for OCS/ODF to make sure that your cluster has at least 6 workers Added precheck for Cluster Name to make sure that it matches environment file value Fixed bug to catch errors and exit when creating ROKS cluster Fixed bug with OS check, will exit now if not running on support platform Fixed bug with OCP 4.10 and Local registry mirrors (dkikuchi@us.ibm.com) Fixed bug with CP4BA using wrong variable CP_DEPLOYMENT_PLATFORM should be CP4BA_AUTO_PLATFORM (Toby.Liu@ibm.com) Fixed order of OCP installs as it relates to Airgap and Local Registry Auth Info (gmarcy@us.ibm.com) v2022-05-23 \u00b6 Tested OpenShift with Current Daffy Release GCP 4.8.40 AWS 4.8.40 Azure 4.8.40 ROKS 4.8.36 VSphere 4.8.40 KVM 4.8.40 OpenShift Added support for AWS Zone overrides(1,2 or 3) Added support for AWS KMS keys for disk encryption (via AWS_ENABLE_KMS_KEY=true) Added support for AWS to skip the AdministratorAccess precheck Added AWS sample security policies to use if not allowing AdministratorAccess policy VSphere Added support for VSphere Data Center with more then one VSphere Cluster Added ability to pause VSphere Install prior deploying ignition files (OCP_DEPLOY_ALL_IGNTIION_FILES_PAUSE=true) Cloud Pak for Security Added new cloud pak and Version 1.9 Cloud Pak for Business Automation Starter Services workflow - workflow-workstreams,bai,baml,baw_authoring,case,content_integration,pfs,workstreams Added IF008 support Added support for basic sample CP4BA CR's. List, Status, Console support at basic non dynamic way Added support for Open Prediction Service HUB(OPS) Added support for OpenShift 4.9 and 4.10 Cloud Pak for Data Renamed Service Cognos to Cognos Dashboards Cloud Pak for WAIOps Added support for version 3.3 Added AI Manager 3.3 Added Event Manager 3.3 IBM TechZone flag to help with using ROKS Cluster provisioned by TechZone (ROKS_PROVIDER=techzone) Misc Added precheck for DNS PTR reverse lookup for UPI install with (DNSMASQ_BUILD=false) **Tech Preview - Added support for Macbook bastion (Except vsphere-*, kvm-upi, db2 and ldap) Added new tools process to install supporting tools outside of the daffy process. (oc,aws,gcloud,cloudctl,mustgather etc) Added new mustgather process Add new Variables to help track usage of daffy tool Bug Added logic to check for errors during prepareHost step Fixed logic to test exact name for storage class, not partial name check Fixed order logic to approveVCenterCert during precheck Fixed channel name for CP4D Scheduling Operator based on release v2022-04-20c \u00b6 OpenShift Added support for AWS to skip the AdministratorAccess precheck Added AWS sample security policies to use if not allowing AdministratorAccess policy Skip DNS check for AWS Publish Internal Support AWS OCP Internal Publish v2022-04-20b \u00b6 OpenShift Support Azure OCP Internal Publish Cloud Pak for Data Support for 4.0.8 (cloudctl and Single Catalog) v2022-04-20a \u00b6 OpenShift Support ability to Skip HAProxy Build Support ability to Skip DNSMasq Build Misc Support Proxy for bastion host v2022-04-20 \u00b6 Tested OpenShift with Current Daffy Release GCP 4.8.36 AWS 4.8.36 Azure 4.8.36 ROKS 4.8.35 VSphere 4.8.36 KVM 4.8.36 Cloud Pak for Business Automation Added IF007 support Added support to install DB2 on Ubuntu/Redhat Linux Server Added support to install IDS(LDAP) on RedHat Linux Server Cloud Pak for Data Precheck and only support OCP 4.8 for CPD 4.0.X Fixed DV bug on ROKS - New Db2 Tuning added(With Warning) Support for 4.0.7 (cloudctl not working only Single Catalog) Added new service Cognos Dashboard Added new service DB2 Data Management Console WebSphere Automation Added support for version 1.3 Added an environment file for wsa VSphere Updated ISO images to latest version for each base version Added better error message for UPI install and Missing ISO Image OpenShift Updated TShirt Size output to allow copy/paste to simplify overrides Added support to install custom openshift-install program (TechZone request) Added VSphere IPI support to specify resource pool from custom openshift-install program (TechZone request) Added support for 4.10 (Note: Currently only CP4I supports 4.10) Added support for OpenShift Data Foundation(ODF) for OCP 4.10 only Misc Added support for RHEL 8.X bastion (Except UPI) Added new Security Cleanup script Added new version script in the root of daffy Added support for the all-in-one command to skip Cloud Pak install Enhanced Secure MQ demo and documentation Bug Fixed master cleaup.sh if user passed wrong env file, it would then do rebuild not cleanup. VSphere UPI install was broken with worker7-9 logic and haproxy and dnsmasq template files. Fixed daffy-init.sh to check for root user and exit if not root v2022-03-31 \u00b6 Tested OpenShift with Current Daffy Release GCP 4.8.31 AWS 4.8.31 Azure 4.8.31 ROKS 4.8.31 VSphere 4.8.31 KVM 4.8.31 Cloud Pak for Business Automation Added IF005 support Cloud Pak for Data Added support for Cloud Pak for Data 4.0.6 Added install for python3 and pip3 for >= 4.0.6 Added build/remove of portworx storage classes Added support for WKC, WML and WS on ROKS Added new service Decision Optimization(DODS) Added new service DataStage Updated cloudctl install only supports latest version of CP4D Cloud Pak for Integration Added new single MQ instance Added new App Connect Designer instance Added new API Connect instance Added new App Connect Dashboard instance Added new Operations Dashboard tracing instance Added new Event Streams instance Added new HA MQ instance Added new Asset Repository instance WebSphere Automation (WSA) Added new component that supports install of WebSphere Automation OpenShift Added support for Proxy Install Added new root level cleanup.sh, that calls the ocp cleanup.sh Added confirmation during cleanup.sh of OCP Added logic to not restart ROKS nodes multiple times if doing more than one service at different times Added support for KVM UPI to support up to 9 worker nodes Misc Added logic to support user overrides for cloud Pak components versions(All Supported Cloud Paks) Added support for AWS custom Subnets and AMI ID overrides in OCP install Added new confirmation of warranty during Daffy install v2022-02-15b \u00b6 Cloud Pak for Business Automation Switched from Patterns to Services terminology Added logic to support user overrides for cloud Pak components versions v2022-02-15a \u00b6 Automated ROKS oc login step validated cluster name to be lower case and alphanumeric v2022-02-15 \u00b6 Tested OpenShift with Current Daffy Release GCP 4.8.26 AWS 4.8.26 Azure 4.8.26 ROKS 4.8.26 VSphere 4.8.26 KVM 4.8.26 Added Cloud Pak Cloud Pak for Business Automation 21.0.3 Starter Services content - filenet,cmis,ier,tm,bai decisions - odm,bai content-decisions - filenet,cmis,ier,tm,odm,bai Added new OCP_INSTALL_TYPE msp = Managed Service Provider(roks-msp) Added ROKS Support Build and Cleanup of ROKS Cluster Install Cloud Paks on existing Cluster(Daffy did not build ROKS Cluster) Supports Classic only Added support for Cloud Pak for Data 4.0.5 Added new consoleFooter Function Added logic to support local cert folder outside of Daffy and not to remove on refresh command Updated refresh logic to display current version and new version during execution Updated command line options to support multiple case options(--precheck|--Precheck, etc) Updated default Storage Class and Vendor logic to support roks via env.sh files Updated all Cloud Paks and Service install precheck to use prepare host function Updated cp4waiops logic, renamed and removed old preinstall logic Updated sample env files to new testing versions of Openshift 4.8.26 Updated imageContentSources.yaml for AirGap install to list more standard mirrors Bug Fixes VSphere OCP install-config.yaml - added quotes around cluster and network values Before OCP Install cluster, remove ocp-install dir to remove old trans files Fixed Typos and Spelling errors Hide errors when sourcing .profile if it does not exist v2022-01-18 \u00b6 Added Cloud Pak Cloud Pak for WAIOps Multi-Cloud Pak in single Cluster Added Support for Case Install Cloud Pak for Data Cloud Pak for Data Services Added support for Cloud Pak for Data 4.0.3 and 4.0.4 Added support for Cloud Pak for Data services operations tools Added support for Cloud Pak for Integration 2021.4.1 Added Cloud Pak for Data services Statistical Package for the Social Sciences(SPSS) Watson Machine Learning(WML) Added Support for Bastion on Windows WLS2(Ubuntu 20.04) Added Daffy version to all status commands Added support for VSphere restricted folder install Added support for Image Registry for UPI install of OCP Added support to allow full install of OpenShift and Cloud Pak from single command(all-in-one) Updated CP4D New Namespace was added and all CP4D services/operands will be deployed in separate/new namespace Updated CP4D Services Added more info to service status output Added new single status command for all Daffy Supported Services Added ability to install more then once service at a time Added ability to remove service via cleanup function for each service Cleaned Up logging Moved more logs from tmp to log folder Displayed more log names and location in output Bug Fixes VSphere install now allows for special character passwords(dalmeter@us.ibm.com) Nodes were mislabeled for OpenShift Container Storage Taint added to OpenShift Nodes for OpenShift Container Storage Local Volumes for UPI install now have tolerance for OpenShift Container Storage nodes Added Overwrite Flag for 99-worker-cp4d-crio-conf to allow for node expansions with IPI install(dkikuchi@us.ibm.com) GCP removed unused roles required for precheck Corrected CP4D version(dkikuchi@us.ibm.com) Data Virtualization was looking for hard coded version and would not install post 4.0.2 KVM IP precheck only checked that IP given was part of local not not full IP Removed Features Removed support for OpenShift 4.7 v2021-12-01 \u00b6 Added Cloud Pak (CP4I) Cloud Pak for Integration 2021.3.1 and 2021.2.1 Added Platform Support AWS(aws) - IPI Added OpenShift Support for 4.9 Added pre check logic to validate base domain and cluster name to verify valid DNS Syntax (FQDN) Added support - Subdomain for base domain on KVM and VSphere Installs Added support for KVM Web Dashboard - vmdashboard Updated OpenShift Container Storage for IPI to use provider Storage Class IPI Installs now have easy disk expansion from Console for OCS Updated OCS to label all Storage nodes as Infrastructure(Infra) nodes Update Daffy Stats URL and new arguments v2021-11-12 - Initial Release \u00b6 OpenShift Supported 4.6,4.7 and 4.8 Cloud Pak for Data Supported 4.01 OpenShift Container Storage Supported Platforms Supported Google Cloud Provider(gcp) - IPI Azure(az) - IPI VSphere(vsphere) - IPI and UPI KVM(kvm) - UPI","title":"Release"},{"location":"release/#v2023-03-24a","text":"Cloud Pak for Business Automation SSL certificates allowed as Alt name *.apps.${CLUSTER}.${BASE_DOMAIN} OpenShift SSL certificates allowed as Alt name *.apps.${CLUSTER}.${BASE_DOMAIN} and api.${CLUSTER}.${BASE_DOMAIN}","title":"v2023-03-24a"},{"location":"release/#v2023-03-24","text":"Cloud Pak for Security Removed entire cloud pak Cloud Pak for Business Automation Support for OpenShift 4.12 Removed Support for 22.0.1 Removed Support for OPS CLoud Pak for WAIOPS Removed Support for 3.4.0 and 3.4.1 Added Support for 3.6.1 and 3.6.2 Cloud Pak for Data Removed Support for 4.5.1 and 4.5.2 Cloud Pak for Integration Added Support for OpenShift 4.12 OpenShift Removed Support for 4.8 and 4.9 Misc Added basic info during the setup of Daffy. Install location, version, cert folder, samples, etc Removed daffy fixpak feature Removed support for running daffy on mac desktop","title":"v2023-03-24"},{"location":"release/#v2023-03-09","text":"Tested OpenShift with Current Daffy Release GCP 4.10.36/4.12.2(OCP Only) AWS 4.10.36/4.12.2(OCP Only) Azure 4.10.36/4.12.2(OCP Only) ROKS 4.10.47/4.12.3(OCP Only) VSphere 4.10.36/4.12.2(OCP Only) KVM 4.10.36/4.12.2(OCP Only) IBM IPI 4.10.36/4.12.2(OCP Only) ROSA 4.10.36/4.12.1(OCP Only) ARO 4.10.40 *** Known issue with OCP 4.10.39 and above. Operators fail to install sporadically on all Cloud Paks Turbonomic Added Support 8.7.0 & 8.7.5 Added Support for KubeTurbo to OCP Build Must have TURBO_KUBE_MONITORING=true in the env file along with the KubeTurbo necessary vars Airgap Added new option to mirror OpenShift Catalog as stand only command line flag Instana Added support for Instana agent to be used in all Cloud Paks (must have an instana download key) INSTANA_MONITORING=\"true\" Cloud Pak for Business Automation Added Support for 22.0.2 IF001 and IF002 Added bring your own SSL certificates to CP4BA(cp-console and cpd) Download SSL certificates from IBM Secrets Manager(Optionally) Removed Support for OPS(Open Prediction Service) Cloud Pak for Integration Added bring your own SSL certificates to CP4I Added support for Event Endpoint Management by default when deploying API Connect Support for cluster logging and monitoring Cloud Pak for Data Support for 4.6.1,4.6.2 and 4.6.3 Support for new service AI FactSheets Support for new service Data Replication Added flags for Manta with WKC OpenShift Support for Azure ARO Cluster install Cloud Paks Supported - CP4BA, CP4I and WSA Support for Ingress Certs install *.apps and api URLs. If stored in /data/daffy/certs/${CLUSTER_NAME} Support for OpenShift 4.12 IBM,AWS,Azure,GCP,KVM,VSphere,Rosa,ROKS AWS Added support during cleanup to remove S3 Buckets that were part of cluster (Default this is off AWS_S3_CLEANUP_CLUSTER_BUCKETS_ON_DESTROY=false) IBM IPI added improvements to cluster cleanup(Retry logic and improved timing and geting IBM Cloud support to fix their backend) IBM Support to download Certs from IBM Cloud Secrets Manager and stored in /data/daffy/certs/${CLUSTER_NAME} Db2 Added new precheck for running as root user Misc Added cleanup logic to remove older log files. Default is 30 days. LOG_DIR_RETENTION=30 Blink messages that are long, will now show a count down to how much longer it will wait for certain functions plus total time","title":"v2023-03-09"},{"location":"release/#v2023-01-11","text":"Tested OpenShift with Current Daffy Release GCP 4.8.51 / 4.10.36 AWS 4.8.51 / 4.10.36 Azure 4.8.51 / 4.10.36 ROKS 4.8.51 / 4.10.43 VSphere 4.8.51 / 4.10.36 KVM 4.8.51 / 4.10.36 IBM IPI 4.10.36 ROSA 4.10.36 *** Known issue with OCP 4.10.39 and above. Operators fail to install sporadically on all Cloud Paks Cloud Pak for Integration Added Support for 2022.4.1 WebSphere Automation Added Support for 1.5 Cloud Pak for Business Automation Added Support for 22.0.2 Cloud Pak for Watson AIOPS Added support for 3.5.1 Added support for 3.6.0 Removed support for all 3.3.x versions OpenShift Support for AWS Rosa(Install OCP and Cloud Paks) Support for EFS for storage(will setup EFS Storage on AWS/ROSA) -Build Cluster and CP4I and CP4BA support Create EFS Provider and Setup in Cluster Bug fix - Proxy install on RHEL, convert NO_PROXY to lower case no_proxy. Lower case works on both Ubuntu and RHEL VSphere New Flag to skip building vsphere folders. If large VSphere network, the checking of folders can take a long time (10 min) Default : VSPHERE_CREATE_FOLDERS=\"true\" Support for full path of Network Path VSPHERE_NETWORK1=\"itzna-itz-wdc04-private/itz-550005mqws-arcqrk30-segment\" AirGap Support for OCP 4.9 and 4.10 and ODF(Support from dwakeman@us.ibm.com) OpenShift Data Foundation(ODF) Changed core logic for ODF to build new machine sets for Infra Storage nodes Separate from standard worker machine sets aws-ipi, vsphere-ipi, azure-ipi, ibm-ipi, gcp-ipi","title":"v2023-01-11"},{"location":"release/#v2022-12-02a","text":"Cloud Pak for Business Automation Added Support for 22.0.1 IF005 Added new Service docprocessing(Automation Document Processing - ADP) Cloud Pak for Data Updated default value for CRIO PIDS tunning to 16384 Bug fix for SPSS and WS - version flag change Misc New logic to install podman on Ubuntu - Issue with opensuse.org cert (Solution from green@techd.com)","title":"v2022-12-02a"},{"location":"release/#v2022-12-02","text":"Tested OpenShift with Current Daffy Release GCP 4.8.51 / 4.10.39 AWS 4.8.51 / 4.10.39 Azure 4.8.51 / 4.10.39 ROKS 4.8.51 / 4.10.39 VSphere 4.8.51 / 4.10.39 KVM 4.8.51 / 4.10.39 IBM IPI 4.10.39 Airgap Ability to build local repo on internet facing bastion, Loads openshift catalog into local Repository Cloud Pak for Data Support for 4.6.0 Added Watson Pipelines (Only supported 4.6.0) Websphere Automation Added support for NFS Storge defaults AppStore Added CP4D Backup and Restore(DR Usecase) TechZone Tiles Cloud Pak for Data Added support for CP4D 4.6.0 Added Watson Pipelines Added Watson OpenScale","title":"v2022-12-02"},{"location":"release/#v2022-11-10b","text":"Airgap Bug fix for testing internet access when firewall blocks but does not drop connections Replace podman default auth file (/run/user/0/containers/auth.json) with new podman command override REGISTRY_AUTH_FILE Ability to build local repo on internet facing bastion, no need to export and move to airgap mirror-registry precheck - tool requires hostname be in /etc/hosts or mirror-registry crashes OpenShift Updated versions for CoreOS(VSphere Airgap) and logic to download correct versions WebSpher Automation Bug fix - missing step to prepare input files is service.sh(step 3) AppStore Added new CP4D Backup and Restore(Use case for DR)","title":"v2022-11-10b"},{"location":"release/#v2022-11-10a","text":"Cloud Pak for Data Added Watson OpenScale capability Misc Cleanup of logs for services","title":"v2022-11-10a"},{"location":"release/#v2022-11-10","text":"Tested OpenShift with Current Daffy Release GCP 4.8.51 / 4.10.36 AWS 4.8.51 / 4.10.36 Azure 4.8.51 / 4.10.36 ROKS 4.8.51 / 4.10.36 VSphere 4.8.51 / 4.10.36 KVM 4.8.51 / 4.10.36 IBM IPI 4.10.36 Cloud Pak for Business Automation Added Support for 22.0.1 IF004 Added ADS to Production Decisions Service and Starter Services decisions and content-decisions Added support for Roks vpn-gen2 for step 2 and step 3(Cloud Pak and Services) RPA Added support for ROKS Cloud Pak for Data Removed Storage Vendor tag (not needed for 4.5.x) Cloud Pak for Integration Fixed bug with APIC that would fail when apply CR if webhook wasn't finalized Updated for latest releases of 2022.4.1 services OpenShift Added new precheck for VSPHERE_USERNAME, it cannot contain \\ Misc Added new Override for KVM Build options (KVM_VM_OPTION) and new default is to not enable VNC Added new Override for Airgap (OCP_REGISTRY_IMAGE_EXPORT_FILE) to enable export of registry. Default is true Added better error message for ROKS when logged into wrong IBM Cloud Account Updated refresh.sh to allow menu choice for version","title":"v2022-11-10"},{"location":"release/#v2022-10-18","text":"Tested OpenShift with Current Daffy Release GCP 4.8.49 / 4.10.32 AWS 4.8.49 / 4.10.32 Azure 4.8.49 / 4.10.32 ROKS 4.8.49 / 4.10.32 VSphere 4.8.49 / 4.10.32 KVM 4.8.49 / 4.10.32 IBM IPI 4.10.32 Cloud Pak for Business Automation Added Support for 22.0.1 IF003 Removed Support for 21.0.3 and all IFIXs Removed Support for 22.0.1 without IFIX Added Logic for Production Decisions to automate the LDAP addition to Zen RPA Added support for 21.0.4 and 21.0.5 Removed support for 21.0.2 and 21.0.3 Cloud Pak for Data Added NEW CP4DCTL Install to Tools Support for 4.5.3 (ROKS issue fixed with WKC) Removed support for 4.0.x Cloud Pak for WAIOps Added support for V3.5 AppStore Created new /data/daffy/appstore.sh to install appstore utilities New Daffy CLI environment configurator Misc Updated all samples to have false value and removed the comment character Added new Deployment Type - PostSale via DAFFY_DEPLOYMENT_TYPE Added install of openshift-install via tools.sh Fixed bug with image registry route for vsphere-ipi","title":"v2022-10-18"},{"location":"release/#v2022-09-29","text":"Tested OpenShift with Current Daffy Release GCP 4.8.49 / 4.10.32 AWS 4.8.49 / 4.10.32 Azure 4.8.49 / 4.10.32 ROKS 4.8.49 / 4.10.32 VSphere 4.8.49 / 4.10.32 KVM 4.8.49 / 4.10.32 IBM IPI 4.10.32 Cloud Pak for Business Automation Added Support for 22.0.1 IF002 Added Support for Decision Production Service(Step 3 - Build DB and LDAP assets and Deploy Services) Added new starter services all (removed old samples) OpenShift Added support for all IPI installs to enable Masters true (OCP_MASTER_NODES_SCHEDULABLE=true) Added support to auto login to IBM Account for ROKS install (IBMCLOUD_ACCOUNT_ID=\"\") Added ability to increase wait time for VSPhere UPI reboot for (VSPHERE_IGNITION_FILES_DEPLOYMENT_WAIT_TIME=\"500\") Added Support to override VSphere UPI network device (VSPHERE_NETWORK_ADAPTER=\"\") Added Red Hat pull secret validation precheck Upgraded RHACM to 2.6 Upgraded Mirror Registry to 1.2.6 Added support for 4.11 Added precheck to IBM ipi for CIS instance and domain name Added support for 4.11(OCP only) Added support for Manual credentials mode on GCP & AWS Added prechecks for bring your own VPC for GCP, AWS, & Azure Cloud Pak for Data Added support for 4.5.2 Removed support for 4.0.2-4.0.5 Cloud Pak for Integration Removed support for 2021.3.1 & 2021.2.1 Cloud Pak for WAIOps Added support for 3.4.1 & 3.4.2 Added deployment of Infrastructure Automation component (CP4WAIOPS_DEPLOY_IA=true) Reconfigured scripts to align with Daffy deployment pattern. Infrastructure Automation and Event Manager will be installed with the service.sh script. AI Manager gets deployed with the base of the cloud pak. IBM IPI Added precheck CIS and DNS domain Added VPC quota precheck that builds VPC, subnet, and instances to test for quota limits (Can be toggled off IBM_ALLOW_VPC_PRECHECK=false) AppStore New AppStore feature New App - IBM Sterling B2B Install tool Misc Remove support for OCP 4.6 and 4.7 - https://access.redhat.com/support/policy/updates/openshift Updated IDS LDAP install and --console info to include search filters on output During cleanup for ROKS, logout from ibmcloud Added ROKS and ibm-ipi to security-cleanup tool Added OpenShift Ceph Tools into any installation of OCS/ODF. Added install of dos2unix util for prepare host Removed install of nmon for prepare host Added ROKS and IBM IPI to security-cleanup.sh Added OpenShift Ceph Tools into any installation of OCS/ODF Updated CloudCTL and added support to install stand alone via tools.sh","title":"v2022-09-29"},{"location":"release/#v2022-08-17b","text":"Cloud Pak for Integration Fixed Daffy code issue with Platform Navigator not finishing operator install","title":"v2022-08-17b"},{"location":"release/#v2022-08-17a","text":"Cloud Pak for Data Fixed issue with operators for services stuck in Upgrade Pending (Known CP4D issue with release of 4.5.2, breaking previous releases of 4.5.0 and 4.5.1. Would occur with Daffy, CP4D-cli, or running manually)","title":"v2022-08-17a"},{"location":"release/#v2022-08-17","text":"Tested OpenShift with Current Daffy Release GCP 4.8.46 / 4.10.22 AWS 4.8.46 / 4.10.22 Azure 4.8.46 / 4.10.22 ROKS 4.8.46 / 4.10.22 VSphere 4.8.46 / 4.10.22 KVM 4.8.46 / 4.10.22 IBM IPI 4.10.22 Cloud Pak for Business Automation Added Support for 22.0.1 IF001 Added Support for Decision Production Service(Step 2 - Namespace and base Operators) Added Support for Content Production Service(Step 2 - Namespace and base Operators) Added Support for Workflow Production Service(Step 2 - Namespace and base Operators) Addd New RPA Server MSSQL Server project and Database configured OpenLDAP for RPA configured Added 21.0.3 Added 21.0.2 IF005 Cloud Pak for Integration Add support for each service to be in its own namespace Cloud Pak for Data Added Support for 4.5.0 and 4.5.1 Added Support to specify different storage class for each service Added new service Match 360 Added new service Open OpenPages Added new service Analytics Engine powered by Apache Spark Added new service Db2 Warehouse Added new service Data Privacy Added new service Cognos Analytics Added new service Db2 OLTP Turbonomics Added Platform Operator Install Added Kubeturbo Operator Metrics Collector Watsion WAIOps Added support for 3.4.0 IBM IPI Added support for new platform type of IBM Cloud (ibm-ipi) OCP 4.10+ Misc Added ability to install older version of daffy with refresh.sh (--list) Fixed bug for KVM precheck on Host that has Memory > 1 TB Added precheck to validate the number of cluster nodes matches the number of nodes in environment file","title":"v2022-08-17"},{"location":"release/#v2022-07-14","text":"Tested OpenShift with Current Daffy Release GCP 4.8.42 / 4.10.17 AWS 4.8.42 / 4.10.17 Azure 4.8.42 / 4.10.17 ROKS 4.8.42 / 4.10.17 VSphere 4.8.42 / 4.10.17 KVM 4.8.42 / 4.10.17 Cloud Pak for Business Automation Added 21.0.3 IF008 and IF009 Added 22.0.1 New Samples ocp-starter-ocs-all-22.0.1 and roks-starter-ibm-all-22.0.1 Cloud Pak for Integration Updated to install via single operator per product Added support for 2022.2.1 Cloud Pak for Data Added 4.0.8 and 4.09 WebSphere Automation Added support for v1.4 Cloud Pak for Security Added support for v1.10 OpenShift Added ability to enable masters as worker nodes (UPI Only) - OCP_MASTER_NODES_SCHEDULABLE=true(Default is false) Added support for mirror-registry to support airgap and building of own registry(Quay mirror-registry) Added support Red Hat Advanced Cluster Management (RHACM) Added support for ODF overview page in OpenShift Console Misc Added new icons to output messages to help users with types of messages being displayed Added new logging to capture all info displayed to console for each process and list log location Added support to create new disk for NFS Added precheck to validate cluster version matches environment file OCP version Added precheck for OCS/ODF to make sure that your cluster has at least 6 workers Added precheck for Cluster Name to make sure that it matches environment file value Fixed bug to catch errors and exit when creating ROKS cluster Fixed bug with OS check, will exit now if not running on support platform Fixed bug with OCP 4.10 and Local registry mirrors (dkikuchi@us.ibm.com) Fixed bug with CP4BA using wrong variable CP_DEPLOYMENT_PLATFORM should be CP4BA_AUTO_PLATFORM (Toby.Liu@ibm.com) Fixed order of OCP installs as it relates to Airgap and Local Registry Auth Info (gmarcy@us.ibm.com)","title":"v2022-07-14"},{"location":"release/#v2022-05-23","text":"Tested OpenShift with Current Daffy Release GCP 4.8.40 AWS 4.8.40 Azure 4.8.40 ROKS 4.8.36 VSphere 4.8.40 KVM 4.8.40 OpenShift Added support for AWS Zone overrides(1,2 or 3) Added support for AWS KMS keys for disk encryption (via AWS_ENABLE_KMS_KEY=true) Added support for AWS to skip the AdministratorAccess precheck Added AWS sample security policies to use if not allowing AdministratorAccess policy VSphere Added support for VSphere Data Center with more then one VSphere Cluster Added ability to pause VSphere Install prior deploying ignition files (OCP_DEPLOY_ALL_IGNTIION_FILES_PAUSE=true) Cloud Pak for Security Added new cloud pak and Version 1.9 Cloud Pak for Business Automation Starter Services workflow - workflow-workstreams,bai,baml,baw_authoring,case,content_integration,pfs,workstreams Added IF008 support Added support for basic sample CP4BA CR's. List, Status, Console support at basic non dynamic way Added support for Open Prediction Service HUB(OPS) Added support for OpenShift 4.9 and 4.10 Cloud Pak for Data Renamed Service Cognos to Cognos Dashboards Cloud Pak for WAIOps Added support for version 3.3 Added AI Manager 3.3 Added Event Manager 3.3 IBM TechZone flag to help with using ROKS Cluster provisioned by TechZone (ROKS_PROVIDER=techzone) Misc Added precheck for DNS PTR reverse lookup for UPI install with (DNSMASQ_BUILD=false) **Tech Preview - Added support for Macbook bastion (Except vsphere-*, kvm-upi, db2 and ldap) Added new tools process to install supporting tools outside of the daffy process. (oc,aws,gcloud,cloudctl,mustgather etc) Added new mustgather process Add new Variables to help track usage of daffy tool Bug Added logic to check for errors during prepareHost step Fixed logic to test exact name for storage class, not partial name check Fixed order logic to approveVCenterCert during precheck Fixed channel name for CP4D Scheduling Operator based on release","title":"v2022-05-23"},{"location":"release/#v2022-04-20c","text":"OpenShift Added support for AWS to skip the AdministratorAccess precheck Added AWS sample security policies to use if not allowing AdministratorAccess policy Skip DNS check for AWS Publish Internal Support AWS OCP Internal Publish","title":"v2022-04-20c"},{"location":"release/#v2022-04-20b","text":"OpenShift Support Azure OCP Internal Publish Cloud Pak for Data Support for 4.0.8 (cloudctl and Single Catalog)","title":"v2022-04-20b"},{"location":"release/#v2022-04-20a","text":"OpenShift Support ability to Skip HAProxy Build Support ability to Skip DNSMasq Build Misc Support Proxy for bastion host","title":"v2022-04-20a"},{"location":"release/#v2022-04-20","text":"Tested OpenShift with Current Daffy Release GCP 4.8.36 AWS 4.8.36 Azure 4.8.36 ROKS 4.8.35 VSphere 4.8.36 KVM 4.8.36 Cloud Pak for Business Automation Added IF007 support Added support to install DB2 on Ubuntu/Redhat Linux Server Added support to install IDS(LDAP) on RedHat Linux Server Cloud Pak for Data Precheck and only support OCP 4.8 for CPD 4.0.X Fixed DV bug on ROKS - New Db2 Tuning added(With Warning) Support for 4.0.7 (cloudctl not working only Single Catalog) Added new service Cognos Dashboard Added new service DB2 Data Management Console WebSphere Automation Added support for version 1.3 Added an environment file for wsa VSphere Updated ISO images to latest version for each base version Added better error message for UPI install and Missing ISO Image OpenShift Updated TShirt Size output to allow copy/paste to simplify overrides Added support to install custom openshift-install program (TechZone request) Added VSphere IPI support to specify resource pool from custom openshift-install program (TechZone request) Added support for 4.10 (Note: Currently only CP4I supports 4.10) Added support for OpenShift Data Foundation(ODF) for OCP 4.10 only Misc Added support for RHEL 8.X bastion (Except UPI) Added new Security Cleanup script Added new version script in the root of daffy Added support for the all-in-one command to skip Cloud Pak install Enhanced Secure MQ demo and documentation Bug Fixed master cleaup.sh if user passed wrong env file, it would then do rebuild not cleanup. VSphere UPI install was broken with worker7-9 logic and haproxy and dnsmasq template files. Fixed daffy-init.sh to check for root user and exit if not root","title":"v2022-04-20"},{"location":"release/#v2022-03-31","text":"Tested OpenShift with Current Daffy Release GCP 4.8.31 AWS 4.8.31 Azure 4.8.31 ROKS 4.8.31 VSphere 4.8.31 KVM 4.8.31 Cloud Pak for Business Automation Added IF005 support Cloud Pak for Data Added support for Cloud Pak for Data 4.0.6 Added install for python3 and pip3 for >= 4.0.6 Added build/remove of portworx storage classes Added support for WKC, WML and WS on ROKS Added new service Decision Optimization(DODS) Added new service DataStage Updated cloudctl install only supports latest version of CP4D Cloud Pak for Integration Added new single MQ instance Added new App Connect Designer instance Added new API Connect instance Added new App Connect Dashboard instance Added new Operations Dashboard tracing instance Added new Event Streams instance Added new HA MQ instance Added new Asset Repository instance WebSphere Automation (WSA) Added new component that supports install of WebSphere Automation OpenShift Added support for Proxy Install Added new root level cleanup.sh, that calls the ocp cleanup.sh Added confirmation during cleanup.sh of OCP Added logic to not restart ROKS nodes multiple times if doing more than one service at different times Added support for KVM UPI to support up to 9 worker nodes Misc Added logic to support user overrides for cloud Pak components versions(All Supported Cloud Paks) Added support for AWS custom Subnets and AMI ID overrides in OCP install Added new confirmation of warranty during Daffy install","title":"v2022-03-31"},{"location":"release/#v2022-02-15b","text":"Cloud Pak for Business Automation Switched from Patterns to Services terminology Added logic to support user overrides for cloud Pak components versions","title":"v2022-02-15b"},{"location":"release/#v2022-02-15a","text":"Automated ROKS oc login step validated cluster name to be lower case and alphanumeric","title":"v2022-02-15a"},{"location":"release/#v2022-02-15","text":"Tested OpenShift with Current Daffy Release GCP 4.8.26 AWS 4.8.26 Azure 4.8.26 ROKS 4.8.26 VSphere 4.8.26 KVM 4.8.26 Added Cloud Pak Cloud Pak for Business Automation 21.0.3 Starter Services content - filenet,cmis,ier,tm,bai decisions - odm,bai content-decisions - filenet,cmis,ier,tm,odm,bai Added new OCP_INSTALL_TYPE msp = Managed Service Provider(roks-msp) Added ROKS Support Build and Cleanup of ROKS Cluster Install Cloud Paks on existing Cluster(Daffy did not build ROKS Cluster) Supports Classic only Added support for Cloud Pak for Data 4.0.5 Added new consoleFooter Function Added logic to support local cert folder outside of Daffy and not to remove on refresh command Updated refresh logic to display current version and new version during execution Updated command line options to support multiple case options(--precheck|--Precheck, etc) Updated default Storage Class and Vendor logic to support roks via env.sh files Updated all Cloud Paks and Service install precheck to use prepare host function Updated cp4waiops logic, renamed and removed old preinstall logic Updated sample env files to new testing versions of Openshift 4.8.26 Updated imageContentSources.yaml for AirGap install to list more standard mirrors Bug Fixes VSphere OCP install-config.yaml - added quotes around cluster and network values Before OCP Install cluster, remove ocp-install dir to remove old trans files Fixed Typos and Spelling errors Hide errors when sourcing .profile if it does not exist","title":"v2022-02-15"},{"location":"release/#v2022-01-18","text":"Added Cloud Pak Cloud Pak for WAIOps Multi-Cloud Pak in single Cluster Added Support for Case Install Cloud Pak for Data Cloud Pak for Data Services Added support for Cloud Pak for Data 4.0.3 and 4.0.4 Added support for Cloud Pak for Data services operations tools Added support for Cloud Pak for Integration 2021.4.1 Added Cloud Pak for Data services Statistical Package for the Social Sciences(SPSS) Watson Machine Learning(WML) Added Support for Bastion on Windows WLS2(Ubuntu 20.04) Added Daffy version to all status commands Added support for VSphere restricted folder install Added support for Image Registry for UPI install of OCP Added support to allow full install of OpenShift and Cloud Pak from single command(all-in-one) Updated CP4D New Namespace was added and all CP4D services/operands will be deployed in separate/new namespace Updated CP4D Services Added more info to service status output Added new single status command for all Daffy Supported Services Added ability to install more then once service at a time Added ability to remove service via cleanup function for each service Cleaned Up logging Moved more logs from tmp to log folder Displayed more log names and location in output Bug Fixes VSphere install now allows for special character passwords(dalmeter@us.ibm.com) Nodes were mislabeled for OpenShift Container Storage Taint added to OpenShift Nodes for OpenShift Container Storage Local Volumes for UPI install now have tolerance for OpenShift Container Storage nodes Added Overwrite Flag for 99-worker-cp4d-crio-conf to allow for node expansions with IPI install(dkikuchi@us.ibm.com) GCP removed unused roles required for precheck Corrected CP4D version(dkikuchi@us.ibm.com) Data Virtualization was looking for hard coded version and would not install post 4.0.2 KVM IP precheck only checked that IP given was part of local not not full IP Removed Features Removed support for OpenShift 4.7","title":"v2022-01-18"},{"location":"release/#v2021-12-01","text":"Added Cloud Pak (CP4I) Cloud Pak for Integration 2021.3.1 and 2021.2.1 Added Platform Support AWS(aws) - IPI Added OpenShift Support for 4.9 Added pre check logic to validate base domain and cluster name to verify valid DNS Syntax (FQDN) Added support - Subdomain for base domain on KVM and VSphere Installs Added support for KVM Web Dashboard - vmdashboard Updated OpenShift Container Storage for IPI to use provider Storage Class IPI Installs now have easy disk expansion from Console for OCS Updated OCS to label all Storage nodes as Infrastructure(Infra) nodes Update Daffy Stats URL and new arguments","title":"v2021-12-01"},{"location":"release/#v2021-11-12-initial-release","text":"OpenShift Supported 4.6,4.7 and 4.8 Cloud Pak for Data Supported 4.01 OpenShift Container Storage Supported Platforms Supported Google Cloud Provider(gcp) - IPI Azure(az) - IPI VSphere(vsphere) - IPI and UPI KVM(kvm) - UPI","title":"v2021-11-12 - Initial Release"},{"location":"AppStore/","text":"document.title = \"AppStore\"; IBM Sterling B2B Integrator Installer IBM Daffy CLI Configurator Installer CP4D Backup & Restore Utility Installer","title":"App Store"},{"location":"AppStore/CP4DBackupRestore/","text":"document.title = \"AppStore - CP4D Backup & Restore Utility\"; Info \u00b6 A Cloud Pak for Data Backup & Restore Utility that automates the steps to take an off-line restic backup and/or perform a full restore to a NEW OpenShift cluster. This utility was created for demonstration puroposes. It should be noted that there are multiple backup / restore methods. This utility ONLY automates the steps for a FULL off-line restic backup and restore to a NEW OpenShift Cluster. Please refer to the Cloud Pak for Data knowledge center for full details on all the backup / restore methods. Below is a diagram of what the process looks like Prerequisites: \u00b6 Bastion Machine (Pre-loaded with the latest Daffy Scripts) (S3 Compatible) Object Store Bucket IBM Entitlement Key with proper entitlement to Cloud Pak for Data Running OpenShift Cluster with Cloud Pak for Data Installed ( For BACKUP ) Running OpenShift Cluster ( for RESTORE ) Must at least match backup cluster compute size Storage configured to match backup cluster: Provider, Storage Classes.. etc DAFFY Bastion - You are using a Linux bastion that is supported by Daffy. You can get a bastion through TechZone or following the steps located at https://ibm.github.io/daffy/Supporting-Software/Bastion/. You have Daffy installed on your bastion. The rest of this document assumes that you have both of these. Installation of OpenShift & Cloud Pak for Data is outside the scope of this utility. Please refer to the Daffy documentation for more information on how you can deploy OpenShift and/or Cloud Pak for Data. S3 Bucket - You will need to have a S3 bucket ready with proper security setup to allow access for the utility. S3 Bucket Connectivity Information - You will need have the proper S3 connection information OBJECT STORE ACCESS KEY OBJECT STORE SECRET ACCESS KEY OBJECT STORE S3 URL OBJECT STORE S3 REGION OBJECT STORE S3 BUCKET NAME If you wish to use IBM Cloud Object Storage, here is some informatino that will help you setup the security for your Object Store. (read about the Open S3 API here The S3 URL, Region, & Bucket name can be found on the IBM Cloud Object Storage Bucket information page. Selecting the bucket and displaying the configuration tab will show you the endpoints. See image below. Outline of Automated Steps \u00b6 Note These are the steps that will be automated by the scripts. The outline here is simply to outline all the individual steps that are automated by the scirpts. Owner/Support \u00b6 Slack Channel #daffy-user-group dakrier@us.ibm.com Install Command \u00b6 Use this command to install the CP4D Backup & Restore utility on the bastion machine. / data / daffy / appstore.sh -- CP4DBackupRestore Setting up the environment variables \u00b6 Edit the following environment file with your specific informtion for your cluster and your S3 Bucket. Cluster Information OCP_URL = \"https://api.<clustername>.dojo-demo.net:6443\" # API URL Used for login to your cluster OCP_TYPE = \"self-managed\" # Valid values: aro, roks, rosa, self-managed OCP_USERNAME = \"ocpadmin\" OCP_PASSWORD = \"********\" Project / Namespaces PROJECT_CPFS_OPS = \"ibm-common-services\" PROJECT_CPD_OPS = \"ibm-common-services\" PROJECT_CATSRC = \"openshift-marketplace\" PROJECT_CPD_INSTANCE = \"cpd-instance\" FOUNDATION_NAMESPACE = \"ibm-common-services\" Object Store OBJECT_STORE_ACCESS_KEY = \"\" OBJECT_STORE_SECRET_ACCESS_KEY = \"\" OBJECT_STORE_S3_URL = \"\" OBJECT_STORE_S3_REGION = \"\" OBJECT_STORE_S3_BUCKET_NAME = \"\" As an example.. Here is a sample. OBJECT_STORE_ACCESS_KEY = \"3336567Y4a3941868123456789012345\" OBJECT_STORE_SECRET_ACCESS_KEY = \"0aa235c11a4fe72795ceb61q2w3e4r56f123456789012345\" OBJECT_STORE_S3_URL = \"https://s3.us-east.cloud-object-storage.appdomain.cloud\" OBJECT_STORE_S3_REGION = \"us-east\" OBJECT_STORE_S3_BUCKET_NAME = \"cp4d-backup\" Name Your Backup This is the prefix name of your backup files that are stored in the S3 bucket. Note There is a 40 character limit on the name of the backups. Please keep your backup name under 30 characters. The scripts will appended -operators to your backup name. OADP_BACKUP_NAME = \"\" IBM Entitlement Key IBM_ENTITLEMENT_KEY = \"\" Cloud Pak for Data Backup Version Supported versions are 4.5.x - 4.6.0 CP4DBR_VERSION = \"\" Running the tool \u00b6 The CP4D Backup & Recovery scripts will be cloned to the following directory / data / appstore / cpd - backup - restore !!! Note This should not be confused with the appstore install scripts in the /data/daffy/appstore/cp4d-backup-restore directory, which are used to clone the backup and recovery scripts to your bastion. The appstore directory should not be modified!! / data / appstore / cpd - backup - restore / run.sh The --help menu will show you what flags are available to run. / data / appstore / cpd - backup - restore / run.sh -- help Help Menu for CP4D Backup Restore Tool ################################################################ -- help |--?|?|-?| help |- help This help menu -- prepareCluster Prepare the Cluster for Backup / Restore -- runBackup Run Backup -- runRestore Run Restore --prepareCluster Flag - What does it do? The run script with this flag MUST be run on BOTH the bastion where you will perform the backup from. It will install the necessary tools and deploy the necesary operators to the OCP cluster(s). See the picture above that outlines the high level steps. This script will perform steps 2 - 6 --runBackup Flag - What does it do? The run script with this flag will execute a CP4D backup. This ONLY needs to be executed on the baston that will connect to the OCP/CP4D cluster your taking a backup from. --runRestore Flag - What does it to? The run script with this flag will execute a CP4D restore. This ONLY needs to be executed on the baston that will connect to the OCP cluster your planning to restore to. Note You do NOT need to install CP4D before running the restore. The restore process will install CP4D as part of the restore process. Performing a Backup \u00b6 You will need to prepare both the Backup Cluster & the Restore Cluster. Please run the following command to install the required tools and Cloud Pack for Data scripts on the bastion machine used for the deployment of OpenShift & Cloud Pak for Data on the cluster you wish to take a backup of. / data / appstore / cpd - backup - restore / run.sh -- prepareCluster Once you have completed the prepareCluster step and there are no issues, you can proceed to taking a CP4D Backup. / data / appstore / cpd - backup - restore / run.sh -- runBackup Note This may take a while. Do not close your session while the script is running.** Performing a Restore \u00b6 You will need to prepare both the Backup Cluster & the Restore Cluster. Please run the following command to install the required tools and Cloud Pack for Data scripts on the bastion machine used to install OpenShift for the Restore cluster. / data / appstore / cpd - backup - restore / run.sh -- prepareCluster Once you have completed the prepareCluster step and there are no issues, you can proceed to taking a CP4D Backup. / data / appstore / cpd - backup - restore / run.sh -- runRestore Note Plase Note: This may take a while. Do not close your session while the script is running.","title":"IBM CP4D Backup & Restore"},{"location":"AppStore/CP4DBackupRestore/#info","text":"A Cloud Pak for Data Backup & Restore Utility that automates the steps to take an off-line restic backup and/or perform a full restore to a NEW OpenShift cluster. This utility was created for demonstration puroposes. It should be noted that there are multiple backup / restore methods. This utility ONLY automates the steps for a FULL off-line restic backup and restore to a NEW OpenShift Cluster. Please refer to the Cloud Pak for Data knowledge center for full details on all the backup / restore methods. Below is a diagram of what the process looks like","title":"Info"},{"location":"AppStore/CP4DBackupRestore/#prerequisites","text":"Bastion Machine (Pre-loaded with the latest Daffy Scripts) (S3 Compatible) Object Store Bucket IBM Entitlement Key with proper entitlement to Cloud Pak for Data Running OpenShift Cluster with Cloud Pak for Data Installed ( For BACKUP ) Running OpenShift Cluster ( for RESTORE ) Must at least match backup cluster compute size Storage configured to match backup cluster: Provider, Storage Classes.. etc DAFFY Bastion - You are using a Linux bastion that is supported by Daffy. You can get a bastion through TechZone or following the steps located at https://ibm.github.io/daffy/Supporting-Software/Bastion/. You have Daffy installed on your bastion. The rest of this document assumes that you have both of these. Installation of OpenShift & Cloud Pak for Data is outside the scope of this utility. Please refer to the Daffy documentation for more information on how you can deploy OpenShift and/or Cloud Pak for Data. S3 Bucket - You will need to have a S3 bucket ready with proper security setup to allow access for the utility. S3 Bucket Connectivity Information - You will need have the proper S3 connection information OBJECT STORE ACCESS KEY OBJECT STORE SECRET ACCESS KEY OBJECT STORE S3 URL OBJECT STORE S3 REGION OBJECT STORE S3 BUCKET NAME If you wish to use IBM Cloud Object Storage, here is some informatino that will help you setup the security for your Object Store. (read about the Open S3 API here The S3 URL, Region, & Bucket name can be found on the IBM Cloud Object Storage Bucket information page. Selecting the bucket and displaying the configuration tab will show you the endpoints. See image below.","title":"Prerequisites:"},{"location":"AppStore/CP4DBackupRestore/#outline-of-automated-steps","text":"Note These are the steps that will be automated by the scripts. The outline here is simply to outline all the individual steps that are automated by the scirpts.","title":"Outline of Automated Steps"},{"location":"AppStore/CP4DBackupRestore/#ownersupport","text":"Slack Channel #daffy-user-group dakrier@us.ibm.com","title":"Owner/Support"},{"location":"AppStore/CP4DBackupRestore/#install-command","text":"Use this command to install the CP4D Backup & Restore utility on the bastion machine. / data / daffy / appstore.sh -- CP4DBackupRestore","title":"Install Command"},{"location":"AppStore/CP4DBackupRestore/#setting-up-the-environment-variables","text":"Edit the following environment file with your specific informtion for your cluster and your S3 Bucket. Cluster Information OCP_URL = \"https://api.<clustername>.dojo-demo.net:6443\" # API URL Used for login to your cluster OCP_TYPE = \"self-managed\" # Valid values: aro, roks, rosa, self-managed OCP_USERNAME = \"ocpadmin\" OCP_PASSWORD = \"********\" Project / Namespaces PROJECT_CPFS_OPS = \"ibm-common-services\" PROJECT_CPD_OPS = \"ibm-common-services\" PROJECT_CATSRC = \"openshift-marketplace\" PROJECT_CPD_INSTANCE = \"cpd-instance\" FOUNDATION_NAMESPACE = \"ibm-common-services\" Object Store OBJECT_STORE_ACCESS_KEY = \"\" OBJECT_STORE_SECRET_ACCESS_KEY = \"\" OBJECT_STORE_S3_URL = \"\" OBJECT_STORE_S3_REGION = \"\" OBJECT_STORE_S3_BUCKET_NAME = \"\" As an example.. Here is a sample. OBJECT_STORE_ACCESS_KEY = \"3336567Y4a3941868123456789012345\" OBJECT_STORE_SECRET_ACCESS_KEY = \"0aa235c11a4fe72795ceb61q2w3e4r56f123456789012345\" OBJECT_STORE_S3_URL = \"https://s3.us-east.cloud-object-storage.appdomain.cloud\" OBJECT_STORE_S3_REGION = \"us-east\" OBJECT_STORE_S3_BUCKET_NAME = \"cp4d-backup\" Name Your Backup This is the prefix name of your backup files that are stored in the S3 bucket. Note There is a 40 character limit on the name of the backups. Please keep your backup name under 30 characters. The scripts will appended -operators to your backup name. OADP_BACKUP_NAME = \"\" IBM Entitlement Key IBM_ENTITLEMENT_KEY = \"\" Cloud Pak for Data Backup Version Supported versions are 4.5.x - 4.6.0 CP4DBR_VERSION = \"\"","title":"Setting up the environment variables"},{"location":"AppStore/CP4DBackupRestore/#running-the-tool","text":"The CP4D Backup & Recovery scripts will be cloned to the following directory / data / appstore / cpd - backup - restore !!! Note This should not be confused with the appstore install scripts in the /data/daffy/appstore/cp4d-backup-restore directory, which are used to clone the backup and recovery scripts to your bastion. The appstore directory should not be modified!! / data / appstore / cpd - backup - restore / run.sh The --help menu will show you what flags are available to run. / data / appstore / cpd - backup - restore / run.sh -- help Help Menu for CP4D Backup Restore Tool ################################################################ -- help |--?|?|-?| help |- help This help menu -- prepareCluster Prepare the Cluster for Backup / Restore -- runBackup Run Backup -- runRestore Run Restore --prepareCluster Flag - What does it do? The run script with this flag MUST be run on BOTH the bastion where you will perform the backup from. It will install the necessary tools and deploy the necesary operators to the OCP cluster(s). See the picture above that outlines the high level steps. This script will perform steps 2 - 6 --runBackup Flag - What does it do? The run script with this flag will execute a CP4D backup. This ONLY needs to be executed on the baston that will connect to the OCP/CP4D cluster your taking a backup from. --runRestore Flag - What does it to? The run script with this flag will execute a CP4D restore. This ONLY needs to be executed on the baston that will connect to the OCP cluster your planning to restore to. Note You do NOT need to install CP4D before running the restore. The restore process will install CP4D as part of the restore process.","title":"Running the tool"},{"location":"AppStore/CP4DBackupRestore/#performing-a-backup","text":"You will need to prepare both the Backup Cluster & the Restore Cluster. Please run the following command to install the required tools and Cloud Pack for Data scripts on the bastion machine used for the deployment of OpenShift & Cloud Pak for Data on the cluster you wish to take a backup of. / data / appstore / cpd - backup - restore / run.sh -- prepareCluster Once you have completed the prepareCluster step and there are no issues, you can proceed to taking a CP4D Backup. / data / appstore / cpd - backup - restore / run.sh -- runBackup Note This may take a while. Do not close your session while the script is running.**","title":"Performing a Backup"},{"location":"AppStore/CP4DBackupRestore/#performing-a-restore","text":"You will need to prepare both the Backup Cluster & the Restore Cluster. Please run the following command to install the required tools and Cloud Pack for Data scripts on the bastion machine used to install OpenShift for the Restore cluster. / data / appstore / cpd - backup - restore / run.sh -- prepareCluster Once you have completed the prepareCluster step and there are no issues, you can proceed to taking a CP4D Backup. / data / appstore / cpd - backup - restore / run.sh -- runRestore Note Plase Note: This may take a while. Do not close your session while the script is running.","title":"Performing a Restore"},{"location":"AppStore/IBMDaffyCLIConfigurator/","text":"document.title = \"AppStore - IBM Daffy CLI Environment Configurator\"; Info \u00b6 A command line tool to build a Daffy environment file based on sets of questions. This tool was developed to simplify the creation of an environment file, figure out cluster details, copy to Daffy to run the environment file, and be able to store your environment files to your own GitHub repository. The Daffy CLI Environment Configurator will walk you through the things it needs for a successful install of OpenShift, CloudPaks, Services, and a combination of any of them that work with the Daffy steps. Prerequisites: \u00b6 You are using a Linux bastion that is supported by Daffy. You can get a bastion through TechZone or following the steps located at https://ibm.github.io/daffy/Supporting-Software/Bastion/. You have Daffy installed on your bastion. The rest of this document assumes that you have both of these. This tool supports starting at multiple places to meet your end goal of what you want Daffy to build. It can start by building OpenShift or you can bring your credentials for an existing cluster and it will provide details of various things for your environment file to continue to a Cloud Pak or a Service within a cloud pak. For more advanced features of this tool, an existing GitHub repository is available and you have a token to login to it. Owner/Support \u00b6 Slack Channel #daffy-user-group jimholz@us.ibm.com Install Command \u00b6 / data / daffy / appstore.sh -- DaffyCLIConfigurator Running the tool \u00b6 The tool by default will be installed in /data/appstore/daffy-cli. It only has one command to use it. / data / appstore / daffy - cli / create.sh Other Features \u00b6 The Daffy CLI Environment Configurator has a few features outside of just running the tool above. / data / appstore / daffy - cli / create.sh -- help To utilize GitHub to store your environment files, you can use these 2 commands / data / appstore / daffy - cli / create.sh -- copytoGitHub / data / appstore / daffy - cli / create.sh -- pullfromGitHub","title":"IBM Daffy CLI Configurator"},{"location":"AppStore/IBMDaffyCLIConfigurator/#info","text":"A command line tool to build a Daffy environment file based on sets of questions. This tool was developed to simplify the creation of an environment file, figure out cluster details, copy to Daffy to run the environment file, and be able to store your environment files to your own GitHub repository. The Daffy CLI Environment Configurator will walk you through the things it needs for a successful install of OpenShift, CloudPaks, Services, and a combination of any of them that work with the Daffy steps.","title":"Info"},{"location":"AppStore/IBMDaffyCLIConfigurator/#prerequisites","text":"You are using a Linux bastion that is supported by Daffy. You can get a bastion through TechZone or following the steps located at https://ibm.github.io/daffy/Supporting-Software/Bastion/. You have Daffy installed on your bastion. The rest of this document assumes that you have both of these. This tool supports starting at multiple places to meet your end goal of what you want Daffy to build. It can start by building OpenShift or you can bring your credentials for an existing cluster and it will provide details of various things for your environment file to continue to a Cloud Pak or a Service within a cloud pak. For more advanced features of this tool, an existing GitHub repository is available and you have a token to login to it.","title":"Prerequisites:"},{"location":"AppStore/IBMDaffyCLIConfigurator/#ownersupport","text":"Slack Channel #daffy-user-group jimholz@us.ibm.com","title":"Owner/Support"},{"location":"AppStore/IBMDaffyCLIConfigurator/#install-command","text":"/ data / daffy / appstore.sh -- DaffyCLIConfigurator","title":"Install Command"},{"location":"AppStore/IBMDaffyCLIConfigurator/#running-the-tool","text":"The tool by default will be installed in /data/appstore/daffy-cli. It only has one command to use it. / data / appstore / daffy - cli / create.sh","title":"Running the tool"},{"location":"AppStore/IBMDaffyCLIConfigurator/#other-features","text":"The Daffy CLI Environment Configurator has a few features outside of just running the tool above. / data / appstore / daffy - cli / create.sh -- help To utilize GitHub to store your environment files, you can use these 2 commands / data / appstore / daffy - cli / create.sh -- copytoGitHub / data / appstore / daffy - cli / create.sh -- pullfromGitHub","title":"Other Features"},{"location":"AppStore/IBMSterlingB2B/","text":"document.title = \"AppStore - IBM Sterling B2B\"; Info \u00b6 Auto-installation tool for IBM-Sterling B2Bi and IBM-Sterling File Gateway containers in a clustered environment. This tool was developed to simplify the basic installation of containers for IBM Sterling B2Bi-SFG on RedHat OpenShift Container Platform. Basic Assumptions & Prerequisites: \u00b6 You have already installed a ROKS cluster. You can do this manually, through TechZone or using the Daffy_RHOCP_Installer. The remainder of this documentation assumes you have already installed a ROKS cluster using the Daffy_RHOCP_Installer but can be easily adapted to the other methods of ROKS installation. You are using a UNIX / Linux bastion host to install and run the scripts. The bastion host can be located in any cloud (AWS, Azure, GCP, IBM or private). You may also be able to use your workstation as a bastion host as long as you have the rights to install software on it. You have already created a file containing your IBM software entitlement key on the bastion host. This key will allow access to the IBM B2Bi/SFG software in the IBM Container registry during the final helm install process. You already know the ingress subdomain of the cluster where you will be installing this software. This is available on the cluster dashboard webpage after Daffy installation. If you are not sure how or where to find this information click here. Owner/Support \u00b6 Slack Channel #b2bi-autoinstall James.Myers@ibm.com Install Command \u00b6 / data / daffy / tools.sh -- installIBMSterlingB2Bi Install Instructions","title":"IBM Sterling B2B Installer"},{"location":"AppStore/IBMSterlingB2B/#info","text":"Auto-installation tool for IBM-Sterling B2Bi and IBM-Sterling File Gateway containers in a clustered environment. This tool was developed to simplify the basic installation of containers for IBM Sterling B2Bi-SFG on RedHat OpenShift Container Platform.","title":"Info"},{"location":"AppStore/IBMSterlingB2B/#basic-assumptions-prerequisites","text":"You have already installed a ROKS cluster. You can do this manually, through TechZone or using the Daffy_RHOCP_Installer. The remainder of this documentation assumes you have already installed a ROKS cluster using the Daffy_RHOCP_Installer but can be easily adapted to the other methods of ROKS installation. You are using a UNIX / Linux bastion host to install and run the scripts. The bastion host can be located in any cloud (AWS, Azure, GCP, IBM or private). You may also be able to use your workstation as a bastion host as long as you have the rights to install software on it. You have already created a file containing your IBM software entitlement key on the bastion host. This key will allow access to the IBM B2Bi/SFG software in the IBM Container registry during the final helm install process. You already know the ingress subdomain of the cluster where you will be installing this software. This is available on the cluster dashboard webpage after Daffy installation. If you are not sure how or where to find this information click here.","title":"Basic Assumptions &amp; Prerequisites:"},{"location":"AppStore/IBMSterlingB2B/#ownersupport","text":"Slack Channel #b2bi-autoinstall James.Myers@ibm.com","title":"Owner/Support"},{"location":"AppStore/IBMSterlingB2B/#install-command","text":"/ data / daffy / tools.sh -- installIBMSterlingB2Bi Install Instructions","title":"Install Command"},{"location":"AppStore/IBMSterlingB2BBeta/","text":"document.title = \"AppStore - IBM Sterling B2B\"; Info \u00b6 Auto-installation tool for IBM-Sterling B2Bi and IBM-Sterling File Gateway containers in a clustered environment. This tool was developed to simplify the basic installation of containers for IBM Sterling B2Bi-SFG on RedHat OpenShift Container Platform. Basic Assumptions & Prerequisites: \u00b6 You have already installed a ROKS cluster. You can do this manually, through TechZone or using the Daffy_RHOCP_Installer. The remainder of this documentation assumes you have already installed a ROKS cluster using the Daffy_RHOCP_Installer but can be easily adapted to the other methods of ROKS installation. You are using a UNIX / Linux bastion host to install and run the scripts. The bastion host can be located in any cloud (AWS, Azure, GCP, IBM or private). You may also be able to use your workstation as a bastion host as long as you have the rights to install software on it. You have already created a file containing your IBM software entitlement key on the bastion host. This key will allow access to the IBM B2Bi/SFG software in the IBM Container registry during the final helm install process. You already know the ingress subdomain of the cluster where you will be installing this software. This is available on the cluster dashboard webpage after Daffy installation. If you are not sure how or where to find this information click here. Owner/Support \u00b6 Slack Channel #b2bi-autoinstall James.Myers@ibm.com Install Command \u00b6 / data / daffy / appstore.sh -- installIBMSterlingB2Bi Install Instructions","title":"IBMSterlingB2BBeta"},{"location":"AppStore/IBMSterlingB2BBeta/#info","text":"Auto-installation tool for IBM-Sterling B2Bi and IBM-Sterling File Gateway containers in a clustered environment. This tool was developed to simplify the basic installation of containers for IBM Sterling B2Bi-SFG on RedHat OpenShift Container Platform.","title":"Info"},{"location":"AppStore/IBMSterlingB2BBeta/#basic-assumptions-prerequisites","text":"You have already installed a ROKS cluster. You can do this manually, through TechZone or using the Daffy_RHOCP_Installer. The remainder of this documentation assumes you have already installed a ROKS cluster using the Daffy_RHOCP_Installer but can be easily adapted to the other methods of ROKS installation. You are using a UNIX / Linux bastion host to install and run the scripts. The bastion host can be located in any cloud (AWS, Azure, GCP, IBM or private). You may also be able to use your workstation as a bastion host as long as you have the rights to install software on it. You have already created a file containing your IBM software entitlement key on the bastion host. This key will allow access to the IBM B2Bi/SFG software in the IBM Container registry during the final helm install process. You already know the ingress subdomain of the cluster where you will be installing this software. This is available on the cluster dashboard webpage after Daffy installation. If you are not sure how or where to find this information click here.","title":"Basic Assumptions &amp; Prerequisites:"},{"location":"AppStore/IBMSterlingB2BBeta/#ownersupport","text":"Slack Channel #b2bi-autoinstall James.Myers@ibm.com","title":"Owner/Support"},{"location":"AppStore/IBMSterlingB2BBeta/#install-command","text":"/ data / daffy / appstore.sh -- installIBMSterlingB2Bi Install Instructions","title":"Install Command"},{"location":"AppStore/indexBeta/","text":"document.title = \"AppStore\"; IBM Sterling B2B Integrator Installer IBM Daffy CLI Configurator Installer","title":"indexBeta"},{"location":"AppStore/info/","text":"document.title = \"AppStore - Info\"; What is the AppStore? \u00b6 The Daffy app store is the official repository for all additional software, services, and platforms built on top of OpenShift and IBM Cloud Paks. Although these apps are maintained by developers outside of the Daffy team, they are seamlessly integrated with Daffy. Installation is simple, most apps can be installed in a single command via the Daffy tools script. /data/daffy/appstore.sh Support Model \u00b6 Apps are integrated with Daffy, but supported by developers outside of Daffy. For questions and troubleshooting, please contact the maintainers of the apps. Add Your App \u00b6 Getting your app added to the Daffy app store is a simple process. More info on this coming soon.","title":"Information"},{"location":"AppStore/info/#what-is-the-appstore","text":"The Daffy app store is the official repository for all additional software, services, and platforms built on top of OpenShift and IBM Cloud Paks. Although these apps are maintained by developers outside of the Daffy team, they are seamlessly integrated with Daffy. Installation is simple, most apps can be installed in a single command via the Daffy tools script. /data/daffy/appstore.sh","title":"What is the AppStore?"},{"location":"AppStore/info/#support-model","text":"Apps are integrated with Daffy, but supported by developers outside of Daffy. For questions and troubleshooting, please contact the maintainers of the apps.","title":"Support Model"},{"location":"AppStore/info/#add-your-app","text":"Getting your app added to the Daffy app store is a simple process. More info on this coming soon.","title":"Add Your App"},{"location":"Beta/","text":"document.title = \"Beta\"; Active Beta Features - Currently Closed \u00b6 Warning Early features - Still in testing mode, may not always work. User be aware!!! To get the beta version of Daffy, run the following command on your bastion: curl http://get.daffy-installer.com/download-scripts/daffy-beta-init.sh | bash Warning Please do not share above with others We update the beta code nightly. If you have the beta versions, you can run this command to get the daily updates to the beta code of daffy: /data/daffy/refresh-beta.sh If you want to switch back to the production of the daffy code, you can run this: /data/daffy/refresh.sh","title":"Index"},{"location":"Beta/#active-beta-features-currently-closed","text":"Warning Early features - Still in testing mode, may not always work. User be aware!!! To get the beta version of Daffy, run the following command on your bastion: curl http://get.daffy-installer.com/download-scripts/daffy-beta-init.sh | bash Warning Please do not share above with others We update the beta code nightly. If you have the beta versions, you can run this command to get the daily updates to the beta code of daffy: /data/daffy/refresh-beta.sh If you want to switch back to the production of the daffy code, you can run this: /data/daffy/refresh.sh","title":"Active Beta Features - Currently Closed"},{"location":"Cloud-Paks/","text":"document.title = \"Cloud Pak\"; Cloud Paks \u00b6 Business Automation Data Integration Watson AIOps WebSphere Automation What is required to deploy a Cloud Pak? \u00b6 Before you can deploy a Cloud Pak, you must have the following: You must have a cluster running \u00b6 An existing cluster in a supported provider Can be built with Daffy or any other process IBM entitlement key \u00b6 You can obtain your existing entitlement key here: https://myibm.ibm.com/products-services/containerlibrary Important: if you're installing on a customer owned platform account or an on-prem customer data center, you MUST instruct your customer to register for a trial account and use their pull secret for the install if they don't own the software. Do not use your own pull secret for customer engagements. Note all IBMer's are entitled to an IBM Entitlement Key. Your key can ONLY be used for training and demo purposes. Do not provide your personal entitlement key to others. If customer or business partner does not have an IBM entitlement key, go to the following link to get one (IBM Internal Link): IBM Trial Software Process Warning For internal IBM use only, Link below will only work while in the IBM Network","title":"Index"},{"location":"Cloud-Paks/#cloud-paks","text":"Business Automation Data Integration Watson AIOps WebSphere Automation","title":"Cloud Paks"},{"location":"Cloud-Paks/#what-is-required-to-deploy-a-cloud-pak","text":"Before you can deploy a Cloud Pak, you must have the following:","title":"What is required to deploy a Cloud Pak?"},{"location":"Cloud-Paks/#you-must-have-a-cluster-running","text":"An existing cluster in a supported provider Can be built with Daffy or any other process","title":"You must have a cluster running"},{"location":"Cloud-Paks/#ibm-entitlement-key","text":"You can obtain your existing entitlement key here: https://myibm.ibm.com/products-services/containerlibrary Important: if you're installing on a customer owned platform account or an on-prem customer data center, you MUST instruct your customer to register for a trial account and use their pull secret for the install if they don't own the software. Do not use your own pull secret for customer engagements. Note all IBMer's are entitled to an IBM Entitlement Key. Your key can ONLY be used for training and demo purposes. Do not provide your personal entitlement key to others. If customer or business partner does not have an IBM entitlement key, go to the following link to get one (IBM Internal Link): IBM Trial Software Process Warning For internal IBM use only, Link below will only work while in the IBM Network","title":"IBM entitlement key"},{"location":"Cloud-Paks/Business-Automation/","text":"document.title = \"Cloud Pak - Business Automation\"; Cloud Pak for Business Automation \u00b6 At this point, you have a working OCP cluster on your platform of choice. Your < ENVIRONMENT_NAME >-env.sh configuration file will contain details of the platform and OCP installation. You will now add the following configurations to this file: 1) The Cloud Pak info that you wish to install 2) The services that you wish to install on the Cloud Pak Step 2: Deploy Cloud Pak \u00b6 Deploying the Cloud Pak for Business Automation only requires two entries to your environment file (/data/daffy/env/ < ENVIRONMENT_NAME >-env.sh): You need to pick starter services and/or production services or RPA service. Variable Name Info Install Type Required CP4BA_VERSION The version you want to install Both Yes CP4BA_IFIX The fix version of your version supported Both No CP4BA_DEPLOYMENT_STARTER_SERVICE The name of the service you want to deploy Starter No CP4BA_DEPLOYMENT_STARTER_SERVICE_SAMPLE The name of sample yaml you want to deploy Starter No CP4BA_DEPLOYMENT_PRODUCTION_DECISIONS true if you want to deploy decisions Production No CP4BA_DEPLOYMENT_PRODUCTION_CONTENT true if you want to deploy content Production No CP4BA_DEPLOYMENT_PRODUCTION_WORKFLOW true if you want to deploy workflow Production No Valid Options: Variable Name Valid Options CP4BA_VERSION 22.0.2 CP4BA_IFIX IF001, IF002 CP4BA_DEPLOYMENT_STARTER_SERVICE content,decisions,docprocessing,content-decisions,workflow,docprocessing,samples,all RPA Server \u00b6 Warning Currently does not support running RPA and Cp4BA in same cluster. Known issue with Common Services as they are not compatible versions. Variable Name Info Required CP4BA_ENABLE_SERVICE_RPA_SERVER true if you want to deploy RPA Server No CP4BA_RPA_SERVER_VERSION Version of RPA to deploy No CP4BA_RPA_SERVER_IFIX The fix version of your version supported Yes if RPA True CP4BA_RPA_SERVER_FIRST_TENANT_OWNER_EMAIL Owner Email Address Yes if RPA True CP4BA_RPA_SERVER_FIRST_TENANT_OWNER_NAME Owner Full Name Yes if RPA True CP4BA_RPA_SERVER_SMTP_USER SMTP User that RPA will use to send Email Yes if RPA True CP4BA_RPA_SERVER_SMTP_PORT SMTP Port that RPA will use to send Email Yes if RPA True CP4BA_RPA_SERVER_SMTP_SERVER SMTP Server/IP that RPA will use to send Email Yes if RPA True Valid Options: Variable Name Valid Options CP4BA_RPA_SERVER_VERSION 21.0.4 or 21.0.5 CP4BA_RPA_SERVER_IFIX N/A You can copy the following to your < ENVIRONMENT_NAME >-env.sh: #Core CP4BA Settings ################################################### CP4BA_VERSION = \"22.0.2\" CP4BA_IFIX = \"IF002\" CP4BA_DEPLOYMENT_STARTER_SERVICE = \"content\" #Prodution Services ################################################### CP4BA_DEPLOYMENT_PRODUCTION_DECISIONS = \"false\" #Prodution Services - only step 2 supported today. ################################################### CP4BA_DEPLOYMENT_PRODUCTION_CONTENT = \"false\" CP4BA_DEPLOYMENT_PRODUCTION_WORKFLOW = \"false\" #RPA Server ############################################ CP4BA_ENABLE_SERVICE_RPA_SERVER = \"false\" CP4BA_RPA_SERVER_VERSION = \"21.0.5\" #CP4BA_RPA_SERVER_IFIX=\"\" #CP4BA_RPA_SERVER_FIRST_TENANT_OWNER_EMAIL=\"daffy@us.ibm.com\" #CP4BA_RPA_SERVER_FIRST_TENANT_OWNER_NAME=\"Daffy Admin\" #CP4BA_RPA_SERVER_SMTP_USER=\"GmailID@Gmail.com\" #CP4BA_RPA_SERVER_SMTP_PORT=\"587\" #CP4BA_RPA_SERVER_SMTP_SERVER=\"gmail.smtp.com\" Starter Service Mapping \u00b6 Service Components CP4BA Version decisions odm, ads, bastudio, aae, bai 22.0.2 content filenet, cmis, ier, tm, bai 22.0.2 content-decisions workflow workflow, workstreams, pfs, baw_authoring, case, bai 22.0.2 docprocessing docprocessing, content, cmis, css, tm 22.0.2 all All Components(except iccsap) 22.0.2 samples Depends on sample 22.0.2 Run the following command to deploy the Cloud Pak for Business Automation: /data/daffy/cp4ba/build.sh <ENVIRONMENT_NAME> When this step is complete, approximately after 10 minutes depending on your environment, you will have the Cloud Pak running. These are just the core Cloud Pak operators, no service is running at this point. The cluster is now ready to deploy the service. At this stage, the cluster consists of IBM Foundation Services and the Cloud Pak for Business Automation operators in the following projects based on selection above: cp4ba-starter cp4ba-starter-decisions cp4ba-starter-docprocessing cp4ba-starter-content cp4ba-starter-workflow cp4ba-content cp4ba-decisions cp4ba-workflow cs-control HTML Video embed Step 3: Deploy Services \u00b6 Deploying the service does not need any new values to your environment file (< ENVIRONMENT_NAME >-env.sh>). It will use the same values during the Cloud Pak deployment. Variable Name Info Install Type Required CP4BA_VERSION The version you want to install Both Yes CP4BA_IFIX The fix version of your version support it Both No CP4BA_DEPLOYMENT_STARTER_SERVICE The name of the service you want to deploy Starter No CP4BA_DEPLOYMENT_PRODUCTION_DECISIONS true if you want to deploy decisions Production No CP4BA_DEPLOYMENT_STARTER_SERVICE_SAMPLE The name of sample yaml you want to deploy Starter No Valid Options: Variable Name Valid Options CP4BA_VERSION 22.0.2 CP4BA_IFIX IF001, IF002 CP4BA_DEPLOYMENT_STARTER_SERVICE content,decisions,docprocessing,content-decisions,workflow,all,samples Instead of using the included services, you can also deploy your own sample. Variable Valid Option Required CP4BA_DEPLOYMENT_STARTER_SERVICE samples No CP4BA_DEPLOYMENT_STARTER_SERVICE_SAMPLE see list below No Sample Name The value you use is without the .yaml in the name. cd /data/daffy/cp4ba/templates/services/samples To use samples, you would have to build your own CR yaml and store in the above directory and you would give the name of the sample. RPA Server \u00b6 Warning Currently does not support a ROKS deployment. There is known issue with RPA Server and ticket is open with IBM Support. Variable Name Info Required CP4BA_ENABLE_SERVICE_RPA_SERVER true if you want to deploy RPA Server No CP4BA_RPA_SERVER_VERSION Version of RPA to deploy Yes if RPA True CP4BA_RPA_SERVER_IFIX The fix version of your version supported Yes if RPA True CP4BA_RPA_SERVER_FIRST_TENANT_OWNER_EMAIL Owner Email Address Yes if RPA True CP4BA_RPA_SERVER_FIRST_TENANT_OWNER_NAME Owner Full Name Yes if RPA True CP4BA_RPA_SERVER_SMTP_USER SMTP User that RPA will use to send Email Yes if RPA True CP4BA_RPA_SERVER_SMTP_PORT SMTP Port that RPA will use to send Email Yes if RPA True CP4BA_RPA_SERVER_SMTP_SERVER SMTP Server/IP that RPA will use to send Email Yes if RPA True You can copy the following to your < ENVIRONMENT_NAME >-env.sh: #Core CP4BA Settings ################################################### CP4BA_VERSION = \"22.0.2\" CP4BA_IFIX = \"IF002\" CP4BA_DEPLOYMENT_STARTER_SERVICE = \"content\" #CP4BA_DEPLOYMENT_STARTER_SERVICE_SAMPLE=\"<YourSampleHere>\" #RPA Server ############################################ CP4BA_ENABLE_SERVICE_RPA_SERVER = \"false\" CP4BA_RPA_SERVER_VERSION = \"21.0.5\" CP4BA_RPA_SERVER_IFIX = \"\" #CP4BA_RPA_SERVER_FIRST_TENANT_OWNER_EMAIL=\"daffy@us.ibm.com\" #CP4BA_RPA_SERVER_FIRST_TENANT_OWNER_NAME=\"Daffy Admin\" #CP4BA_RPA_SERVER_SMTP_USER=\"GmailID@Gmail.com\" #CP4BA_RPA_SERVER_SMTP_PORT=587 #CP4BA_RPA_SERVER_SMTP_SERVER=\"gmail.smtp.com\" Starter Service Mapping \u00b6 Service Components CP4BA Version decisions odm, ads, bastudio, aae, bai 22.0.2 content filenet, cmis, ier, tm, bai 22.0.2 content-decisions workflow workflow, workstreams, pfs, baw_authoring, case, bai 22.0.2 docprocessing docprocessing, content, cmis, css, tm 22.0.2 all all (except iccsap) 22.0.2 samples Depends on sample Depends on sample Be aware, this step is async, meaning that the Daffy engine will deploy the service to the cluster and then complete. This only takes a few minutes to complete. When the deployment of the service script is done, the service is not running yet. Depending on your service, it can take from 1 hour to 6 to complete. You can use the status command below to watch its progress. Production Services \u00b6 Options for Services Service Components CP4BA Version decisions odm, ads, bastudio, aae, bai 22.0.2 Decisions Production \u00b6 To deploy a Decisions Production Pattern , you have to have a db2 database and an IDS LDAP server. This will also include BAI. Daffy can either use your existing assets or can build them locally where daffy is installed. If you just want daffy to build all the needed components on your local bastion, just set the build flags below to true and daffy will build it all. Important To have daffy build your database and LDAP config info, you need to have DB2 and IDS LDAP installed locally. Instructions: DB2 and LDAP Variable Name Info Required Valid Options CP4BA_DEPLOYMENT_PRODUCTION_DECISIONS Do you want to deploy Decisions? No true or false CP4BA_DEPLOYMENT_PRODUCTION_DECISIONS_BUILD_DB Do you want to deploy Decisions DB2 Database locally? No true or false CP4BA_DEPLOYMENT_PRODUCTION_DECISIONS_BUILD_LDAP Do you want to deploy Decisions LDAP locally? No true or false CP4BA_DEPLOYMENT_PRODUCTION_DECISIONS_LDAP_SERVER DNS Name or IP address for your IDS LDAP Server? No DNS or IP address CP4BA_DEPLOYMENT_PRODUCTION_DECISIONS_DC_ODM_DATABASE_SERVERNAME DNS Name or IP address for your DB2 Server No DNS or IP address CP4BA_DEPLOYMENT_PRODUCTION_DECISIONS = \"true\" CP4BA_DEPLOYMENT_PRODUCTION_DECISIONS_BUILD_DB = \"true\" CP4BA_DEPLOYMENT_PRODUCTION_DECISIONS_BUILD_LDAP = \"true\" CP4BA_DEPLOYMENT_PRODUCTION_DECISIONS_LDAP_SERVER = \"XXX.XXX.XXX.XXX\" CP4BA_DEPLOYMENT_PRODUCTION_DECISIONS_DC_ODM_DATABASE_SERVERNAME = \"XXX.XXX.XXX.XXX\" Execute Service \u00b6 Run the following command to deploy the Cloud Pak for Business Automation services: /data/daffy/cp4ba/service.sh <ENVIRONMENT_NAME> HTML Video embed Step 3a: Status \u00b6 The service can take a few hours to complete, based on which one you chose to deploy. To help monitor the status of the service deployment, you can run the help flag to see what flags you can use to get information on your service deployment. Run the following commands to check the Cloud Pak for Business Automation to see what command flags you can run: /data/daffy/cp4ba/service.sh <ENVIRONMENT_NAME> --help The following command will give you the status of all starter components for the service you deployed: /data/daffy/cp4ba/service.sh <ENVIRONMENT_NAME> --StarterStatus If you want to show just one status of starter service that you deployed for example CP4BA_DEPLOYMENT_STARTER_DECISIONS: /data/daffy/cp4ba/service.sh <ENVIRONMENT_NAME> --StarterStatus decisions The following command will give you the status of all Production components for the service you deployed: /data/daffy/cp4ba/service.sh <ENVIRONMENT_NAME> --Status The following command will give you the status of RPA Server you deployed: /data/daffy/cp4ba/service.sh <ENVIRONMENT_NAME> --RPAStatus If you want to have a running job to refresh every few seconds, you can run the above command via the watch command: watch -c /data/daffy/cp4ba/service.sh <ENVIRONMENT_NAME> --StarterStatus To find out the connection info to your new starter services, you can run the console flag to get user names, passwords, and URLs to connect to: /data/daffy/cp4ba/service.sh <ENVIRONMENT_NAME> --StarterConsole If you want to show just one console of starter service that you deployed for example CP4BA_DEPLOYMENT_STARTER_DECISIONS: /data/daffy/cp4ba/service.sh <ENVIRONMENT_NAME> --StarterConsole decisions To find out the connection info to your new Production services, you can run the console flag to get user names, passwords, and URLs to connect to: /data/daffy/cp4ba/service.sh <ENVIRONMENT_NAME> --Console To find out the connection info to your RPA Server , you can run the console flag to get user names, passwords, and URLs to connect to: /data/daffy/cp4ba/service.sh <ENVIRONMENT_NAME> --RPAConsole HTML Video embed Post Daffy Steps \u00b6 RPA Server \u00b6 OpenLdap Config \u00b6 Once you have installed RPA server, you will need add the LDAP Server from the Cloud Pak Dashboard. The following steps will help you manually preform these steps. The details for the next steps will come when you install Step 3 of Daffy for RPA Server, via your command line console. Screenshot 1) Login Cloud Pak Dashboard Link via \" IBM provided credentials(admin only) \" Screenshot 2) From the hamburger menu bar, under Administration, select Access Control Screenshot 3) Click \" Add users\" Screenshot 4) Add your RPA user and give Access to all roles. 5) Logout of the Cloud Pak dashboard and close your browser. At this point, you are ready to logon to your RPA Server Console. Decisions Server \u00b6 Once you have installed Production Decisions Server pattern, you will need to do a few steps. Map Your LDAP Groups to IDP Roles Install and Configure your Rule Designer Map LDAP groups to Roles \u00b6 Run the following command to Import and Map your LDAP groups to Zen roles /data/daffy/cp4ba/service.sh <env_name> --decisionImportLDAPGroups Screenshot Rule Designer \u00b6 After you installed Decisions Services, you need to install and connect Rule Designer to your new instance. For the next steps, any information you need from your environment you should be able to collect from the service.sh --console command output of Daffy. Important Original Instructions can be found here 1) Download and install Eclipse. Download Eclipse Screenshot 2) Install ODM from Marketplace. a. Start Eclipse. Click Help > Eclipse Marketplace . b. In the Find field, enter the text ODM and click Go. c. Locate the entry IBM Operational Decision Manager for Developers v8.11.0 - Rule Designer that matches the version to install, and then click Install . Screenshot 3) Download truststore.jks from your cluster Screenshot 4) Update your Eclipse.ini and add these lines at the end:(update path info based on your setup) - Djavax.net.ssl.trustStore = C :/ Users / Administrator / Desktop / MyTrustStores / truststore.jks - Djavax.net.ssl.trustStorePassword = changeit 5) Get the Zen Key API from the CPD console Important Before the Zen API Key can be generated, you must Map LDAP groups to Roles from above and then via Browser, logon to the Cloud Pak Decisions Desktop once. Screenshot 6) Connect Rule Designer Here is how you can connect to your new Decision Center Right Click your Rule Project Select Decision Center | connect Fill out from based on daffy output from --console URL: Decision Center Authentication: Zen API Key User ID: Decisions Admin Username API Key: Decisions Admin Zen API Key Screenshot d. Click Next and then Finish","title":"Business Automation"},{"location":"Cloud-Paks/Business-Automation/#cloud-pak-for-business-automation","text":"At this point, you have a working OCP cluster on your platform of choice. Your < ENVIRONMENT_NAME >-env.sh configuration file will contain details of the platform and OCP installation. You will now add the following configurations to this file: 1) The Cloud Pak info that you wish to install 2) The services that you wish to install on the Cloud Pak","title":"Cloud Pak for Business Automation"},{"location":"Cloud-Paks/Business-Automation/#step-2-deploy-cloud-pak","text":"Deploying the Cloud Pak for Business Automation only requires two entries to your environment file (/data/daffy/env/ < ENVIRONMENT_NAME >-env.sh): You need to pick starter services and/or production services or RPA service. Variable Name Info Install Type Required CP4BA_VERSION The version you want to install Both Yes CP4BA_IFIX The fix version of your version supported Both No CP4BA_DEPLOYMENT_STARTER_SERVICE The name of the service you want to deploy Starter No CP4BA_DEPLOYMENT_STARTER_SERVICE_SAMPLE The name of sample yaml you want to deploy Starter No CP4BA_DEPLOYMENT_PRODUCTION_DECISIONS true if you want to deploy decisions Production No CP4BA_DEPLOYMENT_PRODUCTION_CONTENT true if you want to deploy content Production No CP4BA_DEPLOYMENT_PRODUCTION_WORKFLOW true if you want to deploy workflow Production No Valid Options: Variable Name Valid Options CP4BA_VERSION 22.0.2 CP4BA_IFIX IF001, IF002 CP4BA_DEPLOYMENT_STARTER_SERVICE content,decisions,docprocessing,content-decisions,workflow,docprocessing,samples,all","title":"Step 2: Deploy Cloud Pak"},{"location":"Cloud-Paks/Business-Automation/#rpa-server","text":"Warning Currently does not support running RPA and Cp4BA in same cluster. Known issue with Common Services as they are not compatible versions. Variable Name Info Required CP4BA_ENABLE_SERVICE_RPA_SERVER true if you want to deploy RPA Server No CP4BA_RPA_SERVER_VERSION Version of RPA to deploy No CP4BA_RPA_SERVER_IFIX The fix version of your version supported Yes if RPA True CP4BA_RPA_SERVER_FIRST_TENANT_OWNER_EMAIL Owner Email Address Yes if RPA True CP4BA_RPA_SERVER_FIRST_TENANT_OWNER_NAME Owner Full Name Yes if RPA True CP4BA_RPA_SERVER_SMTP_USER SMTP User that RPA will use to send Email Yes if RPA True CP4BA_RPA_SERVER_SMTP_PORT SMTP Port that RPA will use to send Email Yes if RPA True CP4BA_RPA_SERVER_SMTP_SERVER SMTP Server/IP that RPA will use to send Email Yes if RPA True Valid Options: Variable Name Valid Options CP4BA_RPA_SERVER_VERSION 21.0.4 or 21.0.5 CP4BA_RPA_SERVER_IFIX N/A You can copy the following to your < ENVIRONMENT_NAME >-env.sh: #Core CP4BA Settings ################################################### CP4BA_VERSION = \"22.0.2\" CP4BA_IFIX = \"IF002\" CP4BA_DEPLOYMENT_STARTER_SERVICE = \"content\" #Prodution Services ################################################### CP4BA_DEPLOYMENT_PRODUCTION_DECISIONS = \"false\" #Prodution Services - only step 2 supported today. ################################################### CP4BA_DEPLOYMENT_PRODUCTION_CONTENT = \"false\" CP4BA_DEPLOYMENT_PRODUCTION_WORKFLOW = \"false\" #RPA Server ############################################ CP4BA_ENABLE_SERVICE_RPA_SERVER = \"false\" CP4BA_RPA_SERVER_VERSION = \"21.0.5\" #CP4BA_RPA_SERVER_IFIX=\"\" #CP4BA_RPA_SERVER_FIRST_TENANT_OWNER_EMAIL=\"daffy@us.ibm.com\" #CP4BA_RPA_SERVER_FIRST_TENANT_OWNER_NAME=\"Daffy Admin\" #CP4BA_RPA_SERVER_SMTP_USER=\"GmailID@Gmail.com\" #CP4BA_RPA_SERVER_SMTP_PORT=\"587\" #CP4BA_RPA_SERVER_SMTP_SERVER=\"gmail.smtp.com\"","title":"RPA Server"},{"location":"Cloud-Paks/Business-Automation/#starter-service-mapping","text":"Service Components CP4BA Version decisions odm, ads, bastudio, aae, bai 22.0.2 content filenet, cmis, ier, tm, bai 22.0.2 content-decisions workflow workflow, workstreams, pfs, baw_authoring, case, bai 22.0.2 docprocessing docprocessing, content, cmis, css, tm 22.0.2 all All Components(except iccsap) 22.0.2 samples Depends on sample 22.0.2 Run the following command to deploy the Cloud Pak for Business Automation: /data/daffy/cp4ba/build.sh <ENVIRONMENT_NAME> When this step is complete, approximately after 10 minutes depending on your environment, you will have the Cloud Pak running. These are just the core Cloud Pak operators, no service is running at this point. The cluster is now ready to deploy the service. At this stage, the cluster consists of IBM Foundation Services and the Cloud Pak for Business Automation operators in the following projects based on selection above: cp4ba-starter cp4ba-starter-decisions cp4ba-starter-docprocessing cp4ba-starter-content cp4ba-starter-workflow cp4ba-content cp4ba-decisions cp4ba-workflow cs-control HTML Video embed","title":"Starter Service Mapping"},{"location":"Cloud-Paks/Business-Automation/#step-3-deploy-services","text":"Deploying the service does not need any new values to your environment file (< ENVIRONMENT_NAME >-env.sh>). It will use the same values during the Cloud Pak deployment. Variable Name Info Install Type Required CP4BA_VERSION The version you want to install Both Yes CP4BA_IFIX The fix version of your version support it Both No CP4BA_DEPLOYMENT_STARTER_SERVICE The name of the service you want to deploy Starter No CP4BA_DEPLOYMENT_PRODUCTION_DECISIONS true if you want to deploy decisions Production No CP4BA_DEPLOYMENT_STARTER_SERVICE_SAMPLE The name of sample yaml you want to deploy Starter No Valid Options: Variable Name Valid Options CP4BA_VERSION 22.0.2 CP4BA_IFIX IF001, IF002 CP4BA_DEPLOYMENT_STARTER_SERVICE content,decisions,docprocessing,content-decisions,workflow,all,samples Instead of using the included services, you can also deploy your own sample. Variable Valid Option Required CP4BA_DEPLOYMENT_STARTER_SERVICE samples No CP4BA_DEPLOYMENT_STARTER_SERVICE_SAMPLE see list below No Sample Name The value you use is without the .yaml in the name. cd /data/daffy/cp4ba/templates/services/samples To use samples, you would have to build your own CR yaml and store in the above directory and you would give the name of the sample.","title":"Step 3: Deploy Services"},{"location":"Cloud-Paks/Business-Automation/#rpa-server_1","text":"Warning Currently does not support a ROKS deployment. There is known issue with RPA Server and ticket is open with IBM Support. Variable Name Info Required CP4BA_ENABLE_SERVICE_RPA_SERVER true if you want to deploy RPA Server No CP4BA_RPA_SERVER_VERSION Version of RPA to deploy Yes if RPA True CP4BA_RPA_SERVER_IFIX The fix version of your version supported Yes if RPA True CP4BA_RPA_SERVER_FIRST_TENANT_OWNER_EMAIL Owner Email Address Yes if RPA True CP4BA_RPA_SERVER_FIRST_TENANT_OWNER_NAME Owner Full Name Yes if RPA True CP4BA_RPA_SERVER_SMTP_USER SMTP User that RPA will use to send Email Yes if RPA True CP4BA_RPA_SERVER_SMTP_PORT SMTP Port that RPA will use to send Email Yes if RPA True CP4BA_RPA_SERVER_SMTP_SERVER SMTP Server/IP that RPA will use to send Email Yes if RPA True You can copy the following to your < ENVIRONMENT_NAME >-env.sh: #Core CP4BA Settings ################################################### CP4BA_VERSION = \"22.0.2\" CP4BA_IFIX = \"IF002\" CP4BA_DEPLOYMENT_STARTER_SERVICE = \"content\" #CP4BA_DEPLOYMENT_STARTER_SERVICE_SAMPLE=\"<YourSampleHere>\" #RPA Server ############################################ CP4BA_ENABLE_SERVICE_RPA_SERVER = \"false\" CP4BA_RPA_SERVER_VERSION = \"21.0.5\" CP4BA_RPA_SERVER_IFIX = \"\" #CP4BA_RPA_SERVER_FIRST_TENANT_OWNER_EMAIL=\"daffy@us.ibm.com\" #CP4BA_RPA_SERVER_FIRST_TENANT_OWNER_NAME=\"Daffy Admin\" #CP4BA_RPA_SERVER_SMTP_USER=\"GmailID@Gmail.com\" #CP4BA_RPA_SERVER_SMTP_PORT=587 #CP4BA_RPA_SERVER_SMTP_SERVER=\"gmail.smtp.com\"","title":"RPA Server"},{"location":"Cloud-Paks/Business-Automation/#starter-service-mapping_1","text":"Service Components CP4BA Version decisions odm, ads, bastudio, aae, bai 22.0.2 content filenet, cmis, ier, tm, bai 22.0.2 content-decisions workflow workflow, workstreams, pfs, baw_authoring, case, bai 22.0.2 docprocessing docprocessing, content, cmis, css, tm 22.0.2 all all (except iccsap) 22.0.2 samples Depends on sample Depends on sample Be aware, this step is async, meaning that the Daffy engine will deploy the service to the cluster and then complete. This only takes a few minutes to complete. When the deployment of the service script is done, the service is not running yet. Depending on your service, it can take from 1 hour to 6 to complete. You can use the status command below to watch its progress.","title":"Starter Service Mapping"},{"location":"Cloud-Paks/Business-Automation/#production-services","text":"Options for Services Service Components CP4BA Version decisions odm, ads, bastudio, aae, bai 22.0.2","title":"Production Services"},{"location":"Cloud-Paks/Business-Automation/#decisions-production","text":"To deploy a Decisions Production Pattern , you have to have a db2 database and an IDS LDAP server. This will also include BAI. Daffy can either use your existing assets or can build them locally where daffy is installed. If you just want daffy to build all the needed components on your local bastion, just set the build flags below to true and daffy will build it all. Important To have daffy build your database and LDAP config info, you need to have DB2 and IDS LDAP installed locally. Instructions: DB2 and LDAP Variable Name Info Required Valid Options CP4BA_DEPLOYMENT_PRODUCTION_DECISIONS Do you want to deploy Decisions? No true or false CP4BA_DEPLOYMENT_PRODUCTION_DECISIONS_BUILD_DB Do you want to deploy Decisions DB2 Database locally? No true or false CP4BA_DEPLOYMENT_PRODUCTION_DECISIONS_BUILD_LDAP Do you want to deploy Decisions LDAP locally? No true or false CP4BA_DEPLOYMENT_PRODUCTION_DECISIONS_LDAP_SERVER DNS Name or IP address for your IDS LDAP Server? No DNS or IP address CP4BA_DEPLOYMENT_PRODUCTION_DECISIONS_DC_ODM_DATABASE_SERVERNAME DNS Name or IP address for your DB2 Server No DNS or IP address CP4BA_DEPLOYMENT_PRODUCTION_DECISIONS = \"true\" CP4BA_DEPLOYMENT_PRODUCTION_DECISIONS_BUILD_DB = \"true\" CP4BA_DEPLOYMENT_PRODUCTION_DECISIONS_BUILD_LDAP = \"true\" CP4BA_DEPLOYMENT_PRODUCTION_DECISIONS_LDAP_SERVER = \"XXX.XXX.XXX.XXX\" CP4BA_DEPLOYMENT_PRODUCTION_DECISIONS_DC_ODM_DATABASE_SERVERNAME = \"XXX.XXX.XXX.XXX\"","title":"Decisions Production"},{"location":"Cloud-Paks/Business-Automation/#execute-service","text":"Run the following command to deploy the Cloud Pak for Business Automation services: /data/daffy/cp4ba/service.sh <ENVIRONMENT_NAME> HTML Video embed","title":"Execute Service"},{"location":"Cloud-Paks/Business-Automation/#step-3a-status","text":"The service can take a few hours to complete, based on which one you chose to deploy. To help monitor the status of the service deployment, you can run the help flag to see what flags you can use to get information on your service deployment. Run the following commands to check the Cloud Pak for Business Automation to see what command flags you can run: /data/daffy/cp4ba/service.sh <ENVIRONMENT_NAME> --help The following command will give you the status of all starter components for the service you deployed: /data/daffy/cp4ba/service.sh <ENVIRONMENT_NAME> --StarterStatus If you want to show just one status of starter service that you deployed for example CP4BA_DEPLOYMENT_STARTER_DECISIONS: /data/daffy/cp4ba/service.sh <ENVIRONMENT_NAME> --StarterStatus decisions The following command will give you the status of all Production components for the service you deployed: /data/daffy/cp4ba/service.sh <ENVIRONMENT_NAME> --Status The following command will give you the status of RPA Server you deployed: /data/daffy/cp4ba/service.sh <ENVIRONMENT_NAME> --RPAStatus If you want to have a running job to refresh every few seconds, you can run the above command via the watch command: watch -c /data/daffy/cp4ba/service.sh <ENVIRONMENT_NAME> --StarterStatus To find out the connection info to your new starter services, you can run the console flag to get user names, passwords, and URLs to connect to: /data/daffy/cp4ba/service.sh <ENVIRONMENT_NAME> --StarterConsole If you want to show just one console of starter service that you deployed for example CP4BA_DEPLOYMENT_STARTER_DECISIONS: /data/daffy/cp4ba/service.sh <ENVIRONMENT_NAME> --StarterConsole decisions To find out the connection info to your new Production services, you can run the console flag to get user names, passwords, and URLs to connect to: /data/daffy/cp4ba/service.sh <ENVIRONMENT_NAME> --Console To find out the connection info to your RPA Server , you can run the console flag to get user names, passwords, and URLs to connect to: /data/daffy/cp4ba/service.sh <ENVIRONMENT_NAME> --RPAConsole HTML Video embed","title":"Step 3a: Status"},{"location":"Cloud-Paks/Business-Automation/#post-daffy-steps","text":"","title":"Post Daffy Steps"},{"location":"Cloud-Paks/Business-Automation/#rpa-server_2","text":"","title":"RPA Server"},{"location":"Cloud-Paks/Business-Automation/#openldap-config","text":"Once you have installed RPA server, you will need add the LDAP Server from the Cloud Pak Dashboard. The following steps will help you manually preform these steps. The details for the next steps will come when you install Step 3 of Daffy for RPA Server, via your command line console. Screenshot 1) Login Cloud Pak Dashboard Link via \" IBM provided credentials(admin only) \" Screenshot 2) From the hamburger menu bar, under Administration, select Access Control Screenshot 3) Click \" Add users\" Screenshot 4) Add your RPA user and give Access to all roles. 5) Logout of the Cloud Pak dashboard and close your browser. At this point, you are ready to logon to your RPA Server Console.","title":"OpenLdap Config"},{"location":"Cloud-Paks/Business-Automation/#decisions-server","text":"Once you have installed Production Decisions Server pattern, you will need to do a few steps. Map Your LDAP Groups to IDP Roles Install and Configure your Rule Designer","title":"Decisions Server"},{"location":"Cloud-Paks/Business-Automation/#map-ldap-groups-to-roles","text":"Run the following command to Import and Map your LDAP groups to Zen roles /data/daffy/cp4ba/service.sh <env_name> --decisionImportLDAPGroups Screenshot","title":"Map LDAP groups to Roles"},{"location":"Cloud-Paks/Business-Automation/#rule-designer","text":"After you installed Decisions Services, you need to install and connect Rule Designer to your new instance. For the next steps, any information you need from your environment you should be able to collect from the service.sh --console command output of Daffy. Important Original Instructions can be found here 1) Download and install Eclipse. Download Eclipse Screenshot 2) Install ODM from Marketplace. a. Start Eclipse. Click Help > Eclipse Marketplace . b. In the Find field, enter the text ODM and click Go. c. Locate the entry IBM Operational Decision Manager for Developers v8.11.0 - Rule Designer that matches the version to install, and then click Install . Screenshot 3) Download truststore.jks from your cluster Screenshot 4) Update your Eclipse.ini and add these lines at the end:(update path info based on your setup) - Djavax.net.ssl.trustStore = C :/ Users / Administrator / Desktop / MyTrustStores / truststore.jks - Djavax.net.ssl.trustStorePassword = changeit 5) Get the Zen Key API from the CPD console Important Before the Zen API Key can be generated, you must Map LDAP groups to Roles from above and then via Browser, logon to the Cloud Pak Decisions Desktop once. Screenshot 6) Connect Rule Designer","title":"Rule Designer"},{"location":"Cloud-Paks/Data/","text":"document.title = \"Cloud Pak - Data\"; Cloud Pak for Data \u00b6 At this point, you have a working OCP cluster on your platform of choice. Your < ENVIRONMENT_NAME >-env.sh configuration file will contain details of the platform and OCP installation. You will now add the following details to your env file: 1) The Cloud Pak info that you wish to install 2) The services that you wish to install on the Cloud Pak Step 2: Deploy Cloud Pak \u00b6 Deploying the Cloud Pak for Data requires one entry to your environment file (/data/daffy/env/< ENVIRONMENT_NAME >-env.sh). CP4D_VERSION= You can copy the following to your < ENVIRONMENT_NAME >-env.sh: CP4D_VERSION = \"4.6.3\" With these values, the Daffy engine will be able to install the version of Cloud Pak for Data and prepare for the desired services. CP4D Supported Version OCP Versions 4.6.3 4.10 4.6.2 4.10 4.6.1 4.10 4.6.0 4.10 Run the following command to deploy the Cloud Pak for Data: /data/daffy/cp4d/build.sh <ENVIRONMENT_NAME> When this step is complete, approximately after 60 minutes depending on your environment, you will have the Cloud Pak running. These are just the core Cloud Pak operators, no service/pattern is running at this point. The cluster is now ready to deploy the services/patterns. At this stage, the cluster consists of bedrock operators and the Cloud Pak for Data operators in the following projects: cpd-instance cpd-operators ibm-common-services Step 3: Deploy Services \u00b6 Set the flags in your environment file (< ENVIRONMENT_NAME >-env.sh) for the CP4D services you wish to deploy. Variable Name Value's Info Required CP4D_ENABLE_SERVICE_WKS true / false Watson Knowledge Studio No CP4D_ENABLE_SERVICE_WKC true / false Watson Knowledge Catalog No CP4D_ENABLE_SERVICE_DV true / false Data Virtualization No CP4D_ENABLE_SERVICE_WS true / false Watson Studio No CP4D_ENABLE_SERVICE_SPSS true / false Statistical Package for Social Sciences No CP4D_ENABLE_SERVICE_WML true / false Watson Machine Learning No CP4D_ENABLE_SERVICE_DATASTAGE true / false DataStage No CP4D_ENABLE_SERVICE_DODS true / false Decision Optimization No CP4D_ENABLE_SERVICE_DMC true / false DB2 Management Console No CP4D_ENABLE_SERVICE_COGNOS true / false Cognos No CP4D_ENABLE_SERVICE_MATCH_360 true / false Match 360 No CP4D_ENABLE_SERVICE_OPENPAGES true / false Open Pages No CP4D_ENABLE_SERVICE_ANALYTICS_ENGINE true / false Analytics Engine powered by Apache Spark No CP4D_ENABLE_SERVICE_DB2_WAREHOUSE true / false DB2 Warehouse No CP4D_ENABLE_SERVICE_DATAPRIVACY true / false Data Privacy No CP4D_ENABLE_SERVICE_COGNOS_ANALYTICS true / false Cognos Analytics No CP4D_ENABLE_SERVICE_DB2 true / false DB2 OLTP No CP4D_ENABLE_SERVICE_WATSON_OPENSCALE true / false Watson OpenScale No CP4D_ENABLE_SERVICE_WS_PIPELINES true / false Watson Pipelines No You can copy the following to your < ENVIRONMENT_NAME >-env.sh: CP4D_ENABLE_SERVICE_WKS = \"false\" CP4D_ENABLE_SERVICE_WKC = \"false\" CP4D_ENABLE_SERVICE_DV = \"false\" CP4D_ENABLE_SERVICE_SPSS = \"false\" CP4D_ENABLE_SERVICE_WS = \"false\" CP4D_ENABLE_SERVICE_WML = \"false\" CP4D_ENABLE_SERVICE_DATASTAGE = \"false\" CP4D_ENABLE_SERVICE_DODS = \"false\" CP4D_ENABLE_SERVICE_DMC = \"false\" CP4D_ENABLE_SERVICE_COGNOS_DASHBOARDS = \"false\" CP4D_ENABLE_SERVICE_MATCH_360 = \"false\" CP4D_ENABLE_SERVICE_OPENPAGES = \"false\" CP4D_ENABLE_SERVICE_ANALYTICS_ENGINE = \"false\" CP4D_ENABLE_SERVICE_DB2_WAREHOUSE = \"false\" CP4D_ENABLE_SERVICE_DATAPRIVACY = \"false\" CP4D_ENABLE_SERVICE_COGNOS_ANALYTICS = \"false\" CP4D_ENABLE_SERVICE_DB2 = \"false\" CP4D_ENABLE_SERVICE_WATSON_OPENSCALE = \"false\" CP4D_ENABLE_SERVICE_WS_PIPELINES = \"false\" Run the following command to deploy the Cloud Pak for Data services: /data/daffy/cp4d/service.sh <ENVIRONMENT_NAME> Step 3a: Status \u00b6 The service can take a few hours to complete, based on which one you chose to deploy. To help monitor the status of the service deployment you can run the --help flag to see what flags you can use to get information on your service deployment. Run the following commands to check the Cloud Pak for Data to see what command flags you can run: /data/daffy/cp4d/service.sh <ENVIRONMENT_NAME> --help The following command will give you the status of all components for the pattern you deployed: /data/daffy/cp4d/service.sh <ENVIRONMENT_NAME> --AllStatus If you want to have a running job to refresh every few seconds, you can run the above command via the watch command: watch -c /data/daffy/cp4d/service.sh <ENVIRONMENT_NAME> --AllStatus If you want to want to see more detail status on an individual service, you can run each service status: /data/daffy/cp4d/service.sh <ENVIRONMENT_NAME> --WKCStatus /data/daffy/cp4d/service.sh <ENVIRONMENT_NAME> --WKSStatus /data/daffy/cp4d/service.sh <ENVIRONMENT_NAME> --WSStatus /data/daffy/cp4d/service.sh <ENVIRONMENT_NAME> --DVStatus /data/daffy/cp4d/service.sh <ENVIRONMENT_NAME> --WMLStatus /data/daffy/cp4d/service.sh <ENVIRONMENT_NAME> --SPSSStatus /data/daffy/cp4d/service.sh <ENVIRONMENT_NAME> --DataStageStatus /data/daffy/cp4d/service.sh <ENVIRONMENT_NAME> --DODSStatus /data/daffy/cp4d/service.sh <ENVIRONMENT_NAME> --Match360Status /data/daffy/cp4d/service.sh <ENVIRONMENT_NAME> --OpenPagesStatus /data/daffy/cp4d/service.sh <ENVIRONMENT_NAME> --AnalyticsEngineStatus /data/daffy/cp4d/service.sh <ENVIRONMENT_NAME> --DB2WarehouseStatus /data/daffy/cp4d/service.sh <ENVIRONMENT_NAME> --DataPrivacyStatus /data/daffy/cp4d/service.sh <ENVIRONMENT_NAME> --CognosAnalyticsStatus /data/daffy/cp4d/service.sh <ENVIRONMENT_NAME> --DB2Status /data/daffy/cp4d/service.sh <ENVIRONMENT_NAME> --OpenscaleStatus /data/daffy/cp4d/service.sh <ENVIRONMENT_NAME> --WSPipelinesStatus /data/daffy/cp4d/build.sh <ENVIRONMENT_NAME> --Console","title":"Data"},{"location":"Cloud-Paks/Data/#cloud-pak-for-data","text":"At this point, you have a working OCP cluster on your platform of choice. Your < ENVIRONMENT_NAME >-env.sh configuration file will contain details of the platform and OCP installation. You will now add the following details to your env file: 1) The Cloud Pak info that you wish to install 2) The services that you wish to install on the Cloud Pak","title":"Cloud Pak for Data"},{"location":"Cloud-Paks/Data/#step-2-deploy-cloud-pak","text":"Deploying the Cloud Pak for Data requires one entry to your environment file (/data/daffy/env/< ENVIRONMENT_NAME >-env.sh). CP4D_VERSION= You can copy the following to your < ENVIRONMENT_NAME >-env.sh: CP4D_VERSION = \"4.6.3\" With these values, the Daffy engine will be able to install the version of Cloud Pak for Data and prepare for the desired services. CP4D Supported Version OCP Versions 4.6.3 4.10 4.6.2 4.10 4.6.1 4.10 4.6.0 4.10 Run the following command to deploy the Cloud Pak for Data: /data/daffy/cp4d/build.sh <ENVIRONMENT_NAME> When this step is complete, approximately after 60 minutes depending on your environment, you will have the Cloud Pak running. These are just the core Cloud Pak operators, no service/pattern is running at this point. The cluster is now ready to deploy the services/patterns. At this stage, the cluster consists of bedrock operators and the Cloud Pak for Data operators in the following projects: cpd-instance cpd-operators ibm-common-services","title":"Step 2: Deploy Cloud Pak"},{"location":"Cloud-Paks/Data/#step-3-deploy-services","text":"Set the flags in your environment file (< ENVIRONMENT_NAME >-env.sh) for the CP4D services you wish to deploy. Variable Name Value's Info Required CP4D_ENABLE_SERVICE_WKS true / false Watson Knowledge Studio No CP4D_ENABLE_SERVICE_WKC true / false Watson Knowledge Catalog No CP4D_ENABLE_SERVICE_DV true / false Data Virtualization No CP4D_ENABLE_SERVICE_WS true / false Watson Studio No CP4D_ENABLE_SERVICE_SPSS true / false Statistical Package for Social Sciences No CP4D_ENABLE_SERVICE_WML true / false Watson Machine Learning No CP4D_ENABLE_SERVICE_DATASTAGE true / false DataStage No CP4D_ENABLE_SERVICE_DODS true / false Decision Optimization No CP4D_ENABLE_SERVICE_DMC true / false DB2 Management Console No CP4D_ENABLE_SERVICE_COGNOS true / false Cognos No CP4D_ENABLE_SERVICE_MATCH_360 true / false Match 360 No CP4D_ENABLE_SERVICE_OPENPAGES true / false Open Pages No CP4D_ENABLE_SERVICE_ANALYTICS_ENGINE true / false Analytics Engine powered by Apache Spark No CP4D_ENABLE_SERVICE_DB2_WAREHOUSE true / false DB2 Warehouse No CP4D_ENABLE_SERVICE_DATAPRIVACY true / false Data Privacy No CP4D_ENABLE_SERVICE_COGNOS_ANALYTICS true / false Cognos Analytics No CP4D_ENABLE_SERVICE_DB2 true / false DB2 OLTP No CP4D_ENABLE_SERVICE_WATSON_OPENSCALE true / false Watson OpenScale No CP4D_ENABLE_SERVICE_WS_PIPELINES true / false Watson Pipelines No You can copy the following to your < ENVIRONMENT_NAME >-env.sh: CP4D_ENABLE_SERVICE_WKS = \"false\" CP4D_ENABLE_SERVICE_WKC = \"false\" CP4D_ENABLE_SERVICE_DV = \"false\" CP4D_ENABLE_SERVICE_SPSS = \"false\" CP4D_ENABLE_SERVICE_WS = \"false\" CP4D_ENABLE_SERVICE_WML = \"false\" CP4D_ENABLE_SERVICE_DATASTAGE = \"false\" CP4D_ENABLE_SERVICE_DODS = \"false\" CP4D_ENABLE_SERVICE_DMC = \"false\" CP4D_ENABLE_SERVICE_COGNOS_DASHBOARDS = \"false\" CP4D_ENABLE_SERVICE_MATCH_360 = \"false\" CP4D_ENABLE_SERVICE_OPENPAGES = \"false\" CP4D_ENABLE_SERVICE_ANALYTICS_ENGINE = \"false\" CP4D_ENABLE_SERVICE_DB2_WAREHOUSE = \"false\" CP4D_ENABLE_SERVICE_DATAPRIVACY = \"false\" CP4D_ENABLE_SERVICE_COGNOS_ANALYTICS = \"false\" CP4D_ENABLE_SERVICE_DB2 = \"false\" CP4D_ENABLE_SERVICE_WATSON_OPENSCALE = \"false\" CP4D_ENABLE_SERVICE_WS_PIPELINES = \"false\" Run the following command to deploy the Cloud Pak for Data services: /data/daffy/cp4d/service.sh <ENVIRONMENT_NAME>","title":"Step 3: Deploy Services"},{"location":"Cloud-Paks/Data/#step-3a-status","text":"The service can take a few hours to complete, based on which one you chose to deploy. To help monitor the status of the service deployment you can run the --help flag to see what flags you can use to get information on your service deployment. Run the following commands to check the Cloud Pak for Data to see what command flags you can run: /data/daffy/cp4d/service.sh <ENVIRONMENT_NAME> --help The following command will give you the status of all components for the pattern you deployed: /data/daffy/cp4d/service.sh <ENVIRONMENT_NAME> --AllStatus If you want to have a running job to refresh every few seconds, you can run the above command via the watch command: watch -c /data/daffy/cp4d/service.sh <ENVIRONMENT_NAME> --AllStatus If you want to want to see more detail status on an individual service, you can run each service status: /data/daffy/cp4d/service.sh <ENVIRONMENT_NAME> --WKCStatus /data/daffy/cp4d/service.sh <ENVIRONMENT_NAME> --WKSStatus /data/daffy/cp4d/service.sh <ENVIRONMENT_NAME> --WSStatus /data/daffy/cp4d/service.sh <ENVIRONMENT_NAME> --DVStatus /data/daffy/cp4d/service.sh <ENVIRONMENT_NAME> --WMLStatus /data/daffy/cp4d/service.sh <ENVIRONMENT_NAME> --SPSSStatus /data/daffy/cp4d/service.sh <ENVIRONMENT_NAME> --DataStageStatus /data/daffy/cp4d/service.sh <ENVIRONMENT_NAME> --DODSStatus /data/daffy/cp4d/service.sh <ENVIRONMENT_NAME> --Match360Status /data/daffy/cp4d/service.sh <ENVIRONMENT_NAME> --OpenPagesStatus /data/daffy/cp4d/service.sh <ENVIRONMENT_NAME> --AnalyticsEngineStatus /data/daffy/cp4d/service.sh <ENVIRONMENT_NAME> --DB2WarehouseStatus /data/daffy/cp4d/service.sh <ENVIRONMENT_NAME> --DataPrivacyStatus /data/daffy/cp4d/service.sh <ENVIRONMENT_NAME> --CognosAnalyticsStatus /data/daffy/cp4d/service.sh <ENVIRONMENT_NAME> --DB2Status /data/daffy/cp4d/service.sh <ENVIRONMENT_NAME> --OpenscaleStatus /data/daffy/cp4d/service.sh <ENVIRONMENT_NAME> --WSPipelinesStatus /data/daffy/cp4d/build.sh <ENVIRONMENT_NAME> --Console","title":"Step 3a: Status"},{"location":"Cloud-Paks/DataBeta/","text":"document.title = \"Cloud Pak - Data\"; Cloud Pak for Data \u00b6 At this point, you have a working OCP cluster on your platform of choice. Your < ENVIRONMENT_NAME >-env.sh configuration file will contain details of the platform and OCP installation. You will now add the following details to your env file: 1) The Cloud Pak info that you wish to install 2) The services that you wish to install on the Cloud Pak Step 2: Deploy Cloud Pak \u00b6 Deploying the Cloud Pak for Data requires one entry to your environment file (/data/daffy/env/< ENVIRONMENT_NAME >-env.sh). CP4D_VERSION= You can copy the following to your < ENVIRONMENT_NAME >-env.sh: CP4D_VERSION = \"4.5.3\" With these values, the Daffy engine will be able to install the version of Cloud Pak for Data and prepare for the desired services. CP4D Supported Version OCP Versions 4.5.3 4.8, 4.10 4.5.2 4.8, 4.10 4.5.1 4.8, 4.10 4.5.0 4.8, 4.10 Run the following command to deploy the Cloud Pak for Data: /data/daffy/cp4d/build.sh <ENVIRONMENT_NAME> When this step is complete, approximately after 60 minutes depending on your environment, you will have the Cloud Pak running. These are just the core Cloud Pak operators, no service/pattern is running at this point. The cluster is now ready to deploy the services/patterns. At this stage, the cluster consists of bedrock operators and the Cloud Pak for Data operators in the following projects: cpd-instance cpd-operators ibm-common-services Step 3: Deploy Services \u00b6 Set the flags in your environment file (< ENVIRONMENT_NAME >-env.sh) for the CP4D services you wish to deploy. Variable Name Value's Info Required CP4D_ENABLE_SERVICE_WKS true / false Watson Knowledge Studio No CP4D_ENABLE_SERVICE_WKC true / false Watson Knowledge Catalog No CP4D_ENABLE_SERVICE_DV true / false Data Virtualization No CP4D_ENABLE_SERVICE_WS true / false Watson Studio No CP4D_ENABLE_SERVICE_SPSS true / false Statistical Package for Social Sciences No CP4D_ENABLE_SERVICE_WML true / false Watson Machine Learning No CP4D_ENABLE_SERVICE_DATASTAGE true / false DataStage No CP4D_ENABLE_SERVICE_DODS true / false Decision Optimization No CP4D_ENABLE_SERVICE_DMC true / false DB2 Management Console No CP4D_ENABLE_SERVICE_COGNOS true / false Cognos No CP4D_ENABLE_SERVICE_MATCH_360 true / false Match 360 No CP4D_ENABLE_SERVICE_OPENPAGES true / false Open Pages No CP4D_ENABLE_SERVICE_ANALYTICS_ENGINE true / false Analytics Engine powered by Apache Spark No CP4D_ENABLE_SERVICE_DB2_WAREHOUSE true / false DB2 Warehouse No CP4D_ENABLE_SERVICE_DATAPRIVACY true / false Data Privacy No CP4D_ENABLE_SERVICE_COGNOS_ANALYTICS true / false Cognos Analytics No CP4D_ENABLE_SERVICE_DB2 true / false DB2 OLTP No You can copy the following to your < ENVIRONMENT_NAME >-env.sh: CP4D_ENABLE_SERVICE_WKS = \"false\" CP4D_ENABLE_SERVICE_WKC = \"false\" CP4D_ENABLE_SERVICE_DV = \"false\" CP4D_ENABLE_SERVICE_SPSS = \"false\" CP4D_ENABLE_SERVICE_WS = \"false\" CP4D_ENABLE_SERVICE_WML = \"false\" CP4D_ENABLE_SERVICE_DATASTAGE = \"false\" CP4D_ENABLE_SERVICE_DODS = \"false\" CP4D_ENABLE_SERVICE_DMC = \"false\" CP4D_ENABLE_SERVICE_COGNOS_DASHBOARDS = \"false\" CP4D_ENABLE_SERVICE_MATCH_360 = \"false\" CP4D_ENABLE_SERVICE_OPENPAGES = \"false\" CP4D_ENABLE_SERVICE_ANALYTICS_ENGINE = \"false\" CP4D_ENABLE_SERVICE_DB2_WAREHOUSE = \"false\" CP4D_ENABLE_SERVICE_DATAPRIVACY = \"false\" CP4D_ENABLE_SERVICE_COGNOS_ANALYTICS = \"false\" CP4D_ENABLE_SERVICE_DB2 = \"false\" Run the following command to deploy the Cloud Pak for Data services: /data/daffy/cp4d/service.sh <ENVIRONMENT_NAME> Step 3a: Status \u00b6 The service can take a few hours to complete, based on which one you chose to deploy. To help monitor the status of the service deployment you can run the --help flag to see what flags you can use to get information on your service deployment. Run the following commands to check the Cloud Pak for Data to see what command flags you can run: /data/daffy/cp4d/service.sh <ENVIRONMENT_NAME> --help The following command will give you the status of all components for the pattern you deployed: /data/daffy/cp4d/service.sh <ENVIRONMENT_NAME> --AllStatus If you want to have a running job to refresh every few seconds, you can run the above command via the watch command: watch -c /data/daffy/cp4d/service.sh <ENVIRONMENT_NAME> --AllStatus If you want to want to see more detail status on an individual service, you can run each service status: /data/daffy/cp4d/service.sh <ENVIRONMENT_NAME> --WKCStatus /data/daffy/cp4d/service.sh <ENVIRONMENT_NAME> --WKSStatus /data/daffy/cp4d/service.sh <ENVIRONMENT_NAME> --WSStatus /data/daffy/cp4d/service.sh <ENVIRONMENT_NAME> --DVStatus /data/daffy/cp4d/service.sh <ENVIRONMENT_NAME> --WMLStatus /data/daffy/cp4d/service.sh <ENVIRONMENT_NAME> --SPSSStatus /data/daffy/cp4d/service.sh <ENVIRONMENT_NAME> --DataStageStatus /data/daffy/cp4d/service.sh <ENVIRONMENT_NAME> --DODSStatus /data/daffy/cp4d/service.sh <ENVIRONMENT_NAME> --Match360Status /data/daffy/cp4d/service.sh <ENVIRONMENT_NAME> --OpenPagesStatus /data/daffy/cp4d/service.sh <ENVIRONMENT_NAME> --AnalyticsEngineStatus /data/daffy/cp4d/service.sh <ENVIRONMENT_NAME> --DB2WarehouseStatus /data/daffy/cp4d/service.sh <ENVIRONMENT_NAME> --DataPrivacyStatus /data/daffy/cp4d/service.sh <ENVIRONMENT_NAME> --CognosAnalyticsStatus /data/daffy/cp4d/service.sh <ENVIRONMENT_NAME> --DB2Status /data/daffy/cp4d/build.sh <ENVIRONMENT_NAME> --Console","title":"DataBeta"},{"location":"Cloud-Paks/DataBeta/#cloud-pak-for-data","text":"At this point, you have a working OCP cluster on your platform of choice. Your < ENVIRONMENT_NAME >-env.sh configuration file will contain details of the platform and OCP installation. You will now add the following details to your env file: 1) The Cloud Pak info that you wish to install 2) The services that you wish to install on the Cloud Pak","title":"Cloud Pak for Data"},{"location":"Cloud-Paks/DataBeta/#step-2-deploy-cloud-pak","text":"Deploying the Cloud Pak for Data requires one entry to your environment file (/data/daffy/env/< ENVIRONMENT_NAME >-env.sh). CP4D_VERSION= You can copy the following to your < ENVIRONMENT_NAME >-env.sh: CP4D_VERSION = \"4.5.3\" With these values, the Daffy engine will be able to install the version of Cloud Pak for Data and prepare for the desired services. CP4D Supported Version OCP Versions 4.5.3 4.8, 4.10 4.5.2 4.8, 4.10 4.5.1 4.8, 4.10 4.5.0 4.8, 4.10 Run the following command to deploy the Cloud Pak for Data: /data/daffy/cp4d/build.sh <ENVIRONMENT_NAME> When this step is complete, approximately after 60 minutes depending on your environment, you will have the Cloud Pak running. These are just the core Cloud Pak operators, no service/pattern is running at this point. The cluster is now ready to deploy the services/patterns. At this stage, the cluster consists of bedrock operators and the Cloud Pak for Data operators in the following projects: cpd-instance cpd-operators ibm-common-services","title":"Step 2: Deploy Cloud Pak"},{"location":"Cloud-Paks/DataBeta/#step-3-deploy-services","text":"Set the flags in your environment file (< ENVIRONMENT_NAME >-env.sh) for the CP4D services you wish to deploy. Variable Name Value's Info Required CP4D_ENABLE_SERVICE_WKS true / false Watson Knowledge Studio No CP4D_ENABLE_SERVICE_WKC true / false Watson Knowledge Catalog No CP4D_ENABLE_SERVICE_DV true / false Data Virtualization No CP4D_ENABLE_SERVICE_WS true / false Watson Studio No CP4D_ENABLE_SERVICE_SPSS true / false Statistical Package for Social Sciences No CP4D_ENABLE_SERVICE_WML true / false Watson Machine Learning No CP4D_ENABLE_SERVICE_DATASTAGE true / false DataStage No CP4D_ENABLE_SERVICE_DODS true / false Decision Optimization No CP4D_ENABLE_SERVICE_DMC true / false DB2 Management Console No CP4D_ENABLE_SERVICE_COGNOS true / false Cognos No CP4D_ENABLE_SERVICE_MATCH_360 true / false Match 360 No CP4D_ENABLE_SERVICE_OPENPAGES true / false Open Pages No CP4D_ENABLE_SERVICE_ANALYTICS_ENGINE true / false Analytics Engine powered by Apache Spark No CP4D_ENABLE_SERVICE_DB2_WAREHOUSE true / false DB2 Warehouse No CP4D_ENABLE_SERVICE_DATAPRIVACY true / false Data Privacy No CP4D_ENABLE_SERVICE_COGNOS_ANALYTICS true / false Cognos Analytics No CP4D_ENABLE_SERVICE_DB2 true / false DB2 OLTP No You can copy the following to your < ENVIRONMENT_NAME >-env.sh: CP4D_ENABLE_SERVICE_WKS = \"false\" CP4D_ENABLE_SERVICE_WKC = \"false\" CP4D_ENABLE_SERVICE_DV = \"false\" CP4D_ENABLE_SERVICE_SPSS = \"false\" CP4D_ENABLE_SERVICE_WS = \"false\" CP4D_ENABLE_SERVICE_WML = \"false\" CP4D_ENABLE_SERVICE_DATASTAGE = \"false\" CP4D_ENABLE_SERVICE_DODS = \"false\" CP4D_ENABLE_SERVICE_DMC = \"false\" CP4D_ENABLE_SERVICE_COGNOS_DASHBOARDS = \"false\" CP4D_ENABLE_SERVICE_MATCH_360 = \"false\" CP4D_ENABLE_SERVICE_OPENPAGES = \"false\" CP4D_ENABLE_SERVICE_ANALYTICS_ENGINE = \"false\" CP4D_ENABLE_SERVICE_DB2_WAREHOUSE = \"false\" CP4D_ENABLE_SERVICE_DATAPRIVACY = \"false\" CP4D_ENABLE_SERVICE_COGNOS_ANALYTICS = \"false\" CP4D_ENABLE_SERVICE_DB2 = \"false\" Run the following command to deploy the Cloud Pak for Data services: /data/daffy/cp4d/service.sh <ENVIRONMENT_NAME>","title":"Step 3: Deploy Services"},{"location":"Cloud-Paks/DataBeta/#step-3a-status","text":"The service can take a few hours to complete, based on which one you chose to deploy. To help monitor the status of the service deployment you can run the --help flag to see what flags you can use to get information on your service deployment. Run the following commands to check the Cloud Pak for Data to see what command flags you can run: /data/daffy/cp4d/service.sh <ENVIRONMENT_NAME> --help The following command will give you the status of all components for the pattern you deployed: /data/daffy/cp4d/service.sh <ENVIRONMENT_NAME> --AllStatus If you want to have a running job to refresh every few seconds, you can run the above command via the watch command: watch -c /data/daffy/cp4d/service.sh <ENVIRONMENT_NAME> --AllStatus If you want to want to see more detail status on an individual service, you can run each service status: /data/daffy/cp4d/service.sh <ENVIRONMENT_NAME> --WKCStatus /data/daffy/cp4d/service.sh <ENVIRONMENT_NAME> --WKSStatus /data/daffy/cp4d/service.sh <ENVIRONMENT_NAME> --WSStatus /data/daffy/cp4d/service.sh <ENVIRONMENT_NAME> --DVStatus /data/daffy/cp4d/service.sh <ENVIRONMENT_NAME> --WMLStatus /data/daffy/cp4d/service.sh <ENVIRONMENT_NAME> --SPSSStatus /data/daffy/cp4d/service.sh <ENVIRONMENT_NAME> --DataStageStatus /data/daffy/cp4d/service.sh <ENVIRONMENT_NAME> --DODSStatus /data/daffy/cp4d/service.sh <ENVIRONMENT_NAME> --Match360Status /data/daffy/cp4d/service.sh <ENVIRONMENT_NAME> --OpenPagesStatus /data/daffy/cp4d/service.sh <ENVIRONMENT_NAME> --AnalyticsEngineStatus /data/daffy/cp4d/service.sh <ENVIRONMENT_NAME> --DB2WarehouseStatus /data/daffy/cp4d/service.sh <ENVIRONMENT_NAME> --DataPrivacyStatus /data/daffy/cp4d/service.sh <ENVIRONMENT_NAME> --CognosAnalyticsStatus /data/daffy/cp4d/service.sh <ENVIRONMENT_NAME> --DB2Status /data/daffy/cp4d/build.sh <ENVIRONMENT_NAME> --Console","title":"Step 3a: Status"},{"location":"Cloud-Paks/Integration/","text":"document.title = \"Cloud Pak - Integration\"; Cloud Pak for Integration \u00b6 At this point, you have a working OCP cluster on your platform of choice. Your < ENVIRONMENT_NAME >-env.sh configuration file will contain details of the platform and OCP installation. You will now add to this file, the details of: 1) The Cloud Pak info that you wish to install 2) The services that you wish to install on the Cloud Pak Step 2: Deploy Cloud Pak \u00b6 Deploying the Cloud Pak for Integration only requires one entry to your environment file (/data/daffy/env/< ENVIRONMENT_NAME >-env.sh) CP4I_VERSION= You can copy the following to your < ENVIRONMENT_NAME >-env.sh: CP4I_VERSION = \"2022.4.1\" With this one value, the Daffy engine will be able to install the version of Cloud Pak for Integration and the Platform Navigator. The service consist of the following products: platform navigator Integration Supported Version OCP Versions 2022.4.1 4.10 2022.2.1 4.10 Run the following command to deploy the Cloud Pak for Integration: /data/daffy/cp4i/build.sh <ENVIRONMENT_NAME> When this step is complete, up to an hour depending on your environment, you will have the Cloud Pak running. This will install all of the Cloud Pak operators including foundational services and the Platform Navigator. The cluster is now ready to deploy additional services/patterns. At this stage, the cluster consists of common services and the Cloud Pak for Integration operators and some services in the following projects: cp4i ibm-common-services Step 3: Deploy Services \u00b6 Deploying services within the Cloud Pak for Integration requires you to set the flags within the environment file (< ENVIRONMENT_NAME >-env.sh). With these values, the Daffy engine will be able to install the version of Cloud Pak for Integration and prepare for the desired services. Variable Name Value's Info Required CP4I_ENABLE_SERVICE_ACEDESIGN true / false App Connect Designer No CP4I_ENABLE_SERVICE_ACEDASH true / false App Connect Dashboard No CP4I_ENABLE_SERVICE_ASSETREPO true / false Integration Asset Repository No CP4I_ENABLE_SERVICE_TRACING true / false Operations Dashboard Tracing No CP4I_ENABLE_SERVICE_MQSINGLE true / false Single Instance of MQ No CP4I_ENABLE_SERVICE_APIC true / false API Connect No CP4I_ENABLE_SERVICE_MQHA true / false Cloud Native MQ HA No CP4I_ENABLE_SERVICE_EVENTSTREAMS true / false Event Streams No Run the following command to deploy the Cloud Pak for Integration services: /data/daffy/cp4i/service.sh <ENVIRONMENT_NAME> Step 3a: Status \u00b6 The service can take a few hours to complete, based on which one you chose to deploy. To help monitor the status of the CP4I service deployment you can run the help flag to see what flags you can use to get information on your service deployment: /data/daffy/cp4i/service.sh <ENVIRONMENT_NAME> --help Run the following commands to check the Cloud Pak for Integration services installation progress: /data/daffy/cp4i/service.sh <ENVIRONMENT_NAME> --AllStatus If you want to want to see more detail status on an individual service, you can run each service status: /data/daffy/cp4i/service.sh <ENVIRONMENT_NAME> --AceDashStatus /data/daffy/cp4i/service.sh <ENVIRONMENT_NAME> --AceDesignStatus /data/daffy/cp4i/service.sh <ENVIRONMENT_NAME> --AssetRepoStatus /data/daffy/cp4i/service.sh <ENVIRONMENT_NAME> --TracingStatus /data/daffy/cp4i/service.sh <ENVIRONMENT_NAME> --MQSingleStatus /data/daffy/cp4i/service.sh <ENVIRONMENT_NAME> --APICStatus /data/daffy/cp4i/service.sh <ENVIRONMENT_NAME> --MQHAStatus /data/daffy/cp4i/service.sh <ENVIRONMENT_NAME> --EventStreamsStatus To find out the connection info to your Integration Platform Navigator instance, you can run the console flag to get user names, passwords, and URLs to connect to: /data/daffy/cp4i/build.sh <ENVIRONMENT_NAME> --console","title":"Integration"},{"location":"Cloud-Paks/Integration/#cloud-pak-for-integration","text":"At this point, you have a working OCP cluster on your platform of choice. Your < ENVIRONMENT_NAME >-env.sh configuration file will contain details of the platform and OCP installation. You will now add to this file, the details of: 1) The Cloud Pak info that you wish to install 2) The services that you wish to install on the Cloud Pak","title":"Cloud Pak for Integration"},{"location":"Cloud-Paks/Integration/#step-2-deploy-cloud-pak","text":"Deploying the Cloud Pak for Integration only requires one entry to your environment file (/data/daffy/env/< ENVIRONMENT_NAME >-env.sh) CP4I_VERSION= You can copy the following to your < ENVIRONMENT_NAME >-env.sh: CP4I_VERSION = \"2022.4.1\" With this one value, the Daffy engine will be able to install the version of Cloud Pak for Integration and the Platform Navigator. The service consist of the following products: platform navigator Integration Supported Version OCP Versions 2022.4.1 4.10 2022.2.1 4.10 Run the following command to deploy the Cloud Pak for Integration: /data/daffy/cp4i/build.sh <ENVIRONMENT_NAME> When this step is complete, up to an hour depending on your environment, you will have the Cloud Pak running. This will install all of the Cloud Pak operators including foundational services and the Platform Navigator. The cluster is now ready to deploy additional services/patterns. At this stage, the cluster consists of common services and the Cloud Pak for Integration operators and some services in the following projects: cp4i ibm-common-services","title":"Step 2: Deploy Cloud Pak"},{"location":"Cloud-Paks/Integration/#step-3-deploy-services","text":"Deploying services within the Cloud Pak for Integration requires you to set the flags within the environment file (< ENVIRONMENT_NAME >-env.sh). With these values, the Daffy engine will be able to install the version of Cloud Pak for Integration and prepare for the desired services. Variable Name Value's Info Required CP4I_ENABLE_SERVICE_ACEDESIGN true / false App Connect Designer No CP4I_ENABLE_SERVICE_ACEDASH true / false App Connect Dashboard No CP4I_ENABLE_SERVICE_ASSETREPO true / false Integration Asset Repository No CP4I_ENABLE_SERVICE_TRACING true / false Operations Dashboard Tracing No CP4I_ENABLE_SERVICE_MQSINGLE true / false Single Instance of MQ No CP4I_ENABLE_SERVICE_APIC true / false API Connect No CP4I_ENABLE_SERVICE_MQHA true / false Cloud Native MQ HA No CP4I_ENABLE_SERVICE_EVENTSTREAMS true / false Event Streams No Run the following command to deploy the Cloud Pak for Integration services: /data/daffy/cp4i/service.sh <ENVIRONMENT_NAME>","title":"Step 3: Deploy Services"},{"location":"Cloud-Paks/Integration/#step-3a-status","text":"The service can take a few hours to complete, based on which one you chose to deploy. To help monitor the status of the CP4I service deployment you can run the help flag to see what flags you can use to get information on your service deployment: /data/daffy/cp4i/service.sh <ENVIRONMENT_NAME> --help Run the following commands to check the Cloud Pak for Integration services installation progress: /data/daffy/cp4i/service.sh <ENVIRONMENT_NAME> --AllStatus If you want to want to see more detail status on an individual service, you can run each service status: /data/daffy/cp4i/service.sh <ENVIRONMENT_NAME> --AceDashStatus /data/daffy/cp4i/service.sh <ENVIRONMENT_NAME> --AceDesignStatus /data/daffy/cp4i/service.sh <ENVIRONMENT_NAME> --AssetRepoStatus /data/daffy/cp4i/service.sh <ENVIRONMENT_NAME> --TracingStatus /data/daffy/cp4i/service.sh <ENVIRONMENT_NAME> --MQSingleStatus /data/daffy/cp4i/service.sh <ENVIRONMENT_NAME> --APICStatus /data/daffy/cp4i/service.sh <ENVIRONMENT_NAME> --MQHAStatus /data/daffy/cp4i/service.sh <ENVIRONMENT_NAME> --EventStreamsStatus To find out the connection info to your Integration Platform Navigator instance, you can run the console flag to get user names, passwords, and URLs to connect to: /data/daffy/cp4i/build.sh <ENVIRONMENT_NAME> --console","title":"Step 3a: Status"},{"location":"Cloud-Paks/Watson-AIOPS/","text":"document.title = \"Cloud Pak - Watson AIOPS\"; Cloud Pak for Watson AI Ops \u00b6 At this point, you should have a working OCP cluster on your platform of choice. Your < ENVIRONMENT_NAME >-env.sh configuration file will contain details of the platform and OCP installation. To install CP4WAIOPS, you must add some additional information to the env file. Below is a description of the information, you wll need to add. 1) Version of the CP4WAIOPS product to install 2) The CP4WAIOPS services that you wish to install Daffy automation scripts currently support the deployment of AI Manager - (This is installed with the cp4waiops/build.sh script) Event Manager - This is an optional component that can be installed with the service.sh scrpt. You must set the install fag to true before you run the service.sh script. Note The Daffy Event Manager deployment script ONLY installs the operator. You must configure and deploy the custom resource. Infrastructure Automation - This is an optional component that can be installed with the service.sh scrpt. You must set the install fag to true before you run the service.sh script. Step 2: Deploy Cloud Pak for WAIOPS + AI Manager \u00b6 Deploying the Cloud Pak for Watson AIOps only requires one entry to your environment file (/data/daffy/env/< ENVIRONMENT_NAME >-env.sh) CP4WAIOPS_VERSION= You can copy the following to your < ENVIRONMENT_NAME >-env.sh: CP4WAIOPS_VERSION = \"3.6.0\" With this one value, the Daffy engine will be able to install the version of Cloud Pak for Watson AI Ops and the Platform Navigator. Along with the base cloud pak components, the AI Manager will be installed. The service consist of the following products: AI Manager AIOps Supported Version OCP Versions 3.6.2 4.10 3.6.1 4.10 3.6.0 4.10 3.5.1 4.10 3.5.0 4.10 Run the following command to deploy the Cloud Pak for Watson AIOps + AI Manger: /data/daffy/cp4waiops/build.sh <ENVIRONMENT_NAME> When this step is complete, up to an hour depending on your environment, you have the Cloud Pak running. This will install all of the Cloud Pak operators including foundational services and the Platform Navigator. The cluster is now ready to deploy additional services and or demos. At this stage, the cluster consists of common services and the Cloud Pak for Watson AIOps operators and some services in the following projects: cp4waiops ibm-common-services Warning Occasionally you may see the following error message, which is usually not a big concern. We have noticed that in some cases ( primarily on ROKS when doing an all in one deployment ) the install of the event manager will take longer than normal to deploy. In this case you may see a message like this below. If that happens, please give some additional time (usually no more than 30 minutes) to verify your installation. Run the --console command after 30 minutes to show you the login information. Details of the --console command are below. Step 3: Deploy Cloud Pak for WAIOPS Optional Services \u00b6 There are 2 services that can be optionally installed after you have the Cloud Pak for Watson AIOPS installed. Event Manager (Operator ONLY) Infrastructure Automation The Event Manager for WatsonAIOps is an optional service deployment that can be added to your WatsonAIOps Cloud Pak deployment. To deploy the Event Manager component of WatsonAIOps, you will need to set the flag within your environment file then run the service.sh script. Warning As of today, you can ONLY deploy the Event Manager service as an additional component to the Cloud Pak for Watson AIOps. Installing the Watson AIOps Cloud Pak will by default install the AI Manager component. It is not possible today to only install the Event Manager component without the AI Manager. Here is the flag that will need to be set to enable the deployment of Event Manager Operator: CP4WAIOPS_DEPLOY_EMGR=<true|false> Note POST EVENT MANAGER INSTALL STEPS The Daffy scripts for deployment of Watson AIOPS Event Manager will configure the subscription and deploy the event manager operator. You will need to configure the NOI (Event Manager) instance manually. This is because Event Manager can be configured to collect, consolidate, and correlate events and topology data from a multitude of sources, which may require additional parameters specific to your environment. This is a screen shot of what you will see after Daffy deploys the event manager operator. Please follow the instructions to complete the configuration of the Event Manager (NOI) instance. Here is the flag that will need to be set to enable the deployment of Infrastructure Automation : CP4WAIOPS_DEPLOY_IA=<true|false> Run the following command to deploy the Cloud Pak for Watson AIOps Optional Services: /data/daffy/cp4waiops/service.sh <ENVIRONMENT_NAME> When this step is complete you will have the Cloud Pak and the optional services running. Step 4: Status & Console \u00b6 The service can take a few hours to complete, based on which one you chose to deploy. To help monitor the status of the service/pattern deployment, you can run the help flag to see what flags you can use to get information on your service/pattern deployment: /data/daffy/cp4waiops/build.sh <ENVIRONMENT_NAME> --help Run the following commands to check the Cloud Pak for Watson AIOps installation progress: /data/daffy/cp4waiops/build.sh <ENVIRONMENT_NAME> --status If you want to have a running job to refresh every few seconds, you can run the status script using the watch command: watch -c /data/daffy/cp4waiops/build.sh <ENVIRONMENT_NAME> --status To find out the connection info to your new service/pattern, you can run the console flag to get user names, passwords and URLs to connect to: /data/daffy/cp4waiops/build.sh <ENVIRONMENT_NAME> --console","title":"Watson AIOPS"},{"location":"Cloud-Paks/Watson-AIOPS/#cloud-pak-for-watson-ai-ops","text":"At this point, you should have a working OCP cluster on your platform of choice. Your < ENVIRONMENT_NAME >-env.sh configuration file will contain details of the platform and OCP installation. To install CP4WAIOPS, you must add some additional information to the env file. Below is a description of the information, you wll need to add. 1) Version of the CP4WAIOPS product to install 2) The CP4WAIOPS services that you wish to install Daffy automation scripts currently support the deployment of AI Manager - (This is installed with the cp4waiops/build.sh script) Event Manager - This is an optional component that can be installed with the service.sh scrpt. You must set the install fag to true before you run the service.sh script. Note The Daffy Event Manager deployment script ONLY installs the operator. You must configure and deploy the custom resource. Infrastructure Automation - This is an optional component that can be installed with the service.sh scrpt. You must set the install fag to true before you run the service.sh script.","title":"Cloud Pak for Watson AI Ops"},{"location":"Cloud-Paks/Watson-AIOPS/#step-2-deploy-cloud-pak-for-waiops-ai-manager","text":"Deploying the Cloud Pak for Watson AIOps only requires one entry to your environment file (/data/daffy/env/< ENVIRONMENT_NAME >-env.sh) CP4WAIOPS_VERSION= You can copy the following to your < ENVIRONMENT_NAME >-env.sh: CP4WAIOPS_VERSION = \"3.6.0\" With this one value, the Daffy engine will be able to install the version of Cloud Pak for Watson AI Ops and the Platform Navigator. Along with the base cloud pak components, the AI Manager will be installed. The service consist of the following products: AI Manager AIOps Supported Version OCP Versions 3.6.2 4.10 3.6.1 4.10 3.6.0 4.10 3.5.1 4.10 3.5.0 4.10 Run the following command to deploy the Cloud Pak for Watson AIOps + AI Manger: /data/daffy/cp4waiops/build.sh <ENVIRONMENT_NAME> When this step is complete, up to an hour depending on your environment, you have the Cloud Pak running. This will install all of the Cloud Pak operators including foundational services and the Platform Navigator. The cluster is now ready to deploy additional services and or demos. At this stage, the cluster consists of common services and the Cloud Pak for Watson AIOps operators and some services in the following projects: cp4waiops ibm-common-services Warning Occasionally you may see the following error message, which is usually not a big concern. We have noticed that in some cases ( primarily on ROKS when doing an all in one deployment ) the install of the event manager will take longer than normal to deploy. In this case you may see a message like this below. If that happens, please give some additional time (usually no more than 30 minutes) to verify your installation. Run the --console command after 30 minutes to show you the login information. Details of the --console command are below.","title":"Step 2: Deploy Cloud Pak for WAIOPS + AI Manager"},{"location":"Cloud-Paks/Watson-AIOPS/#step-3-deploy-cloud-pak-for-waiops-optional-services","text":"There are 2 services that can be optionally installed after you have the Cloud Pak for Watson AIOPS installed. Event Manager (Operator ONLY) Infrastructure Automation The Event Manager for WatsonAIOps is an optional service deployment that can be added to your WatsonAIOps Cloud Pak deployment. To deploy the Event Manager component of WatsonAIOps, you will need to set the flag within your environment file then run the service.sh script. Warning As of today, you can ONLY deploy the Event Manager service as an additional component to the Cloud Pak for Watson AIOps. Installing the Watson AIOps Cloud Pak will by default install the AI Manager component. It is not possible today to only install the Event Manager component without the AI Manager. Here is the flag that will need to be set to enable the deployment of Event Manager Operator: CP4WAIOPS_DEPLOY_EMGR=<true|false> Note POST EVENT MANAGER INSTALL STEPS The Daffy scripts for deployment of Watson AIOPS Event Manager will configure the subscription and deploy the event manager operator. You will need to configure the NOI (Event Manager) instance manually. This is because Event Manager can be configured to collect, consolidate, and correlate events and topology data from a multitude of sources, which may require additional parameters specific to your environment. This is a screen shot of what you will see after Daffy deploys the event manager operator. Please follow the instructions to complete the configuration of the Event Manager (NOI) instance. Here is the flag that will need to be set to enable the deployment of Infrastructure Automation : CP4WAIOPS_DEPLOY_IA=<true|false> Run the following command to deploy the Cloud Pak for Watson AIOps Optional Services: /data/daffy/cp4waiops/service.sh <ENVIRONMENT_NAME> When this step is complete you will have the Cloud Pak and the optional services running.","title":"Step 3: Deploy Cloud Pak for WAIOPS Optional Services"},{"location":"Cloud-Paks/Watson-AIOPS/#step-4-status-console","text":"The service can take a few hours to complete, based on which one you chose to deploy. To help monitor the status of the service/pattern deployment, you can run the help flag to see what flags you can use to get information on your service/pattern deployment: /data/daffy/cp4waiops/build.sh <ENVIRONMENT_NAME> --help Run the following commands to check the Cloud Pak for Watson AIOps installation progress: /data/daffy/cp4waiops/build.sh <ENVIRONMENT_NAME> --status If you want to have a running job to refresh every few seconds, you can run the status script using the watch command: watch -c /data/daffy/cp4waiops/build.sh <ENVIRONMENT_NAME> --status To find out the connection info to your new service/pattern, you can run the console flag to get user names, passwords and URLs to connect to: /data/daffy/cp4waiops/build.sh <ENVIRONMENT_NAME> --console","title":"Step 4: Status &amp; Console"},{"location":"Cloud-Paks/WebSphere-Automation/","text":"document.title = \"Cloud Pak - WebSphere Automation\"; Cloud Pak for WebSphere Automation \u00b6 At this point, you have a working OCP cluster on your platform of choice. Your < ENVIRONMENT_NAME >-env.sh configuration file will contain details of the platform and OCP installation. You will now add to this file, the details of: 1) The Cloud Pak info that you wish to install 2) The services that you wish to install on the Cloud Pak Step 2: Deploy WSA \u00b6 Deploying WebSphere Automation only requires one entry to your environment file (/data/daffy/env/< ENVIRONMENT_NAME >-env.sh) CPWSA_VERSION= You can copy the following to your < ENVIRONMENT_NAME >-env.sh: CPWSA_VERSION = \"1.4\" With this one value, the Daffy engine will be able to install the version of WebSphere Automation. The service consists of the following products: WebSphere Automation Small Profile (consists of): WebSphere Automation WebSphere Health WebSphere Secure WSA Supported Version OCP Versions 1.4 4.10 Run the following command to deploy WebSphere Automation: /data/daffy/wsa/build.sh <ENVIRONMENT_NAME> When this step is complete, up to an hour depending on your environment, you will have the basics of WebSphere Automation running. This will install all of the operators including foundational services. The cluster is now ready to deploy additional services and/or demos. At this stage, the cluster consists of common services and WebSphere Automation operators and some services in the following projects: websphere-automation ibm-common-services Step 3: Deploy Services \u00b6 Currently there is one service/demo for WebSphere Automation. We are adding new features on a regular basis, so please stay tuned. If you have a feature request for an additional service or demo, please fill out a request. Step 3a: Status \u00b6 The service can take a few hours to complete. To help monitor the status of the service/pattern deployment you can run the help flag to see what flags you can use to get information on your service/pattern deployment: /data/daffy/wsa/build.sh <ENVIRONMENT_NAME> --help Run the following commands to check the WebSphere Automation installation progress: /data/daffy/wsa/service.sh <ENVIRONMENT_NAME> --status If you want to have a running job to refresh every few seconds, you can run the status script using the watch command: watch -c /data/daffy/wsa/service.sh <ENVIRONMENT_NAME> --status To find out the connection info to your new service/pattern, you can run the console flag to get user names, passwords and URLs to connect to: /data/daffy/wsa/build.sh <ENVIRONMENT_NAME> --console","title":"WebSphere Automation"},{"location":"Cloud-Paks/WebSphere-Automation/#cloud-pak-for-websphere-automation","text":"At this point, you have a working OCP cluster on your platform of choice. Your < ENVIRONMENT_NAME >-env.sh configuration file will contain details of the platform and OCP installation. You will now add to this file, the details of: 1) The Cloud Pak info that you wish to install 2) The services that you wish to install on the Cloud Pak","title":"Cloud Pak for WebSphere Automation"},{"location":"Cloud-Paks/WebSphere-Automation/#step-2-deploy-wsa","text":"Deploying WebSphere Automation only requires one entry to your environment file (/data/daffy/env/< ENVIRONMENT_NAME >-env.sh) CPWSA_VERSION= You can copy the following to your < ENVIRONMENT_NAME >-env.sh: CPWSA_VERSION = \"1.4\" With this one value, the Daffy engine will be able to install the version of WebSphere Automation. The service consists of the following products: WebSphere Automation Small Profile (consists of): WebSphere Automation WebSphere Health WebSphere Secure WSA Supported Version OCP Versions 1.4 4.10 Run the following command to deploy WebSphere Automation: /data/daffy/wsa/build.sh <ENVIRONMENT_NAME> When this step is complete, up to an hour depending on your environment, you will have the basics of WebSphere Automation running. This will install all of the operators including foundational services. The cluster is now ready to deploy additional services and/or demos. At this stage, the cluster consists of common services and WebSphere Automation operators and some services in the following projects: websphere-automation ibm-common-services","title":"Step 2: Deploy WSA"},{"location":"Cloud-Paks/WebSphere-Automation/#step-3-deploy-services","text":"Currently there is one service/demo for WebSphere Automation. We are adding new features on a regular basis, so please stay tuned. If you have a feature request for an additional service or demo, please fill out a request.","title":"Step 3: Deploy Services"},{"location":"Cloud-Paks/WebSphere-Automation/#step-3a-status","text":"The service can take a few hours to complete. To help monitor the status of the service/pattern deployment you can run the help flag to see what flags you can use to get information on your service/pattern deployment: /data/daffy/wsa/build.sh <ENVIRONMENT_NAME> --help Run the following commands to check the WebSphere Automation installation progress: /data/daffy/wsa/service.sh <ENVIRONMENT_NAME> --status If you want to have a running job to refresh every few seconds, you can run the status script using the watch command: watch -c /data/daffy/wsa/service.sh <ENVIRONMENT_NAME> --status To find out the connection info to your new service/pattern, you can run the console flag to get user names, passwords and URLs to connect to: /data/daffy/wsa/build.sh <ENVIRONMENT_NAME> --console","title":"Step 3a: Status"},{"location":"Deploying-OCP/","text":"document.title = \"Deploy OCP\"; Deploying OCP \u00b6 IPI Installs AWS Azure GCP IBM VSphere MSP Installs ARO ROKS ROSA Other Installs HCCX Gym TechZone TechZoneTiles UPI Installs KVM VSphere","title":"Index"},{"location":"Deploying-OCP/#deploying-ocp","text":"IPI Installs AWS Azure GCP IBM VSphere MSP Installs ARO ROKS ROSA Other Installs HCCX Gym TechZone TechZoneTiles UPI Installs KVM VSphere","title":"Deploying OCP"},{"location":"Deploying-OCP/ARO/","text":"document.title = \"Deploy OCP - Azure ARO\"; Azure Install \u00b6 At this point, you have a bastion machine where you have installed the Daffy tool, and ready to created your core environment-name -env.sh and so you can execute the install of OCP on Azure via ARO. Platform Requirements \u00b6 To use Daffy on Azure ARO , there are some platform info and requirements that need to be gathered or met. We have a simple doc that you should refer to that list all providers and what would be needed. For Azure ARO , it breaks down to the following basic three items: Account Details - The account that you plan to install OpenShift Permissions - The permissions need to perform the install Quota - The ability to add new workload to that platform For detailed list of the above, you can find in the Daffy Provider Requirements. Please review before proceeding. https://ibm.box.com/v/DaffyProviderRequirements Finding Provider Details \u00b6 To install Daffy on Azure ARO, the hardest part can be finding the provider details. Subscription ID \u00b6 First find subscriptions in your account from the search box Screenshot Locate Subscriptions More Info Once you find the subscription you want to use, you can see the Subscription ID Screenshot Locate Subscription ID Tenant ID \u00b6 First you need to find the Active Direcotry for your account Screenshot Active Directory From your active directory, you can locate the Tenant ID Screenshot Tenant ID More Info Client ID \u00b6 First you need to find the Active Directory for your account Screenshot Active Directory Search for your appliation and from here you can find the client ID for the application you plan to use Screenshot Client ID More Info Region \u00b6 More Info To find the region name, you can use the above link to list all azure region names, make sure you pick one that has avaiblity zone support Screenshot Quota \u00b6 In your subscription, under settings, you can find Usage + Quotas Screenshot In the Qutoa section, you can filter by regtion and type. Then you can see your used and your max qutoa limits. Screenshot More Info Permission \u00b6 Within your Azure project, you would need to go to IAM Section and create/use Service Account. From the requirements doc , make sure your service account has the correct permissions. Look at the Azure section, it is same plus a few extra needed for ARO. Specifically Access to Active Directory. Environment File \u00b6 Deploying the OpenShift on Azure only requires three entries to your existing core environment file (< ENVIRONMENT_NAME >-env.sh) plus a local service account file. Note You can look in the samples directory on your bastion for example of ARO install : /data/daffy/env/samples/ aro-msp-env.sh You can copy the sample file to build your new environment file: cp / data / daffy / env / samples / aro - msp - env.sh / data / daffy / env /< ENVIRONMENT_NAME >- env.sh Valid Options: Variable Name Info Default Value Required OCP_INSTALL_TYPE Install type must be aro-msp Yes AZURE_SUBSCRIPTION_ID The subscription ID for your account in Azure Yes AZURE_CLIENT_ID The client ID for your account in Azure Yes AZURE_TENANT_ID The Tenant ID for your account in Azure Yes AZURE_REGION The Azure region you want to deploy to Yes ARO_RESOURCE_GROUP_NAME The Azure network resource group name ${CLUSTER_NAME}-aro-vnet No ARO_CLUSTER_RESOURCE_GROUP_NAME The Azure clsuter resource group name ${CLUSTER_NAME}-aro-cluster No ARO_VCPU_QUOTA_NAME The Azure Compute name you will use for deployment Standard DSv3 Family vCPUs No OCP_CREATE_OPENSHIFT_CONTAINER_STORAGE Do you want to deploy ODF storage false No OCP_RELEASE With ARO, you can not choose the version of OpenShift. Base version or minor version. At deployment time, Azure will pick for you. So OCP_RELEASE value is not used or allowed for ARO deployment. Once deployment is done, daffy will calculate the OCP version and download the correct version of ocp tools to match your new cluster. #ARO Base Settings #################### OCP_INSTALL_TYPE = \"aro-msp\" AZURE_SUBSCRIPTION_ID = \"999999-999999-999999-99999\" AZURE_CLIENT_ID = \"999999-999999-999999-99999\" AZURE_TENANT_ID = \"999999-999999-999999-99999\" AZURE_REGION = \"<YOUR_REGION>\" #ARO Override Settings #################### #ARO_RESOURCE_GROUP_NAME=\"${CLUSTER_NAME}-aro-vnet\" #ARO_CLUSTER_RESOURCE_GROUP_NAME=\"${CLUSTER_NAME}-aro-cluster\" #ARO_VCPU_QUOTA_NAME=\"Standard DSv3 Family vCPUs\" #OpenShift Storage #################### #OCP_CREATE_OPENSHIFT_CONTAINER_STORAGE=\"true\" Warning If you plan to install a cloud pak and/or need storage, you need to set the flag to setup OCS Storage Info It will prompt you for the Client Secret during the install Execution \u00b6 To deploy your OCP cluster to Azure ARO , run the build.sh script from the /data/daffy/ocp directory: / data / daffy / ocp / build.sh < ENVIRONMENT_NAME > Once your cluster is fully deployed you can access the help menu which has a number of options. Note < ENVIRONMENT_NAME > is the first part of your name that you used for the < ENVIRONMENT_NAME >-env.sh file Installing Cloud Paks","title":"ARO"},{"location":"Deploying-OCP/ARO/#azure-install","text":"At this point, you have a bastion machine where you have installed the Daffy tool, and ready to created your core environment-name -env.sh and so you can execute the install of OCP on Azure via ARO.","title":"Azure Install"},{"location":"Deploying-OCP/ARO/#platform-requirements","text":"To use Daffy on Azure ARO , there are some platform info and requirements that need to be gathered or met. We have a simple doc that you should refer to that list all providers and what would be needed. For Azure ARO , it breaks down to the following basic three items: Account Details - The account that you plan to install OpenShift Permissions - The permissions need to perform the install Quota - The ability to add new workload to that platform For detailed list of the above, you can find in the Daffy Provider Requirements. Please review before proceeding. https://ibm.box.com/v/DaffyProviderRequirements","title":"Platform Requirements"},{"location":"Deploying-OCP/ARO/#finding-provider-details","text":"To install Daffy on Azure ARO, the hardest part can be finding the provider details.","title":"Finding Provider Details"},{"location":"Deploying-OCP/ARO/#subscription-id","text":"First find subscriptions in your account from the search box Screenshot Locate Subscriptions More Info Once you find the subscription you want to use, you can see the Subscription ID Screenshot Locate Subscription ID","title":"Subscription ID"},{"location":"Deploying-OCP/ARO/#tenant-id","text":"First you need to find the Active Direcotry for your account Screenshot Active Directory From your active directory, you can locate the Tenant ID Screenshot Tenant ID More Info","title":"Tenant ID"},{"location":"Deploying-OCP/ARO/#client-id","text":"First you need to find the Active Directory for your account Screenshot Active Directory Search for your appliation and from here you can find the client ID for the application you plan to use Screenshot Client ID More Info","title":"Client ID"},{"location":"Deploying-OCP/ARO/#region","text":"More Info To find the region name, you can use the above link to list all azure region names, make sure you pick one that has avaiblity zone support Screenshot","title":"Region"},{"location":"Deploying-OCP/ARO/#quota","text":"In your subscription, under settings, you can find Usage + Quotas Screenshot In the Qutoa section, you can filter by regtion and type. Then you can see your used and your max qutoa limits. Screenshot More Info","title":"Quota"},{"location":"Deploying-OCP/ARO/#permission","text":"Within your Azure project, you would need to go to IAM Section and create/use Service Account. From the requirements doc , make sure your service account has the correct permissions. Look at the Azure section, it is same plus a few extra needed for ARO. Specifically Access to Active Directory.","title":"Permission"},{"location":"Deploying-OCP/ARO/#environment-file","text":"Deploying the OpenShift on Azure only requires three entries to your existing core environment file (< ENVIRONMENT_NAME >-env.sh) plus a local service account file. Note You can look in the samples directory on your bastion for example of ARO install : /data/daffy/env/samples/ aro-msp-env.sh You can copy the sample file to build your new environment file: cp / data / daffy / env / samples / aro - msp - env.sh / data / daffy / env /< ENVIRONMENT_NAME >- env.sh Valid Options: Variable Name Info Default Value Required OCP_INSTALL_TYPE Install type must be aro-msp Yes AZURE_SUBSCRIPTION_ID The subscription ID for your account in Azure Yes AZURE_CLIENT_ID The client ID for your account in Azure Yes AZURE_TENANT_ID The Tenant ID for your account in Azure Yes AZURE_REGION The Azure region you want to deploy to Yes ARO_RESOURCE_GROUP_NAME The Azure network resource group name ${CLUSTER_NAME}-aro-vnet No ARO_CLUSTER_RESOURCE_GROUP_NAME The Azure clsuter resource group name ${CLUSTER_NAME}-aro-cluster No ARO_VCPU_QUOTA_NAME The Azure Compute name you will use for deployment Standard DSv3 Family vCPUs No OCP_CREATE_OPENSHIFT_CONTAINER_STORAGE Do you want to deploy ODF storage false No OCP_RELEASE With ARO, you can not choose the version of OpenShift. Base version or minor version. At deployment time, Azure will pick for you. So OCP_RELEASE value is not used or allowed for ARO deployment. Once deployment is done, daffy will calculate the OCP version and download the correct version of ocp tools to match your new cluster. #ARO Base Settings #################### OCP_INSTALL_TYPE = \"aro-msp\" AZURE_SUBSCRIPTION_ID = \"999999-999999-999999-99999\" AZURE_CLIENT_ID = \"999999-999999-999999-99999\" AZURE_TENANT_ID = \"999999-999999-999999-99999\" AZURE_REGION = \"<YOUR_REGION>\" #ARO Override Settings #################### #ARO_RESOURCE_GROUP_NAME=\"${CLUSTER_NAME}-aro-vnet\" #ARO_CLUSTER_RESOURCE_GROUP_NAME=\"${CLUSTER_NAME}-aro-cluster\" #ARO_VCPU_QUOTA_NAME=\"Standard DSv3 Family vCPUs\" #OpenShift Storage #################### #OCP_CREATE_OPENSHIFT_CONTAINER_STORAGE=\"true\" Warning If you plan to install a cloud pak and/or need storage, you need to set the flag to setup OCS Storage Info It will prompt you for the Client Secret during the install","title":"Environment File"},{"location":"Deploying-OCP/ARO/#execution","text":"To deploy your OCP cluster to Azure ARO , run the build.sh script from the /data/daffy/ocp directory: / data / daffy / ocp / build.sh < ENVIRONMENT_NAME > Once your cluster is fully deployed you can access the help menu which has a number of options. Note < ENVIRONMENT_NAME > is the first part of your name that you used for the < ENVIRONMENT_NAME >-env.sh file Installing Cloud Paks","title":"Execution"},{"location":"Deploying-OCP/AWS/","text":"document.title = \"Deploy OCP - AWS\"; AWS Install \u00b6 Platform Requirements \u00b6 To use Daffy on Amazon Web Services , there are some platform info and requirements that need to be gathered or met. We have a simple doc that you should refer to that list all providers and what would be needed. For AWS , it breaks down to the following basic three items: Account Details - The account that you plan to install OpenShift Permissions - The permissions need to perform the install Quota - The ability to add new workload to that platform For detailed list of the above, you can find in the Daffy Provider Requirements. Please review before proceeding. https://ibm.box.com/v/DaffyProviderRequirements Finding Provider Details \u00b6 To install Daffy on AWS , the hardest part can be finding the provider details in the portal. To create or use an existing AWS Access Key ID you can refer to this: https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_access-keys.html?icmpid=docs_iam_console#Using_CreateAccessKey Note: Use the Identity and Access Management (IAM) service to manage access keys. Select Search - find IAM service You can create a new access key or use an existing key. The access key must have authority to the account you wan to install OpenShift into. Secret Access Key: The secret access key is ONLY displayed at the time of creation. When you create the access key, you will then have the opportunity to capture the secret access key Note This is sensitive information, please make sure you store this in a secure location The screen to the right is an example of what you will see when you create a NEW access Key. Region: For you to use Daffy to install on AWS you need to choose a valid region identifier. This will be the target region you are planning to deploy OpenShift into. To see a complete list of available AWS Regions, you can select the region drop down list in the AWA Portal. This will be in the upper right hand corner next to your account name. (See picture to the right) Note: Take note of the region identifier such as: us-east-2. This is the value you would use to deploy a OCP cluster into the US East (Ohio) region. Permission: Within your AWS project, you would need to go to IAM Section and make sure the user that is associated with your Access Key is assigned the correct roles. At minimum, you need to have this role: AdministratorAccess Please see the requirements doc for more information! Hosted Zone: For each OpenShift deployment into AWS , you need to create a Route 53 Hosted Zone . Important : You must create a Hosted Zone that exactly matches your Base Domain. Important: Once you create your Hosted Zone, you must point your DNS registry Name Server records to the assigned AWS DNS Name Server records listed in this Hosted Zone. You will see the Name Servers listed once you have created the Hosted Zone. Setting up DNS \u00b6 HTML Video embed Quota: Please refer to the requirements doc for a list of resource quota's that are required for deployment of OpenShift in AWS. Environment File \u00b6 Below are the AWS specific environment variables that must be defined in the /data/daffy/env/< ENVIRONMENT_NAME >-env.sh file: AWS_REGION AWS_ACCESS_KEY_ID Note You can look in the samples directory on your bastion for example of AWS install : /data/daffy/env/samples/ aws-ipi-env.sh You can run this command to build your new file from the sample. cp / data / daffy / env / samples / aws - ipi - env.sh / data / daffy / env /< ENVIRONMENT_NAME >- env.sh Valid Options: OCP_INSTALL_TYPE =aws-ipi AWS_REGION =AWS-REGION AWS_ACCESS_KEY_ID =AWS-ACCESS_KEY-ID OCP_INSTALL_TYPE = \"aws-ipi\" AWS_REGION = \"<AWS-REGION>\" AWS_ACCESS_KEY_ID = \"<AWS-ACCESS_KEY-ID>\" #OCP_CREATE_OPENSHIFT_CONTAINER_STORAGE=true Optional: OCP_CREATE_OPENSHIFT_CONTAINER_STORAGE =true If you plan to install a cloud pak and/or need storage, you need to set the flag to setup OCS Storage. Execution \u00b6 To deploy your cluster, run the build.sh script from the /data/daffy/ocp directory: / data / daffy / ocp / build.sh < ENVIRONMENT_NAME > Once your cluster is fully deployed you can access the help menu which as a number of options. Note < environment > is the first part of your name that you used for the < environment >-env.sh file / data / daffy / ocp / build.sh < ENVIRONMENT_NAME > -- help Installing Cloud Paks","title":"AWS"},{"location":"Deploying-OCP/AWS/#aws-install","text":"","title":"AWS Install"},{"location":"Deploying-OCP/AWS/#platform-requirements","text":"To use Daffy on Amazon Web Services , there are some platform info and requirements that need to be gathered or met. We have a simple doc that you should refer to that list all providers and what would be needed. For AWS , it breaks down to the following basic three items: Account Details - The account that you plan to install OpenShift Permissions - The permissions need to perform the install Quota - The ability to add new workload to that platform For detailed list of the above, you can find in the Daffy Provider Requirements. Please review before proceeding. https://ibm.box.com/v/DaffyProviderRequirements","title":"Platform Requirements"},{"location":"Deploying-OCP/AWS/#finding-provider-details","text":"To install Daffy on AWS , the hardest part can be finding the provider details in the portal. To create or use an existing AWS Access Key ID you can refer to this: https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_access-keys.html?icmpid=docs_iam_console#Using_CreateAccessKey Note: Use the Identity and Access Management (IAM) service to manage access keys. Select Search - find IAM service You can create a new access key or use an existing key. The access key must have authority to the account you wan to install OpenShift into. Secret Access Key: The secret access key is ONLY displayed at the time of creation. When you create the access key, you will then have the opportunity to capture the secret access key Note This is sensitive information, please make sure you store this in a secure location The screen to the right is an example of what you will see when you create a NEW access Key. Region: For you to use Daffy to install on AWS you need to choose a valid region identifier. This will be the target region you are planning to deploy OpenShift into. To see a complete list of available AWS Regions, you can select the region drop down list in the AWA Portal. This will be in the upper right hand corner next to your account name. (See picture to the right) Note: Take note of the region identifier such as: us-east-2. This is the value you would use to deploy a OCP cluster into the US East (Ohio) region. Permission: Within your AWS project, you would need to go to IAM Section and make sure the user that is associated with your Access Key is assigned the correct roles. At minimum, you need to have this role: AdministratorAccess Please see the requirements doc for more information! Hosted Zone: For each OpenShift deployment into AWS , you need to create a Route 53 Hosted Zone . Important : You must create a Hosted Zone that exactly matches your Base Domain. Important: Once you create your Hosted Zone, you must point your DNS registry Name Server records to the assigned AWS DNS Name Server records listed in this Hosted Zone. You will see the Name Servers listed once you have created the Hosted Zone.","title":"Finding Provider Details"},{"location":"Deploying-OCP/AWS/#setting-up-dns","text":"HTML Video embed Quota: Please refer to the requirements doc for a list of resource quota's that are required for deployment of OpenShift in AWS.","title":"Setting up DNS"},{"location":"Deploying-OCP/AWS/#environment-file","text":"Below are the AWS specific environment variables that must be defined in the /data/daffy/env/< ENVIRONMENT_NAME >-env.sh file: AWS_REGION AWS_ACCESS_KEY_ID Note You can look in the samples directory on your bastion for example of AWS install : /data/daffy/env/samples/ aws-ipi-env.sh You can run this command to build your new file from the sample. cp / data / daffy / env / samples / aws - ipi - env.sh / data / daffy / env /< ENVIRONMENT_NAME >- env.sh Valid Options: OCP_INSTALL_TYPE =aws-ipi AWS_REGION =AWS-REGION AWS_ACCESS_KEY_ID =AWS-ACCESS_KEY-ID OCP_INSTALL_TYPE = \"aws-ipi\" AWS_REGION = \"<AWS-REGION>\" AWS_ACCESS_KEY_ID = \"<AWS-ACCESS_KEY-ID>\" #OCP_CREATE_OPENSHIFT_CONTAINER_STORAGE=true Optional: OCP_CREATE_OPENSHIFT_CONTAINER_STORAGE =true If you plan to install a cloud pak and/or need storage, you need to set the flag to setup OCS Storage.","title":"Environment File"},{"location":"Deploying-OCP/AWS/#execution","text":"To deploy your cluster, run the build.sh script from the /data/daffy/ocp directory: / data / daffy / ocp / build.sh < ENVIRONMENT_NAME > Once your cluster is fully deployed you can access the help menu which as a number of options. Note < environment > is the first part of your name that you used for the < environment >-env.sh file / data / daffy / ocp / build.sh < ENVIRONMENT_NAME > -- help Installing Cloud Paks","title":"Execution"},{"location":"Deploying-OCP/Azure/","text":"document.title = \"Deploy OCP - Azure\"; Azure Install \u00b6 At this point, you have a bastion machine where you have installed the Daffy tool, created your core environment-name -env.sh and can execute the install of OCP on Azure. Platform Requirements \u00b6 To use Daffy on Azure , there are some platform info and requirements that need to be gathered or met. We have a simple doc that you should refer to that list all providers and what would be needed. For Azure , it breaks down to the following basic three items: Account Details - The account that you plan to install OpenShift Permissions - The permissions need to perform the install Quota - The ability to add new workload to that platform For detailed list of the above, you can find in the Daffy Provider Requirements. Please review before proceeding. https://ibm.box.com/v/DaffyProviderRequirements Finding Provider Details \u00b6 To install Daffy on Azure, the hardest part can be finding the provider details. Subscription ID \u00b6 First find subscriptions in your account from the search box Screenshot Locate Subscriptions More Info Once you find the subscription you want to use, you can see the Subscription ID Screenshot Locate Subscription ID Tenant ID \u00b6 First you need to find the Active Direcotry for your account Screenshot Active Directory From your active directory, you can locate the Tenant ID Screenshot Tenant ID More Info Client ID \u00b6 First you need to find the Active Directory for your account Screenshot Active Directory Search for your appliation and from here you can find the client ID for the application you plan to use Screenshot Client ID More Info Region \u00b6 More Info To find the region name, you can use the above link to list all azure region names, make sure you pick one that has avaiblity zone support Screenshot Quota \u00b6 In your subscription, under settings, you can find Usage + Quotas Screenshot In the Qutoa section, you can filter by regtion and type. Then you can see your used and your max qutoa limits. Screenshot More Info Permission: Within your Azure project, you would need to go to IAM Section and create/use Service Account. From the requirements doc , make sure your service account has the correct permissions. Dedicated public host Zone: You will need to create a DNS Zone within a new/existing resource group. For the OpenShift install, you need the following: Registered DNS Name - myexample.com Azure DNS Zone - myexample-com Transfer the domain to Azure Name services listed in your new Azure DNS Zone Setting up DNS \u00b6 Environment File \u00b6 Deploying the OpenShift on Azure only requires three entries to your existing core environment file (< ENVIRONMENT_NAME >-env.sh) plus a local service account file. Note You can look in the samples directory on your bastion for example of Azure install : /data/daffy/env/samples/ azure-ipi-env.sh You can copy the sample file to build your new environment file: cp / data / daffy / env / samples / azure - ipi - env.sh / data / daffy / env /< ENVIRONMENT_NAME >- env.sh Valid Options: Variable Name Info Default Value Required OCP_INSTALL_TYPE Install type must be aro-msp Yes AZURE_SUBSCRIPTION_ID The subscription ID for your account in Azure Yes AZURE_CLIENT_ID The client ID for your account in Azure Yes AZURE_TENANT_ID The Tenant ID for your account in Azure Yes AZURE_REGION The Azure region you want to deploy to Yes AZURE_RESOURCE_GROUP_NAME The Azure network resource group name No AZURE_BASE_DOMAIN_RESOURCE_GROUP_NAME The Azure clsuter resource group name No OCP_CREATE_OPENSHIFT_CONTAINER_STORAGE Do you want to deploy ODF storage false No AZURE_RESOURCE_GROUP_NAME_CREATE_MISSING Do you want to deploy ODF storage false No #Azure Base Settings #################### OCP_INSTALL_TYPE = \"azure-ipi\" AZURE_SUBSCRIPTION_ID = \"999999-999999-999999-99999\" AZURE_CLIENT_ID = \"999999-999999-999999-99999\" AZURE_TENANT_ID = \"999999-999999-999999-99999\" AZURE_RESOURCE_GROUP_NAME = \"<YOUR_RESOURCE_GROUP_FOR_CLUSTER>\" AZURE_BASE_DOMAIN_RESOURCE_GROUP_NAME = \"<YOUR_RESOURCE_GROUP_FOR_DNS>\" AZURE_REGION = \"<YOUR_REGION>\" #OpenShift Storage #################### #OCP_CREATE_OPENSHIFT_CONTAINER_STORAGE=\"true\" #AZURE_RESOURCE_GROUP_NAME_CREATE_MISSING=\"true\" If you plan to install a cloud pak and/or need storage, you need to set the flag to setup OCS Storage ** It will prompt you for the Client Secret during the install. Execution \u00b6 To deploy your OCP cluster to Azure , run the build.sh script from the /data/daffy/ocp directory: / data / daffy / ocp / build.sh < ENVIRONMENT_NAME > Once your cluster is fully deployed you can access the help menu which has a number of options. Note < ENVIRONMENT_NAME > is the first part of your name that you used for the < ENVIRONMENT_NAME >-env.sh file Installing Cloud Paks","title":"Azure"},{"location":"Deploying-OCP/Azure/#azure-install","text":"At this point, you have a bastion machine where you have installed the Daffy tool, created your core environment-name -env.sh and can execute the install of OCP on Azure.","title":"Azure Install"},{"location":"Deploying-OCP/Azure/#platform-requirements","text":"To use Daffy on Azure , there are some platform info and requirements that need to be gathered or met. We have a simple doc that you should refer to that list all providers and what would be needed. For Azure , it breaks down to the following basic three items: Account Details - The account that you plan to install OpenShift Permissions - The permissions need to perform the install Quota - The ability to add new workload to that platform For detailed list of the above, you can find in the Daffy Provider Requirements. Please review before proceeding. https://ibm.box.com/v/DaffyProviderRequirements","title":"Platform Requirements"},{"location":"Deploying-OCP/Azure/#finding-provider-details","text":"To install Daffy on Azure, the hardest part can be finding the provider details.","title":"Finding Provider Details"},{"location":"Deploying-OCP/Azure/#subscription-id","text":"First find subscriptions in your account from the search box Screenshot Locate Subscriptions More Info Once you find the subscription you want to use, you can see the Subscription ID Screenshot Locate Subscription ID","title":"Subscription ID"},{"location":"Deploying-OCP/Azure/#tenant-id","text":"First you need to find the Active Direcotry for your account Screenshot Active Directory From your active directory, you can locate the Tenant ID Screenshot Tenant ID More Info","title":"Tenant ID"},{"location":"Deploying-OCP/Azure/#client-id","text":"First you need to find the Active Directory for your account Screenshot Active Directory Search for your appliation and from here you can find the client ID for the application you plan to use Screenshot Client ID More Info","title":"Client ID"},{"location":"Deploying-OCP/Azure/#region","text":"More Info To find the region name, you can use the above link to list all azure region names, make sure you pick one that has avaiblity zone support Screenshot","title":"Region"},{"location":"Deploying-OCP/Azure/#quota","text":"In your subscription, under settings, you can find Usage + Quotas Screenshot In the Qutoa section, you can filter by regtion and type. Then you can see your used and your max qutoa limits. Screenshot More Info Permission: Within your Azure project, you would need to go to IAM Section and create/use Service Account. From the requirements doc , make sure your service account has the correct permissions. Dedicated public host Zone: You will need to create a DNS Zone within a new/existing resource group. For the OpenShift install, you need the following: Registered DNS Name - myexample.com Azure DNS Zone - myexample-com Transfer the domain to Azure Name services listed in your new Azure DNS Zone","title":"Quota"},{"location":"Deploying-OCP/Azure/#setting-up-dns","text":"","title":"Setting up DNS"},{"location":"Deploying-OCP/Azure/#environment-file","text":"Deploying the OpenShift on Azure only requires three entries to your existing core environment file (< ENVIRONMENT_NAME >-env.sh) plus a local service account file. Note You can look in the samples directory on your bastion for example of Azure install : /data/daffy/env/samples/ azure-ipi-env.sh You can copy the sample file to build your new environment file: cp / data / daffy / env / samples / azure - ipi - env.sh / data / daffy / env /< ENVIRONMENT_NAME >- env.sh Valid Options: Variable Name Info Default Value Required OCP_INSTALL_TYPE Install type must be aro-msp Yes AZURE_SUBSCRIPTION_ID The subscription ID for your account in Azure Yes AZURE_CLIENT_ID The client ID for your account in Azure Yes AZURE_TENANT_ID The Tenant ID for your account in Azure Yes AZURE_REGION The Azure region you want to deploy to Yes AZURE_RESOURCE_GROUP_NAME The Azure network resource group name No AZURE_BASE_DOMAIN_RESOURCE_GROUP_NAME The Azure clsuter resource group name No OCP_CREATE_OPENSHIFT_CONTAINER_STORAGE Do you want to deploy ODF storage false No AZURE_RESOURCE_GROUP_NAME_CREATE_MISSING Do you want to deploy ODF storage false No #Azure Base Settings #################### OCP_INSTALL_TYPE = \"azure-ipi\" AZURE_SUBSCRIPTION_ID = \"999999-999999-999999-99999\" AZURE_CLIENT_ID = \"999999-999999-999999-99999\" AZURE_TENANT_ID = \"999999-999999-999999-99999\" AZURE_RESOURCE_GROUP_NAME = \"<YOUR_RESOURCE_GROUP_FOR_CLUSTER>\" AZURE_BASE_DOMAIN_RESOURCE_GROUP_NAME = \"<YOUR_RESOURCE_GROUP_FOR_DNS>\" AZURE_REGION = \"<YOUR_REGION>\" #OpenShift Storage #################### #OCP_CREATE_OPENSHIFT_CONTAINER_STORAGE=\"true\" #AZURE_RESOURCE_GROUP_NAME_CREATE_MISSING=\"true\" If you plan to install a cloud pak and/or need storage, you need to set the flag to setup OCS Storage ** It will prompt you for the Client Secret during the install.","title":"Environment File"},{"location":"Deploying-OCP/Azure/#execution","text":"To deploy your OCP cluster to Azure , run the build.sh script from the /data/daffy/ocp directory: / data / daffy / ocp / build.sh < ENVIRONMENT_NAME > Once your cluster is fully deployed you can access the help menu which has a number of options. Note < ENVIRONMENT_NAME > is the first part of your name that you used for the < ENVIRONMENT_NAME >-env.sh file Installing Cloud Paks","title":"Execution"},{"location":"Deploying-OCP/Core-steps/","text":"document.title = \"Deploy OCP - Core Steps\"; Step 1: Bastion Server \u00b6 Create Bastion Steps *** If you do not have a bastion, above button/link will walk you through the process to create a Linux bastion server. Step 2: Sizing \u00b6 Go to the following site to size your OpenShift cluster to meet your software needs CloudPak Sizing Step 3: Install Daffy \u00b6 Log into your Bastion Machine (as root) and run the following command to download the latest Daffy Scripts: wget http://get.daffy-installer.com/download-scripts/daffy-init.sh; chmod 755 daffy-init.sh;./daffy-init.sh Optional: You may choose to use the Daffy Web Configurator or Daffy CLI Configurator! The purpose of these tools are to help you build your environment file Online Configurator CLI Configurator Step 4: Environment File \u00b6 #Daffy Values ######################### DAFFY_UNIQUE_ID = \"<YourID@email.com>\" #This is required - Values POC/Demo/Enablement/HCCX/TechZone DAFFY_DEPLOYMENT_TYPE = \"PickValueFromLineAbove\" #If POC/Demo, these are required. #ISC number must be 18 characters #DAFFY_ISC_NUMBER=\"0045h00000w1nvKAAG\" #DAFFY_CUSTOMER_NAME=\"Acme Customer\" #Core Values ######################### BASE_DOMAIN = \"<YOUR.BASEDOMAIN.COM>\" #Cluster name must only be lowercase/alphanumberic and \"-\", no spaces CLUSTER_NAME = \"<ENVIRONMENT_NAME>\" #This is required - Values aws-ipi/azure-ipi/gcp-ipi/vsphere-ipi/vsphere-upi/kvm-upi/roks-msp OCP_INSTALL_TYPE = \"PickValueFromLineAbove\" OCP_RELEASE = \"4.10.36\" VM_TSHIRT_SIZE = \"Large\" This file is where you store values that will define your environment and Daffy will use to build your environment. Place your file in the following folder with your environment name in the following formatting: /data/daffy/env/<**ENVIRONMENT_NAME**>-env.sh Best practice is to set < ENVIRONMENT_NAME > as your cluster name, but that is not required. Name Example Values Description Requirements DAFFY_DEPLOYMENT_TYPE Enablement The type of Depolment this will be for POC/Demo/Enablement/HCCX/TechZone/PostSale DAFFY_ISC_NUMBER 0045h00000w1nvKAAG Required if Demo, POC or PostSale Must be 18 characters, no spaces DAFFY_CUSTOMER_NAME Acme Shoes Required if Demo, POC or PostSale May contain spaces BASE_DOMAIN acme-shoes.com Is your DNS name your cluster will use valid dns domain name CLUSTER_NAME demo01 The name you want to give your OpenShift Cluster Only lowercase/alphanumeric and \"-\", no spaces OCP_INSTALL_TYPE aws-ipi The name of the install type you want aws-ipi/azure-ipi/gcp-ipi/vsphere-ipi/vsphere-upi/kvm-upi/roks-msp OCP_RELEASE 4.10.36 What version of OpenShift you want to Install VM_TSHIRT_SIZE Large How large you want the OpenShift Cluster to be. Min and Large Supported today Info If MSP type install like ROKS, BASE_DOMAIN is not needed. Optionally: As a starting point, you can copy a sample environment file from the samples folder located here: /data/daffy/env/samples/<platform>-env.sh cd /data/daffy/env/samples Replace these values for the next command: < platform > = the sample file name for the platform you are planning to deploy your OCP Cluster. < environment > = the name of your environment file. As a best practice, we recommend you use the name of your cluster. Example: cp /data/daffy/env/samples/aws-ipi-env.sh /data/daffy/env/demo01-env.sh This command will copy the sample file and place it in the /data/daffy/env directory, and your new environment name will be demo01 cp /data/daffy/env/samples/<platform>-env.sh /data/daffy/env/<environment>-env.sh Debug Flag Setting the debug flag to true will stop you at every check point and ask you to hit enter. Setting the debug flag to false will run though the script without any interference. DEBUG = \"false\" Step 5: DNS Requirements \u00b6 For OpenShift to be installed, you will need to setup your own DNS or use existing domain/subdomain. You can not use local host files or local resolver. vSphere and KVM UPI \u00b6 api.${CLUSTER}.${YOUR.DOMAIN.COM} ---> ${YOUR.BASTION.IP} api-int.${CLUSTER}.${YOUR.DOMAIN.COM} ---> ${YOUR.BASTION.IP} *.apps.${CLUSTER}.${YOUR.DOMAIN.COM} ---> ${YOUR.BASTION.IP} vSphere IPI \u00b6 api.${CLUSTER}.${YOUR.DOMAIN.COM} ---> Unused Static IP #1 *.apps.${CLUSTER}.${YOUR.DOMAIN.COM} ---> Unused Static IP #2 Allow Daffy to crate DNS entries in IBM Cloud If you want the daffy tool to create your above DNS entires in IBM Cloud, add the following to your ~/.profiles DNS_API_KEY = \"YOURDNSAPIKEY\" DNS_DOMAIN_ID = \"YOURDNSDOMAINID\" AWS, Azure, GCP IPI \u00b6 Have/Create DNS domain/subdomain ( More Detail for DNS on next steps ) Transfer domain/subdomain to your provider (if not created there already) a. Create hosted zone in provider b. Use name servers from hosted zone to transfer your domain/subdomain TechZone, HCCX, ROKS \u00b6 You will not need DNS as they will provide for you Step 6a: Install OpenShift \u00b6 Info Before you start to install OpenShift on the Provider, please look at the OpenShift Requirements You are NOW ready to begin making the necessary edits to your /data/daffy/env/< ENVIRONMENT_NAME >-env.sh file for a deployment of OCP to a specific platform. HCCX Gym TechZone ROKS VSphere Azure AWS GCP Step 6b: Existing Cluster \u00b6 If you already have an existing cluster that was not built with Daffy, you can still use daffy for Step 2 and/or Step 3. The only extra step you need to do is via the command line on your bastion. You will need to login to your cluster via the oc login command. You can get this command from your OpenShift console. You would then move on to the Cloud Pak steps and skip the OpenShift Install. For Step 6b only , If you do not have the oc command installed on your bastion, you can use the daffy tools command to installing it for you. https://ibm.github.io/daffy/Tips-and-Tricks/Common-Commands/#daffy-tools /data/daffy/tools.sh --installOC Cloud Paks","title":"Core Steps"},{"location":"Deploying-OCP/Core-steps/#step-1-bastion-server","text":"Create Bastion Steps *** If you do not have a bastion, above button/link will walk you through the process to create a Linux bastion server.","title":"Step 1: Bastion Server"},{"location":"Deploying-OCP/Core-steps/#step-2-sizing","text":"Go to the following site to size your OpenShift cluster to meet your software needs CloudPak Sizing","title":"Step 2: Sizing"},{"location":"Deploying-OCP/Core-steps/#step-3-install-daffy","text":"Log into your Bastion Machine (as root) and run the following command to download the latest Daffy Scripts: wget http://get.daffy-installer.com/download-scripts/daffy-init.sh; chmod 755 daffy-init.sh;./daffy-init.sh Optional: You may choose to use the Daffy Web Configurator or Daffy CLI Configurator! The purpose of these tools are to help you build your environment file Online Configurator CLI Configurator","title":"Step 3: Install Daffy"},{"location":"Deploying-OCP/Core-steps/#step-4-environment-file","text":"#Daffy Values ######################### DAFFY_UNIQUE_ID = \"<YourID@email.com>\" #This is required - Values POC/Demo/Enablement/HCCX/TechZone DAFFY_DEPLOYMENT_TYPE = \"PickValueFromLineAbove\" #If POC/Demo, these are required. #ISC number must be 18 characters #DAFFY_ISC_NUMBER=\"0045h00000w1nvKAAG\" #DAFFY_CUSTOMER_NAME=\"Acme Customer\" #Core Values ######################### BASE_DOMAIN = \"<YOUR.BASEDOMAIN.COM>\" #Cluster name must only be lowercase/alphanumberic and \"-\", no spaces CLUSTER_NAME = \"<ENVIRONMENT_NAME>\" #This is required - Values aws-ipi/azure-ipi/gcp-ipi/vsphere-ipi/vsphere-upi/kvm-upi/roks-msp OCP_INSTALL_TYPE = \"PickValueFromLineAbove\" OCP_RELEASE = \"4.10.36\" VM_TSHIRT_SIZE = \"Large\" This file is where you store values that will define your environment and Daffy will use to build your environment. Place your file in the following folder with your environment name in the following formatting: /data/daffy/env/<**ENVIRONMENT_NAME**>-env.sh Best practice is to set < ENVIRONMENT_NAME > as your cluster name, but that is not required. Name Example Values Description Requirements DAFFY_DEPLOYMENT_TYPE Enablement The type of Depolment this will be for POC/Demo/Enablement/HCCX/TechZone/PostSale DAFFY_ISC_NUMBER 0045h00000w1nvKAAG Required if Demo, POC or PostSale Must be 18 characters, no spaces DAFFY_CUSTOMER_NAME Acme Shoes Required if Demo, POC or PostSale May contain spaces BASE_DOMAIN acme-shoes.com Is your DNS name your cluster will use valid dns domain name CLUSTER_NAME demo01 The name you want to give your OpenShift Cluster Only lowercase/alphanumeric and \"-\", no spaces OCP_INSTALL_TYPE aws-ipi The name of the install type you want aws-ipi/azure-ipi/gcp-ipi/vsphere-ipi/vsphere-upi/kvm-upi/roks-msp OCP_RELEASE 4.10.36 What version of OpenShift you want to Install VM_TSHIRT_SIZE Large How large you want the OpenShift Cluster to be. Min and Large Supported today Info If MSP type install like ROKS, BASE_DOMAIN is not needed. Optionally: As a starting point, you can copy a sample environment file from the samples folder located here: /data/daffy/env/samples/<platform>-env.sh cd /data/daffy/env/samples Replace these values for the next command: < platform > = the sample file name for the platform you are planning to deploy your OCP Cluster. < environment > = the name of your environment file. As a best practice, we recommend you use the name of your cluster. Example: cp /data/daffy/env/samples/aws-ipi-env.sh /data/daffy/env/demo01-env.sh This command will copy the sample file and place it in the /data/daffy/env directory, and your new environment name will be demo01 cp /data/daffy/env/samples/<platform>-env.sh /data/daffy/env/<environment>-env.sh Debug Flag Setting the debug flag to true will stop you at every check point and ask you to hit enter. Setting the debug flag to false will run though the script without any interference. DEBUG = \"false\"","title":"Step 4: Environment File"},{"location":"Deploying-OCP/Core-steps/#step-5-dns-requirements","text":"For OpenShift to be installed, you will need to setup your own DNS or use existing domain/subdomain. You can not use local host files or local resolver.","title":"Step 5:  DNS Requirements"},{"location":"Deploying-OCP/Core-steps/#vsphere-and-kvm-upi","text":"api.${CLUSTER}.${YOUR.DOMAIN.COM} ---> ${YOUR.BASTION.IP} api-int.${CLUSTER}.${YOUR.DOMAIN.COM} ---> ${YOUR.BASTION.IP} *.apps.${CLUSTER}.${YOUR.DOMAIN.COM} ---> ${YOUR.BASTION.IP}","title":"vSphere and KVM UPI"},{"location":"Deploying-OCP/Core-steps/#vsphere-ipi","text":"api.${CLUSTER}.${YOUR.DOMAIN.COM} ---> Unused Static IP #1 *.apps.${CLUSTER}.${YOUR.DOMAIN.COM} ---> Unused Static IP #2 Allow Daffy to crate DNS entries in IBM Cloud If you want the daffy tool to create your above DNS entires in IBM Cloud, add the following to your ~/.profiles DNS_API_KEY = \"YOURDNSAPIKEY\" DNS_DOMAIN_ID = \"YOURDNSDOMAINID\"","title":"vSphere IPI"},{"location":"Deploying-OCP/Core-steps/#aws-azure-gcp-ipi","text":"Have/Create DNS domain/subdomain ( More Detail for DNS on next steps ) Transfer domain/subdomain to your provider (if not created there already) a. Create hosted zone in provider b. Use name servers from hosted zone to transfer your domain/subdomain","title":"AWS, Azure, GCP IPI"},{"location":"Deploying-OCP/Core-steps/#techzone-hccx-roks","text":"You will not need DNS as they will provide for you","title":"TechZone, HCCX, ROKS"},{"location":"Deploying-OCP/Core-steps/#step-6a-install-openshift","text":"Info Before you start to install OpenShift on the Provider, please look at the OpenShift Requirements You are NOW ready to begin making the necessary edits to your /data/daffy/env/< ENVIRONMENT_NAME >-env.sh file for a deployment of OCP to a specific platform. HCCX Gym TechZone ROKS VSphere Azure AWS GCP","title":"Step 6a: Install OpenShift"},{"location":"Deploying-OCP/Core-steps/#step-6b-existing-cluster","text":"If you already have an existing cluster that was not built with Daffy, you can still use daffy for Step 2 and/or Step 3. The only extra step you need to do is via the command line on your bastion. You will need to login to your cluster via the oc login command. You can get this command from your OpenShift console. You would then move on to the Cloud Pak steps and skip the OpenShift Install. For Step 6b only , If you do not have the oc command installed on your bastion, you can use the daffy tools command to installing it for you. https://ibm.github.io/daffy/Tips-and-Tricks/Common-Commands/#daffy-tools /data/daffy/tools.sh --installOC Cloud Paks","title":"Step 6b: Existing Cluster"},{"location":"Deploying-OCP/GCP/","text":"document.title = \"Deploy OCP - GCP\"; GCP Install \u00b6 At this point, you have a bastion machine where you have installed the Daffy tool, created your core -env.sh and can execute the install of OCP on GCP . Platform Requirements \u00b6 To use Daffy on G oogle C loud P latform, there are some platform info and requirements that need to be gathered or met. We have a simple doc that you should refer to that list all providers and what would be needed. For GCP, it breaks down to the following basic three items: Account Details - the account that you plan to install OpenShift Permissions - the permissions need to perform the install Quota - the ability to add new workload to that platform For detailed list of the above, you can find in the Daffy Provider Requirements. Please review before proceeding. https://ibm.box.com/v/DaffyProviderRequirements Finding Provider Details \u00b6 To install Daffy on G oogle C loud P latform, the hardest part can be finding the provider details. Project ID To find your GCP project you can refer to this: https://cloud.google.com/resource-manager/docs/creating-managing-projects You can list your Project ID from the drop down You can see your Project ID from the dashboard Identifying a region or zone Each region in Compute Engine contains a number of zones. Each zone name contains two parts that describe each zone in detail. The first part of the zone name is the region and the second part of the name describes the zone in the region. Region Regions are collections of zones. Zones have high-bandwidth, low-latency network connections to other zones in the same region. In order to deploy fault-tolerant applications that have high availability, Google recommends deploying applications across multiple zones and multiple regions. This helps protect against unexpected failures of components, up to and including a single zone or region. Choose regions that makes sense for your scenario. For example, if you only have customers in the US, or if you have specific needs that require your data to live in the US, it makes sense to store your resources in zones in the us-central1 region or zones in the us-east1 region. Region: to find a list of regions, you can refer to this: https://cloud.google.com/compute/docs/regions-zones What are service accounts? A service account is a special kind of account used by an application or compute workload, such as a Compute Engine virtual machine (VM) instance, rather than a person. Applications use service accounts to make authorized API calls , authorized as either the service account itself, or as Google Workspace or Cloud Identity users through domain-wide delegation . Service Account: In order to install on GCP with Daffy, you need to create a service account that has the correct permission to install. https://cloud.google.com/iam/docs/creating-managing-service-account-keys#creating Permission: Within your GCP project, you would need to go to IAM Section and create/use Service Account. From the requirements doc , make sure your service account has the correct permissions. API Services Enabled: For each GCP project, you need to enable API access. Within your GCP project, you would need to enable each API needed for the OpenShift install. From the requirements doc , you can search for each API and confirm/enable each API Service. Quota: For each GCP project, you need to enable correct amount of quota to run openshift. Within your GCP project, you would need to Verify your current quota. From the requirements doc , you can search for each quota to find current limit. Search for \"Quotas\" within your GCP project Select the \"All Quotas\" Search for the quota you are looking for Verify Limit/Used Percentage Once you open the \"All Quotas\" page, you can search for each quota to see its limits. Dedicated public host Zone: You will need to create a DNS hosted Zone project. For the OpenShift install, you need the following: Registered DNS Name - myexample.com GCP DNS Zone - myexample-com Transfer the domain to GCP Name services listed in your new GCP DNS Zone Setting up DNS \u00b6 Environment File \u00b6 Deploying the OpenShift on GCP only requires three entries to your existing core environment file (< ENVIRONMENT_NAME >-env.sh) plus a local service account file. Note You can look in the samples directory on your bastion for example of GCP install : /data/daffy/env/samples/ gcp-ipi-env.sh You can copy the sample file to build your new environment file. cp /data/daffy/env/samples/ gcp-ipi-env.sh /data/daffy/env/< ENVIRONMENT_NAME >-env.sh Valid Options: OCP_INSTALL_TYPE= gcp-ipi GCP_PROJECT_ID= GCP_REGION = Optional: OCP_CREATE_OPENSHIFT_CONTAINER_STORAGE =true OCP_INSTALL_TYPE = \"gcp-ipi\" GCP_PROJECT_ID = \"<YourGCPProjectID>\" GCP_REGION = \"<AnyValidGCPRegion>\" #OCP_CREATE_OPENSHIFT_CONTAINER_STORAGE=true If you plan to install a cloud pak and/or need storage, you need to set the flag to setup OCS Storage. For GCP , you need to download your Service Key as well. Save this to your home path: ~/.gcp/osServiceAccount.json In your GCP project, go to IAM and Select Service accounts Select or create new Service Account From the Service Account, select the Keys tab to create new key. *** FYI after you create the key, you can not view/download it. You can only get the details at the time of creation Execution \u00b6 To deploy your OCP cluster to GCP, run the build.sh script from the /data/daffy/ocp directory: /data/daffy/ocp/build.sh <ENVIRONMENT_NAME> Once your cluster is fully deployed, you can access the help menu which has a number of options. Note <ENVIRONMENT_NAME> is the first part of your name that you used for the -env.sh file /data/daffy/ocp/build.sh <ENVIRONMENT_NAME> --help Installing Cloud Paks","title":"GCP"},{"location":"Deploying-OCP/GCP/#gcp-install","text":"At this point, you have a bastion machine where you have installed the Daffy tool, created your core -env.sh and can execute the install of OCP on GCP .","title":"GCP Install"},{"location":"Deploying-OCP/GCP/#platform-requirements","text":"To use Daffy on G oogle C loud P latform, there are some platform info and requirements that need to be gathered or met. We have a simple doc that you should refer to that list all providers and what would be needed. For GCP, it breaks down to the following basic three items: Account Details - the account that you plan to install OpenShift Permissions - the permissions need to perform the install Quota - the ability to add new workload to that platform For detailed list of the above, you can find in the Daffy Provider Requirements. Please review before proceeding. https://ibm.box.com/v/DaffyProviderRequirements","title":"Platform Requirements"},{"location":"Deploying-OCP/GCP/#finding-provider-details","text":"To install Daffy on G oogle C loud P latform, the hardest part can be finding the provider details. Project ID To find your GCP project you can refer to this: https://cloud.google.com/resource-manager/docs/creating-managing-projects You can list your Project ID from the drop down You can see your Project ID from the dashboard Identifying a region or zone Each region in Compute Engine contains a number of zones. Each zone name contains two parts that describe each zone in detail. The first part of the zone name is the region and the second part of the name describes the zone in the region. Region Regions are collections of zones. Zones have high-bandwidth, low-latency network connections to other zones in the same region. In order to deploy fault-tolerant applications that have high availability, Google recommends deploying applications across multiple zones and multiple regions. This helps protect against unexpected failures of components, up to and including a single zone or region. Choose regions that makes sense for your scenario. For example, if you only have customers in the US, or if you have specific needs that require your data to live in the US, it makes sense to store your resources in zones in the us-central1 region or zones in the us-east1 region. Region: to find a list of regions, you can refer to this: https://cloud.google.com/compute/docs/regions-zones What are service accounts? A service account is a special kind of account used by an application or compute workload, such as a Compute Engine virtual machine (VM) instance, rather than a person. Applications use service accounts to make authorized API calls , authorized as either the service account itself, or as Google Workspace or Cloud Identity users through domain-wide delegation . Service Account: In order to install on GCP with Daffy, you need to create a service account that has the correct permission to install. https://cloud.google.com/iam/docs/creating-managing-service-account-keys#creating Permission: Within your GCP project, you would need to go to IAM Section and create/use Service Account. From the requirements doc , make sure your service account has the correct permissions. API Services Enabled: For each GCP project, you need to enable API access. Within your GCP project, you would need to enable each API needed for the OpenShift install. From the requirements doc , you can search for each API and confirm/enable each API Service. Quota: For each GCP project, you need to enable correct amount of quota to run openshift. Within your GCP project, you would need to Verify your current quota. From the requirements doc , you can search for each quota to find current limit. Search for \"Quotas\" within your GCP project Select the \"All Quotas\" Search for the quota you are looking for Verify Limit/Used Percentage Once you open the \"All Quotas\" page, you can search for each quota to see its limits. Dedicated public host Zone: You will need to create a DNS hosted Zone project. For the OpenShift install, you need the following: Registered DNS Name - myexample.com GCP DNS Zone - myexample-com Transfer the domain to GCP Name services listed in your new GCP DNS Zone","title":"Finding Provider Details"},{"location":"Deploying-OCP/GCP/#setting-up-dns","text":"","title":"Setting up DNS"},{"location":"Deploying-OCP/GCP/#environment-file","text":"Deploying the OpenShift on GCP only requires three entries to your existing core environment file (< ENVIRONMENT_NAME >-env.sh) plus a local service account file. Note You can look in the samples directory on your bastion for example of GCP install : /data/daffy/env/samples/ gcp-ipi-env.sh You can copy the sample file to build your new environment file. cp /data/daffy/env/samples/ gcp-ipi-env.sh /data/daffy/env/< ENVIRONMENT_NAME >-env.sh Valid Options: OCP_INSTALL_TYPE= gcp-ipi GCP_PROJECT_ID= GCP_REGION = Optional: OCP_CREATE_OPENSHIFT_CONTAINER_STORAGE =true OCP_INSTALL_TYPE = \"gcp-ipi\" GCP_PROJECT_ID = \"<YourGCPProjectID>\" GCP_REGION = \"<AnyValidGCPRegion>\" #OCP_CREATE_OPENSHIFT_CONTAINER_STORAGE=true If you plan to install a cloud pak and/or need storage, you need to set the flag to setup OCS Storage. For GCP , you need to download your Service Key as well. Save this to your home path: ~/.gcp/osServiceAccount.json In your GCP project, go to IAM and Select Service accounts Select or create new Service Account From the Service Account, select the Keys tab to create new key. *** FYI after you create the key, you can not view/download it. You can only get the details at the time of creation","title":"Environment File"},{"location":"Deploying-OCP/GCP/#execution","text":"To deploy your OCP cluster to GCP, run the build.sh script from the /data/daffy/ocp directory: /data/daffy/ocp/build.sh <ENVIRONMENT_NAME> Once your cluster is fully deployed, you can access the help menu which has a number of options. Note <ENVIRONMENT_NAME> is the first part of your name that you used for the -env.sh file /data/daffy/ocp/build.sh <ENVIRONMENT_NAME> --help Installing Cloud Paks","title":"Execution"},{"location":"Deploying-OCP/HCCX-gym/","text":"document.title = \"Deploy OCP - HCCX\"; HCCX Gym \u00b6 Overview \u00b6 The OpenShift Gym is a learning environment offering individuals the ability to deploy virtual machines to support installation of IBM technologies such as OpenShift and Cloud Paks. The HCCX Gym has documentation on using Daffy, below are some shortcuts for their instructions. It's the same basic instructions based on VSphere, but they have some pre-built steps for Gym Members. Main Gym Page VMware IPI deployment - using DAFFY Prerequisites \u00b6 Warning For internal IBM use only, Links may only work while in the IBM Network A Gym Membership request can be made by filling out the form An active TECNet VPN ID is required to access and use the Gym An email containing details on accessing the features of the OpenShift Gym is sent to the email address supplied once provisioning is completed. General information about the environment supplied is listed below. Refer to the provisioning email for detailed information. Connection \u00b6 Make sure you are connected to the Technet VPN. During initial setup, it may require you to reset your password. Launch your preferred Terminal Termius \u00b6 On the left tab, click on hosts Click on + New Host Add the IP address that was given in the provisioning email Add password that was given in the email Change port to 32222 Standard Terminal \u00b6 ssh admin@{Server IP address} -p 32222 Set Up \u00b6 Login as root \u00b6 After logging in as admin, switch to root user. sudo su - Warning Before you can start with daffy, you must registry your RedHat Enterprise Linux(RHEL)( Here ) subscription-manager register --username <username> --password <password> --auto-attach Install latest daffy \u00b6 curl http://get.daffy-installer.com/download-scripts/daffy-init.sh | bash Copy environment file \u00b6 Next you will copy the pre-populated env file in your home directory to your Daffy env directory cp ~/vmware-ipi-env.sh /data/daffy/env/{env-name}-env.sh You may make any changes needed in this file, add cloud paks, change sizing, etc. Deploying \u00b6 You can now run the daffy process. /data/daffy/build.sh {env-name} Installing Cloud Paks","title":"HCCX Gym"},{"location":"Deploying-OCP/HCCX-gym/#hccx-gym","text":"","title":"HCCX Gym"},{"location":"Deploying-OCP/HCCX-gym/#overview","text":"The OpenShift Gym is a learning environment offering individuals the ability to deploy virtual machines to support installation of IBM technologies such as OpenShift and Cloud Paks. The HCCX Gym has documentation on using Daffy, below are some shortcuts for their instructions. It's the same basic instructions based on VSphere, but they have some pre-built steps for Gym Members. Main Gym Page VMware IPI deployment - using DAFFY","title":"Overview"},{"location":"Deploying-OCP/HCCX-gym/#prerequisites","text":"Warning For internal IBM use only, Links may only work while in the IBM Network A Gym Membership request can be made by filling out the form An active TECNet VPN ID is required to access and use the Gym An email containing details on accessing the features of the OpenShift Gym is sent to the email address supplied once provisioning is completed. General information about the environment supplied is listed below. Refer to the provisioning email for detailed information.","title":"Prerequisites"},{"location":"Deploying-OCP/HCCX-gym/#connection","text":"Make sure you are connected to the Technet VPN. During initial setup, it may require you to reset your password. Launch your preferred Terminal","title":"Connection"},{"location":"Deploying-OCP/HCCX-gym/#termius","text":"On the left tab, click on hosts Click on + New Host Add the IP address that was given in the provisioning email Add password that was given in the email Change port to 32222","title":"Termius"},{"location":"Deploying-OCP/HCCX-gym/#standard-terminal","text":"ssh admin@{Server IP address} -p 32222","title":"Standard Terminal"},{"location":"Deploying-OCP/HCCX-gym/#set-up","text":"","title":"Set Up"},{"location":"Deploying-OCP/HCCX-gym/#login-as-root","text":"After logging in as admin, switch to root user. sudo su - Warning Before you can start with daffy, you must registry your RedHat Enterprise Linux(RHEL)( Here ) subscription-manager register --username <username> --password <password> --auto-attach","title":"Login as root"},{"location":"Deploying-OCP/HCCX-gym/#install-latest-daffy","text":"curl http://get.daffy-installer.com/download-scripts/daffy-init.sh | bash","title":"Install latest daffy"},{"location":"Deploying-OCP/HCCX-gym/#copy-environment-file","text":"Next you will copy the pre-populated env file in your home directory to your Daffy env directory cp ~/vmware-ipi-env.sh /data/daffy/env/{env-name}-env.sh You may make any changes needed in this file, add cloud paks, change sizing, etc.","title":"Copy environment file"},{"location":"Deploying-OCP/HCCX-gym/#deploying","text":"You can now run the daffy process. /data/daffy/build.sh {env-name} Installing Cloud Paks","title":"Deploying"},{"location":"Deploying-OCP/IBM/","text":"document.title = \"Deploy OCP - IBM\"; IBM Install \u00b6 Platform Requirements \u00b6 To use Daffy on IBM Cloud , there are some platform info and requirements that need to be gathered or met. We have a simple doc that you should refer to that list all providers and what would be needed. For IBM , it breaks down to the following basic three items: Account Details - The account that you plan to install OpenShift Permissions - The permissions need to perform the install Cloud Internet Services - The ability to add DNS For detailed list of the above, you can find in the Daffy Provider Requirements. Please review before proceeding. https://ibm.box.com/v/DaffyProviderRequirements Finding Provider Details \u00b6 To install Daffy on IBM , the hardest part can be finding the provider details in the portal. To create or use an existing IBM API Key you can refer to this: https://cloud.ibm.com/docs/account?topic=account-userapikey&interface=ui Note: Use the Identity and Access Management (IAM) service to manage access keys. Select Manage - Access (IAM) from drop down menu, then select API keys on the left menu You can create a new access key or use an existing key. The access key must have authority to the account you wan to install OpenShift into. IBM API Key: The IBM API key is ONLY displayed at the time of creation. When you create the access key, you will then have the opportunity to capture or download the key Note This is sensitive information, please make sure you store this in a secure location The screen below is an example of what you will see when you create a NEW access Key. Region: For you to use Daffy to install on IBM you need to choose a valid region identifier. This will be the target region you are planning to deploy OpenShift into. To see a complete list of available IBM Regions, go to the following website. https://cloud.ibm.com/docs/overview?topic=overview-locations By default, Daffy sets this to us-south, but others are fully supported. Note: Take note of the region identifier such as: us-south. This is the value you would use to deploy a OCP cluster into the US South (Dallas) region. This is the default if not in your environment file Permission: Within your IBM account, you would need to go to IAM Section and make sure the user that is associated with your account is assigned the correct roles. At minimum, you need to have this role: All Account Management services = All Please see the requirements doc for more information! Hosted Zone (CIS) For each OpenShift deployment into IBM , you need to have your own Domain and a Cloud Internet Services. (CIS) service. Important : The domain you add in the CIS Service must EXACTlY match the domain you configured in the Domain Name Registration service. Important: Once you create your domain in your CIS instance, you must point your DNS registry Name Server records to the assigned IBM DNS Name Server records assigned by the CIS Service. You will see the Name Servers listed once you have added the domain to the CIS Service. Setting up DNS \u00b6 HTML Video embed Environment File \u00b6 Below is the IBM specific environment variable that must be defined in the /data/daffy/env/< ENVIRONMENT_NAME >-env.sh file: CIS_INSTANCE_NAME Note You can look in the samples directory on your bastion for example of IBM install : /data/daffy/env/samples/ ibm-ipi-env.sh You can run this command to build your new file from the sample. cp / data / daffy / env / samples / ibm - ipi - env.sh / data / daffy / env /< ENVIRONMENT_NAME >- env.sh Valid Options: OCP_INSTALL_TYPE =ibm-ipi CIS_INSTANCE_NAME =\"YOUR IBM CIS Instance\" OCP_INSTALL_TYPE = \"ibm-ipi\" CIS_INSTANCE_NAME = \"YOUR IBM CIS Instance\" #OCP_CREATE_OPENSHIFT_CONTAINER_STORAGE=true Optional: OCP_CREATE_OPENSHIFT_CONTAINER_STORAGE =true If you plan to install a cloud pak and/or need storage, you need to set the flag to setup OCS Storage. Execution \u00b6 To deploy your cluster, run the build.sh script from the /data/daffy/ocp directory: / data / daffy / ocp / build.sh < ENVIRONMENT_NAME > Once your cluster is fully deployed you can access the help menu which as a number of options. Note < environment > is the first part of your name that you used for the < environment >-env.sh file / data / daffy / ocp / build.sh < ENVIRONMENT_NAME > -- help Installing Cloud Paks","title":"IBM"},{"location":"Deploying-OCP/IBM/#ibm-install","text":"","title":"IBM Install"},{"location":"Deploying-OCP/IBM/#platform-requirements","text":"To use Daffy on IBM Cloud , there are some platform info and requirements that need to be gathered or met. We have a simple doc that you should refer to that list all providers and what would be needed. For IBM , it breaks down to the following basic three items: Account Details - The account that you plan to install OpenShift Permissions - The permissions need to perform the install Cloud Internet Services - The ability to add DNS For detailed list of the above, you can find in the Daffy Provider Requirements. Please review before proceeding. https://ibm.box.com/v/DaffyProviderRequirements","title":"Platform Requirements"},{"location":"Deploying-OCP/IBM/#finding-provider-details","text":"To install Daffy on IBM , the hardest part can be finding the provider details in the portal. To create or use an existing IBM API Key you can refer to this: https://cloud.ibm.com/docs/account?topic=account-userapikey&interface=ui Note: Use the Identity and Access Management (IAM) service to manage access keys. Select Manage - Access (IAM) from drop down menu, then select API keys on the left menu You can create a new access key or use an existing key. The access key must have authority to the account you wan to install OpenShift into. IBM API Key: The IBM API key is ONLY displayed at the time of creation. When you create the access key, you will then have the opportunity to capture or download the key Note This is sensitive information, please make sure you store this in a secure location The screen below is an example of what you will see when you create a NEW access Key. Region: For you to use Daffy to install on IBM you need to choose a valid region identifier. This will be the target region you are planning to deploy OpenShift into. To see a complete list of available IBM Regions, go to the following website. https://cloud.ibm.com/docs/overview?topic=overview-locations By default, Daffy sets this to us-south, but others are fully supported. Note: Take note of the region identifier such as: us-south. This is the value you would use to deploy a OCP cluster into the US South (Dallas) region. This is the default if not in your environment file Permission: Within your IBM account, you would need to go to IAM Section and make sure the user that is associated with your account is assigned the correct roles. At minimum, you need to have this role: All Account Management services = All Please see the requirements doc for more information! Hosted Zone (CIS) For each OpenShift deployment into IBM , you need to have your own Domain and a Cloud Internet Services. (CIS) service. Important : The domain you add in the CIS Service must EXACTlY match the domain you configured in the Domain Name Registration service. Important: Once you create your domain in your CIS instance, you must point your DNS registry Name Server records to the assigned IBM DNS Name Server records assigned by the CIS Service. You will see the Name Servers listed once you have added the domain to the CIS Service.","title":"Finding Provider Details"},{"location":"Deploying-OCP/IBM/#setting-up-dns","text":"HTML Video embed","title":"Setting up DNS"},{"location":"Deploying-OCP/IBM/#environment-file","text":"Below is the IBM specific environment variable that must be defined in the /data/daffy/env/< ENVIRONMENT_NAME >-env.sh file: CIS_INSTANCE_NAME Note You can look in the samples directory on your bastion for example of IBM install : /data/daffy/env/samples/ ibm-ipi-env.sh You can run this command to build your new file from the sample. cp / data / daffy / env / samples / ibm - ipi - env.sh / data / daffy / env /< ENVIRONMENT_NAME >- env.sh Valid Options: OCP_INSTALL_TYPE =ibm-ipi CIS_INSTANCE_NAME =\"YOUR IBM CIS Instance\" OCP_INSTALL_TYPE = \"ibm-ipi\" CIS_INSTANCE_NAME = \"YOUR IBM CIS Instance\" #OCP_CREATE_OPENSHIFT_CONTAINER_STORAGE=true Optional: OCP_CREATE_OPENSHIFT_CONTAINER_STORAGE =true If you plan to install a cloud pak and/or need storage, you need to set the flag to setup OCS Storage.","title":"Environment File"},{"location":"Deploying-OCP/IBM/#execution","text":"To deploy your cluster, run the build.sh script from the /data/daffy/ocp directory: / data / daffy / ocp / build.sh < ENVIRONMENT_NAME > Once your cluster is fully deployed you can access the help menu which as a number of options. Note < environment > is the first part of your name that you used for the < environment >-env.sh file / data / daffy / ocp / build.sh < ENVIRONMENT_NAME > -- help Installing Cloud Paks","title":"Execution"},{"location":"Deploying-OCP/Pre-Built-Cluster/","text":"","title":"Pre Built Cluster"},{"location":"Deploying-OCP/Pre-Req/","text":"document.title = \"Deploy OCP - Pre-Req\"; Daffy Pre-Requirements \u00b6 What is required to use Daffy? \u00b6 Before you can use the Daffy scripts, you must have the following: SSH client on your local workstation \u00b6 We highly recommend installing Termius as your SSH client The Termius installer can be found here: Windows or Mac (only the free version is needed) A Bastion Machine \u00b6 Create Bastion Instructions Ubuntu 20.04 (Minimum Requirements: 2 CPU, 2GB Memory) with full root access (VSphere-UPI, -IPI and -MSP) Ubuntu 20.04 (Minimum Requirements: 60+ CPU, 128GB+ Memory) with full root access (KVM-UPI) Ubuntu 20.04 (Minimum Requirements: 4+ CPU, 32GB+ Memory, 100 GB Disk) with full root access (AirGap Bastion/Jump Box Mirror Registry) RHEL 8.X (Minimum Requirements: 2 CPU, 2GB Memory) with full root access (VSphere-UPI, -IPI and -MSP) RHEL 8.X (Minimum Requirements: 4+ CPU, 32GB+ Memory, 100 GB Disk) with full root access (AirGap Bastion/Jump Box Mirror Registry) Red Hat pull secret \u00b6 If you or your customer does not have a Red Hat pull secret: Sign up for 60 day trail for OpenShift: RedHat Pull secret Site Important: If you're installing on a customer owned platform account or an on-prem customer datacenter, you MUST instruct your customer to register for a trial account and use their pull secret for the install. Do not use your own pull secret for customer engagements. \u200b\u200b\u200b\u200b Sign up for IBM/Red Hat partner program NOTE: All IBMers are entitled to the Red Hat partner program. Your Red Hat pull secret can ONLY be used for training and demo purposes. Do not provide your personal pull secret to customers. If you have a Red Hat account, you can find your existing pull secret here: Login to Red Hat Scroll down the page until you see \"Tokens\" and download the pull secret Accessing Red Hat entitlements from your IBM Cloud Paks: Accessing-red-hat-entitlements-from-your-cloud-paks IBM Entitlement Key \u00b6 If you need to get your own IBM entitlement key, you can get it here Copy to clipboard and save to a local file If you need create one for a customer, you can submit a request here Customers can use these links to request their own trial keys here Core Steps","title":"Pre-Req"},{"location":"Deploying-OCP/Pre-Req/#daffy-pre-requirements","text":"","title":"Daffy Pre-Requirements"},{"location":"Deploying-OCP/Pre-Req/#what-is-required-to-use-daffy","text":"Before you can use the Daffy scripts, you must have the following:","title":"What is required to use Daffy?"},{"location":"Deploying-OCP/Pre-Req/#ssh-client-on-your-local-workstation","text":"We highly recommend installing Termius as your SSH client The Termius installer can be found here: Windows or Mac (only the free version is needed)","title":"SSH client on your local workstation"},{"location":"Deploying-OCP/Pre-Req/#a-bastion-machine","text":"Create Bastion Instructions Ubuntu 20.04 (Minimum Requirements: 2 CPU, 2GB Memory) with full root access (VSphere-UPI, -IPI and -MSP) Ubuntu 20.04 (Minimum Requirements: 60+ CPU, 128GB+ Memory) with full root access (KVM-UPI) Ubuntu 20.04 (Minimum Requirements: 4+ CPU, 32GB+ Memory, 100 GB Disk) with full root access (AirGap Bastion/Jump Box Mirror Registry) RHEL 8.X (Minimum Requirements: 2 CPU, 2GB Memory) with full root access (VSphere-UPI, -IPI and -MSP) RHEL 8.X (Minimum Requirements: 4+ CPU, 32GB+ Memory, 100 GB Disk) with full root access (AirGap Bastion/Jump Box Mirror Registry)","title":"A Bastion Machine"},{"location":"Deploying-OCP/Pre-Req/#red-hat-pull-secret","text":"If you or your customer does not have a Red Hat pull secret: Sign up for 60 day trail for OpenShift: RedHat Pull secret Site Important: If you're installing on a customer owned platform account or an on-prem customer datacenter, you MUST instruct your customer to register for a trial account and use their pull secret for the install. Do not use your own pull secret for customer engagements. \u200b\u200b\u200b\u200b Sign up for IBM/Red Hat partner program NOTE: All IBMers are entitled to the Red Hat partner program. Your Red Hat pull secret can ONLY be used for training and demo purposes. Do not provide your personal pull secret to customers. If you have a Red Hat account, you can find your existing pull secret here: Login to Red Hat Scroll down the page until you see \"Tokens\" and download the pull secret Accessing Red Hat entitlements from your IBM Cloud Paks: Accessing-red-hat-entitlements-from-your-cloud-paks","title":"Red Hat pull secret"},{"location":"Deploying-OCP/Pre-Req/#ibm-entitlement-key","text":"If you need to get your own IBM entitlement key, you can get it here Copy to clipboard and save to a local file If you need create one for a customer, you can submit a request here Customers can use these links to request their own trial keys here Core Steps","title":"IBM Entitlement Key"},{"location":"Deploying-OCP/ROKS/","text":"document.title = \"Deploy OCP - ROKS\"; ROKS Install \u00b6 Warning Only ROKS type of classic or satellite supported today. VPC Type not Supported At this point, you have a bastion machine where you have installed the Daffy tool, created your core environment-name -env.sh and can execute the install of OCP on ROKS. Platform Requirements \u00b6 To use Daffy to provision R ed H at O penShift K ubernetes S ervices on IBM Cloud (ROKS), there are some platform info and requirements that need to be gathered or met. We have a simple doc that you should refer to that lists all providers and what would be needed. For ROKS, it breaks down to the following basic two items: Account Details - The account that you plan to install ROKS on Account Type - The account type needed to perform the install For a detailed list of the above, you can read the Daffy Provider Requirements. Please review before proceeding. https://ibm.box.com/v/DaffyProviderRequirements Finding Provider Details \u00b6 To use Daffy to install ROKS, you must find the provider details. Luckily, Daffy automatically walks you through this process using IBM Cloud CLI. Below are the steps you can use to make sure you use the right information. Account: To find more details on managing your IBM Cloud account, you can refer to this: https://cloud.ibm.com/docs/account?topic=account-account-getting-started You must have an IBMid before logging in. The link above can help create one. If you are an IBM employee, your IBMid is most likely some numbers followed by your name. You can list your Account ID from the drop down: Location/Zone: To find a list of available data center locations/zones, you can refer to this: https://cloud.ibm.com/docs/overview?topic=overview-locations#mzr-table Note Daffy currently only supports single datacenter location installs with classic infrastructure Zones \u00b6 Regions are collections of zones. Zones have high-bandwidth, low-latency network connections to other zones in the same region. In order to deploy fault-tolerant applications that have high availability, IBM recommends deploying applications across multiple zones and multiple regions. This helps protect against unexpected failures of components, up to and including a single zone or region. Choose regions that makes sense for your scenario. For example, if you only have customers in the US, or if you have specific needs that require your data to live in the US, it makes sense to store your resources in zones in the dal13 zone or in the wdc07 zone. Daffy currently defaults to dal13 when deploying a ROKS cluster. https://cloud.ibm.com/docs/containers?topic=containers-regions-and-zones#locations Account types \u00b6 Your IBM Cloud account includes many interacting components and systems for resource, user, and access management. Concepts like how certain components are connected or how access works help you in understanding how to set up your account type. Many features are free to use regardless of account type. Account Type: For you to use Daffy to install on ROKS, you need to have a Pay-As-You-Go or subscription IBM Cloud account: https://cloud.ibm.com/docs/account?topic=account-accounts Environment File \u00b6 Deploying OpenShift on ROKS only requires one entry to your existing core environment file (< ENVIRONMENT_NAME >-env.sh). Note You can look in the samples directory on your bastion for example of ROKS install : /data/daffy/env/samples/roks-msp-env.sh You can copy the sample file to build your new environment file. cp /data/daffy/env/samples/roks-msp-env.sh /data/daffy/env/<ENVIRONMENT_NAME>-env.sh Valid Options: OCP_INSTALL_TYPE= roks-msp Optional: ROKS_ZONE=dal13 OCP_INSTALL_TYPE = \"roks-msp\" #ROKS_ZONE=\"dal13\" Execution \u00b6 To deploy your OCP cluster to ROKS, run the build.sh script from the /data/daffy/ocp directory. The installer will ask you a number of questions to login to IBM Cloud via the CLI. When prompted with a region, select any but stay within your geography. For instance, us-south. This is used to talk with IBM Cloud via the right API endpoint. /data/daffy/ocp/build.sh <ENVIRONMENT_NAME> Once your cluster is fully deployed, you can access the help menu which has a number of options. Note <ENVIRONMENT_NAME> is the first part of your name that you used for the -env.sh file /data/daffy/ocp/build.sh <ENVIRONMENT_NAME> --help Here is a full example for deploying OpenShift on ROKS with the Daffy process. Installing Cloud Paks","title":"ROKS"},{"location":"Deploying-OCP/ROKS/#roks-install","text":"Warning Only ROKS type of classic or satellite supported today. VPC Type not Supported At this point, you have a bastion machine where you have installed the Daffy tool, created your core environment-name -env.sh and can execute the install of OCP on ROKS.","title":"ROKS Install"},{"location":"Deploying-OCP/ROKS/#platform-requirements","text":"To use Daffy to provision R ed H at O penShift K ubernetes S ervices on IBM Cloud (ROKS), there are some platform info and requirements that need to be gathered or met. We have a simple doc that you should refer to that lists all providers and what would be needed. For ROKS, it breaks down to the following basic two items: Account Details - The account that you plan to install ROKS on Account Type - The account type needed to perform the install For a detailed list of the above, you can read the Daffy Provider Requirements. Please review before proceeding. https://ibm.box.com/v/DaffyProviderRequirements","title":"Platform Requirements"},{"location":"Deploying-OCP/ROKS/#finding-provider-details","text":"To use Daffy to install ROKS, you must find the provider details. Luckily, Daffy automatically walks you through this process using IBM Cloud CLI. Below are the steps you can use to make sure you use the right information. Account: To find more details on managing your IBM Cloud account, you can refer to this: https://cloud.ibm.com/docs/account?topic=account-account-getting-started You must have an IBMid before logging in. The link above can help create one. If you are an IBM employee, your IBMid is most likely some numbers followed by your name. You can list your Account ID from the drop down: Location/Zone: To find a list of available data center locations/zones, you can refer to this: https://cloud.ibm.com/docs/overview?topic=overview-locations#mzr-table Note Daffy currently only supports single datacenter location installs with classic infrastructure","title":"Finding Provider Details"},{"location":"Deploying-OCP/ROKS/#zones","text":"Regions are collections of zones. Zones have high-bandwidth, low-latency network connections to other zones in the same region. In order to deploy fault-tolerant applications that have high availability, IBM recommends deploying applications across multiple zones and multiple regions. This helps protect against unexpected failures of components, up to and including a single zone or region. Choose regions that makes sense for your scenario. For example, if you only have customers in the US, or if you have specific needs that require your data to live in the US, it makes sense to store your resources in zones in the dal13 zone or in the wdc07 zone. Daffy currently defaults to dal13 when deploying a ROKS cluster. https://cloud.ibm.com/docs/containers?topic=containers-regions-and-zones#locations","title":"Zones"},{"location":"Deploying-OCP/ROKS/#account-types","text":"Your IBM Cloud account includes many interacting components and systems for resource, user, and access management. Concepts like how certain components are connected or how access works help you in understanding how to set up your account type. Many features are free to use regardless of account type. Account Type: For you to use Daffy to install on ROKS, you need to have a Pay-As-You-Go or subscription IBM Cloud account: https://cloud.ibm.com/docs/account?topic=account-accounts","title":"Account types"},{"location":"Deploying-OCP/ROKS/#environment-file","text":"Deploying OpenShift on ROKS only requires one entry to your existing core environment file (< ENVIRONMENT_NAME >-env.sh). Note You can look in the samples directory on your bastion for example of ROKS install : /data/daffy/env/samples/roks-msp-env.sh You can copy the sample file to build your new environment file. cp /data/daffy/env/samples/roks-msp-env.sh /data/daffy/env/<ENVIRONMENT_NAME>-env.sh Valid Options: OCP_INSTALL_TYPE= roks-msp Optional: ROKS_ZONE=dal13 OCP_INSTALL_TYPE = \"roks-msp\" #ROKS_ZONE=\"dal13\"","title":"Environment File"},{"location":"Deploying-OCP/ROKS/#execution","text":"To deploy your OCP cluster to ROKS, run the build.sh script from the /data/daffy/ocp directory. The installer will ask you a number of questions to login to IBM Cloud via the CLI. When prompted with a region, select any but stay within your geography. For instance, us-south. This is used to talk with IBM Cloud via the right API endpoint. /data/daffy/ocp/build.sh <ENVIRONMENT_NAME> Once your cluster is fully deployed, you can access the help menu which has a number of options. Note <ENVIRONMENT_NAME> is the first part of your name that you used for the -env.sh file /data/daffy/ocp/build.sh <ENVIRONMENT_NAME> --help Here is a full example for deploying OpenShift on ROKS with the Daffy process. Installing Cloud Paks","title":"Execution"},{"location":"Deploying-OCP/ROSA/","text":"document.title = \"Deploy OCP - AWS ROSA\"; ROSA Install \u00b6 At this point, you have a bastion machine where you have installed the Daffy tool, and ready to created your core environment-name -env.sh and so you can execute the install of OCP on AWS via ROSA. Platform Requirements \u00b6 To use Daffy on AWS , there are some platform info and requirements that need to be gathered or met. We have a simple doc that you should refer to that list all providers and what would be needed. For AWS ROSA , it breaks down to the following basic three items: Account Details - The account that you plan to install OpenShift Permissions - The permissions need to perform the install Quota - The ability to add new workload to that platform For detailed list of ROSA instructions, refer to the ROSA documentation. https://docs.openshift.com/rosa/welcome/index.html For detailed list of the above, you can find in the Daffy Provider Requirements. Please review before proceeding. https://ibm.box.com/v/DaffyProviderRequirements One time ROSA setup \u00b6 First login to the AWS Console, and search for ROSA service. Click Enable OpenShift button in the ROSA Services page. Next, go to the Red Hat token page to get a ROSA token for login and generate or load an OpenShift Cluster Manager API Token. Daffy scripts will prompt you for this token at install time. Note You must have a valid Red Hat login id. You can signup with any email address https://console.redhat.com/openshift/token/rosa Finding Provider Details \u00b6 To install OpenShift on AWS ROSA using Daffy, the hardest part can be finding the provider details. To create or use an existing AWS Access Key ID you can refer to this: https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_access-keys.html?icmpid=docs_iam_console#Using_CreateAccessKey Note: Use the Identity and Access Management (IAM) service to manage access keys. Select Search - find IAM service You can create a new access key or use an existing key. The access key must have authority to the account you wan to install OpenShift into. Secret Access Key: The secret access key is ONLY displayed at the time of creation. When you create the access key, you will then have the opportunity to capture the secret access key Note This is sensitive information, please make sure you store this in a secure location The screen to the right is an example of what you will see when you create a NEW access Key. Region: For you to use Daffy to install on AWS you need to choose a valid region identifier. This will be the target region you are planning to deploy OpenShift into. To see a complete list of available AWS Regions, you can select the region drop down list in the AWA Portal. This will be in the upper right hand corner next to your account name. (See picture to the right) Note: Take note of the region identifier such as: us-east-2. This is the value you would use to deploy a OCP cluster into the US East (Ohio) region. Permission: Within your AWS project, you would need to go to IAM Section and make sure the user that is associated with your Access Key is assigned the correct roles. At minimum, you need to have this role: AdministratorAccess Please see the requirements doc for more information! Environment File \u00b6 Deploying the OpenShift on AWS only requires three entries to your existing core environment file (< ENVIRONMENT_NAME >-env.sh). Note You can look in the samples directory on your bastion for example of ROSA install : /data/daffy/env/samples/ rosa-msp-env.sh You can copy the sample file to build your new environment file: cp / data / daffy / env / samples / rosa - msp - env.sh / data / daffy / env /< ENVIRONMENT_NAME >- env.sh Valid Options: Variable Name Info Default Value Required OCP_INSTALL_TYPE Install type must be rosa-msp Yes AWS_REGION AWS region you want to deploy to Yes AWS_ACCESS_KEY_ID Your AWS Access Key ID Yes AWS_CREATE_EFS_STORAGE Do you want to create EFS storage false No #ROSA Base Settings #################### OCP_INSTALL_TYPE = \"rosa-msp\" AWS_REGION = \"us-east-2\" AWS_ACCESS_KEY_ID = \"123YOURACCESSKEYID12\" #Enable Features ############################# #AWS_CREATE_EFS_STORAGE=\"true\" #ROSA Override Settings #################### Storage If you plan to install a cloud pak and/or need storage, ODF/OCS is not currently supportd on ROSA. If you want daffy to setup the EFS Storage for you, you need to set the AWS_CREATE_EFS_STORAGE to true. Or you can build our use some other Storage provider. Client Secret It will prompt you for the Client Secret during the install Execution \u00b6 To deploy your OCP cluster to AWS ROSA , run the build.sh script from the /data/daffy/ocp directory: / data / daffy / ocp / build.sh < ENVIRONMENT_NAME > Once your cluster is fully deployed you can access the help menu which has a number of options. Note < ENVIRONMENT_NAME > is the first part of your name that you used for the < ENVIRONMENT_NAME >-env.sh file Installing Cloud Paks","title":"ROSA"},{"location":"Deploying-OCP/ROSA/#rosa-install","text":"At this point, you have a bastion machine where you have installed the Daffy tool, and ready to created your core environment-name -env.sh and so you can execute the install of OCP on AWS via ROSA.","title":"ROSA Install"},{"location":"Deploying-OCP/ROSA/#platform-requirements","text":"To use Daffy on AWS , there are some platform info and requirements that need to be gathered or met. We have a simple doc that you should refer to that list all providers and what would be needed. For AWS ROSA , it breaks down to the following basic three items: Account Details - The account that you plan to install OpenShift Permissions - The permissions need to perform the install Quota - The ability to add new workload to that platform For detailed list of ROSA instructions, refer to the ROSA documentation. https://docs.openshift.com/rosa/welcome/index.html For detailed list of the above, you can find in the Daffy Provider Requirements. Please review before proceeding. https://ibm.box.com/v/DaffyProviderRequirements","title":"Platform Requirements"},{"location":"Deploying-OCP/ROSA/#one-time-rosa-setup","text":"First login to the AWS Console, and search for ROSA service. Click Enable OpenShift button in the ROSA Services page. Next, go to the Red Hat token page to get a ROSA token for login and generate or load an OpenShift Cluster Manager API Token. Daffy scripts will prompt you for this token at install time. Note You must have a valid Red Hat login id. You can signup with any email address https://console.redhat.com/openshift/token/rosa","title":"One time ROSA setup"},{"location":"Deploying-OCP/ROSA/#finding-provider-details","text":"To install OpenShift on AWS ROSA using Daffy, the hardest part can be finding the provider details. To create or use an existing AWS Access Key ID you can refer to this: https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_access-keys.html?icmpid=docs_iam_console#Using_CreateAccessKey Note: Use the Identity and Access Management (IAM) service to manage access keys. Select Search - find IAM service You can create a new access key or use an existing key. The access key must have authority to the account you wan to install OpenShift into. Secret Access Key: The secret access key is ONLY displayed at the time of creation. When you create the access key, you will then have the opportunity to capture the secret access key Note This is sensitive information, please make sure you store this in a secure location The screen to the right is an example of what you will see when you create a NEW access Key. Region: For you to use Daffy to install on AWS you need to choose a valid region identifier. This will be the target region you are planning to deploy OpenShift into. To see a complete list of available AWS Regions, you can select the region drop down list in the AWA Portal. This will be in the upper right hand corner next to your account name. (See picture to the right) Note: Take note of the region identifier such as: us-east-2. This is the value you would use to deploy a OCP cluster into the US East (Ohio) region. Permission: Within your AWS project, you would need to go to IAM Section and make sure the user that is associated with your Access Key is assigned the correct roles. At minimum, you need to have this role: AdministratorAccess Please see the requirements doc for more information!","title":"Finding Provider Details"},{"location":"Deploying-OCP/ROSA/#environment-file","text":"Deploying the OpenShift on AWS only requires three entries to your existing core environment file (< ENVIRONMENT_NAME >-env.sh). Note You can look in the samples directory on your bastion for example of ROSA install : /data/daffy/env/samples/ rosa-msp-env.sh You can copy the sample file to build your new environment file: cp / data / daffy / env / samples / rosa - msp - env.sh / data / daffy / env /< ENVIRONMENT_NAME >- env.sh Valid Options: Variable Name Info Default Value Required OCP_INSTALL_TYPE Install type must be rosa-msp Yes AWS_REGION AWS region you want to deploy to Yes AWS_ACCESS_KEY_ID Your AWS Access Key ID Yes AWS_CREATE_EFS_STORAGE Do you want to create EFS storage false No #ROSA Base Settings #################### OCP_INSTALL_TYPE = \"rosa-msp\" AWS_REGION = \"us-east-2\" AWS_ACCESS_KEY_ID = \"123YOURACCESSKEYID12\" #Enable Features ############################# #AWS_CREATE_EFS_STORAGE=\"true\" #ROSA Override Settings #################### Storage If you plan to install a cloud pak and/or need storage, ODF/OCS is not currently supportd on ROSA. If you want daffy to setup the EFS Storage for you, you need to set the AWS_CREATE_EFS_STORAGE to true. Or you can build our use some other Storage provider. Client Secret It will prompt you for the Client Secret during the install","title":"Environment File"},{"location":"Deploying-OCP/ROSA/#execution","text":"To deploy your OCP cluster to AWS ROSA , run the build.sh script from the /data/daffy/ocp directory: / data / daffy / ocp / build.sh < ENVIRONMENT_NAME > Once your cluster is fully deployed you can access the help menu which has a number of options. Note < ENVIRONMENT_NAME > is the first part of your name that you used for the < ENVIRONMENT_NAME >-env.sh file Installing Cloud Paks","title":"Execution"},{"location":"Deploying-OCP/Restricted-Network/","text":"document.title = \"Deploy OCP - Restricted Network\"; Slide Show \u00b6 * {box-sizing:border-box} /* Slideshow container */ .slideshow-container { max-width: 1000px; position: relative; margin: auto; } /* Hide the images by default */ .mySlides { display: none; } /* Next & previous buttons */ .prev, .next { cursor: pointer; position: absolute; top: 50%; width: auto; margin-top: -22px; padding: 16px; color: white; font-weight: bold; font-size: 18px; transition: 0.6s ease; border-radius: 0 3px 3px 0; user-select: none; } /* Position the \"next button\" to the right */ .next { right: 0; border-radius: 3px 0 0 3px; } /* On hover, add a black background color with a little bit see-through */ .prev:hover, .next:hover { background-color: rgba(0,0,0,0.8); } /* Caption text */ .text { color: #f2f2f2; font-size: 15px; padding: 8px 12px; position: absolute; bottom: 8px; width: 100%; text-align: center; } /* Number text (1/3 etc) */ .numbertext { color: #f2f2f2; font-size: 12px; padding: 8px 12px; position: absolute; top: 0; } /* The dots/bullets/indicators */ .dot { cursor: pointer; height: 15px; width: 15px; margin: 0 2px; background-color: #bbb; border-radius: 50%; display: inline-block; transition: background-color 0.6s ease; } .active, .dot:hover { background-color: #717171; } /* Fading animation */ .fade { animation-name: fade; animation-duration: 1.5s; } @keyframes fade { from {opacity: .4} to {opacity: 1} } 1 / 3 Intro 1 / 3 Caption Text 1 / 3 Caption Text 1 / 3 Caption Text 1 / 3 Caption Text 1 / 3 Caption Text 1 / 3 Caption Text 1 / 3 Caption Text 1 / 3 Caption Text \u276e \u276f let slideIndex = 1; showSlides(slideIndex); // Next/previous controls function plusSlides(n) { showSlides(slideIndex += n); } // Thumbnail image controls function currentSlide(n) { showSlides(slideIndex = n); } function showSlides(n) { let i; let slides = document.getElementsByClassName(\"mySlides\"); let dots = document.getElementsByClassName(\"dot\"); if (n > slides.length) {slideIndex = 1} if (n < 1) {slideIndex = slides.length} for (i = 0; i < slides.length; i++) { slides[i].style.display = \"none\"; } for (i = 0; i < dots.length; i++) { dots[i].className = dots[i].className.replace(\" active\", \"\"); } slides[slideIndex-1].style.display = \"block\"; dots[slideIndex-1].className += \" active\"; } Download Slides Overview \u00b6 Deploying OpenShift in a restricted network (air gap), can be done with the Daffy Restricted Network scripts. This page will primarily focus on the steps to perform an air gap install. However, please note the Proxy Install approach can be a viable option and is a easier approach. Regardless of which one you choose, both install options will result in an OpenShift cluster with restricted network access. Also, note that the air gap install is more complicated and will require a JUMP BOX that has internet access to the Quay Repository. There are two basic options for deployment Option 1. Air gap install Option 2. Proxy install Proxy Install \u00b6 OpenShift environments can deny direct access to the internet and instead have an HTTP or HTTPS proxy available. This can be done during an cluster installation. This would allow the cluster to be in a restricted network but gain access to public registries via proxy server. The bastion and cluster you build must also have direct access to the proxy server. This proxy server can then control what access is granted via whitelist. If you are doing proxy install, you only need one bastion and no jump box. This bastion must have access to the restricted network and the proxy server as well. #Airgap - Proxy Install# ###################################################################################### OCP_PROXY_HTTP_PROXY = OCP_PROXY_HTTPS_PROXY = OCP_PROXY_NO_PROXY = Info At this point for a proxy install, you can skip the following steps and move to the last step and install your cluster. Airgap Install \u00b6 OpenShift environments can deny direct access to the internet completely. With this method you must replicate or mirror the public registries local in your restricted network. To do this we will use the following terms: Jump box - machine that will have direct access to the internet to mirror (download) public registry software. Bastion - machine that will install the cluster and have direct access to the restricted network where the cluster will run. Requirements \u00b6 Jump Box - 4X32 300GB Disk (disk depending on what you plan to mirror) Default will need storage /data/export (100GB) and /mirror/registry (100GB) Bastion - 4X32 300GB Disk (disk depending on what you plan to mirror) Default will need storage /data/import (100GB) and /mirror/registry (100GB) Environment File \u00b6 You should build your environment file based on the final install you want to build and should include those values in this file as well. The following values are required specifically for airgap install.(Repo Build and Airgap) Variable Name Info Install Type Required default LOCAL_REGISTRY_ENABLED Do you want this to be an air gap install Both Yes false LOCAL_REGISTRY_DNS_NAME This is your jump box DNS name(can Be IP value) Both Yes LOCAL_REGISTRY_IP This is your jump box IP address Both Yes LOCAL_AIRGAP_REGISTRY_DNS_NAME This is your restricted network bastion DNS name(can Be IP value) airgap Yes(if Airgap) LOCAL_AIRGAP_REGISTRY_IP This is your restricted network bastion box IP address airgap Yes(if Airgap) #Local Registry Info ############################### LOCAL_REGISTRY_ENABLED = \"true\" LOCAL_REGISTRY_DNS_NAME = \"<LOCAL IP>\" LOCAL_REGISTRY_IP = \"<LOCAL IP>\" #LOCAL_AIRGAP_REGISTRY_DNS_NAME=\"<Optional DNS Name>\" #LOCAL_AIRGAP_REGISTRY_IP=\"<Optional DNS Name>\" Repo Only \u00b6 If you want to just build a repo on your internet facing bastion, here is only settings you need to mirror. #Local Registry Info ############################### LOCAL_REGISTRY_ENABLED = \"true\" LOCAL_REGISTRY_IP = \"<LOCAL IP>\" #LOCAL_REGISTRY_DNS_NAME=\"<Optional DNS Name>\" Overrides \u00b6 Current values that you can add to your own environment file to override if needed but not required: Click to expand! #Catalogs to mirror #################### OCP_CATALOG_MIRRORS = \"compliance-operator,container-security-operator,file-integrity-operator,local-storage-operator,ocs-operator\" #Directory Info #################### OCP_REGISTRY_ROOT = \"/mirror/registry\" OCP_AIRGAP_EXPORT = \"${DATA_DIR}/export/airgap\" OCP_AIRGAP_IMPORT = \"${DATA_DIR}/import/airgap\" OCP_AIRGAP_EXPORT_FREE_DISK_SIZE_NEEDED = \"100\" OCP_AIRGAP_IMPORT_FREE_DISK_SIZE_NEEDED = \"100\" #Cert Info #################### CA_CERT_OU = \"ca.${CLUSTER_NAME}.${BASE_DOMAIN}\" LOCAL_REGISTRY_CERTS_FOLDER = \"${DATA_DIR}/${PROJECT_NAME}/certs\" #Registry Info #################### LOCAL_REGISTRY_PORT = \"8443\" LOCAL_OCP_REPOSITORY_NAME = \"ocp4/openshift4\" LOCAL_OLM_MIRROR_REPOSITORY_NAME = \"olm-mirror\" Mirror locally \u00b6 From your jump box run the following command: /data/daffy/ocp/registry/build.sh <ENVIRONMENT_NAME> Export Files \u00b6 After the first command runs, it will display all the files that it created and you will now need to move them to your bastion box in the restricted network. You can do this via portable USB disk, scp, etc. You just need to move these files any way you can to your restricted network bastion. Move Files \u00b6 In our example we will move via scp because our jump box has access to the bastion. This could be via firewall or it has dual NIC cards (Public Nic/Private Nic). ssh <BASTION-IP> mkdir -p /data/import/airgap scp /data/export/airgap/* <BASTION-IP>:/data/import/airgap Prepare Bastion \u00b6 Once all the files are on the bastion in the restricted network, you can run the script that was built from the previous step and copied over. This will untar all files, install all command line tools, and also install daffy locally. Info It does not mirror the registry or build the local registry, but gets the bastion ready for that next step. /data/import/airgap/airgap-prep.sh Build Local Mirror \u00b6 From your bastion box run the following command: /data/daffy/ocp/registry/build.sh <ENVIRONMENT_NAME> Install Cluster \u00b6 Now you would just follow the normal steps to build your OpenShift registry. /data/daffy/ocp/build.sh <ENVIRONMENT_NAME> Demo Video \u00b6 HTML Video embed","title":"Air Gap"},{"location":"Deploying-OCP/Restricted-Network/#slide-show","text":"* {box-sizing:border-box} /* Slideshow container */ .slideshow-container { max-width: 1000px; position: relative; margin: auto; } /* Hide the images by default */ .mySlides { display: none; } /* Next & previous buttons */ .prev, .next { cursor: pointer; position: absolute; top: 50%; width: auto; margin-top: -22px; padding: 16px; color: white; font-weight: bold; font-size: 18px; transition: 0.6s ease; border-radius: 0 3px 3px 0; user-select: none; } /* Position the \"next button\" to the right */ .next { right: 0; border-radius: 3px 0 0 3px; } /* On hover, add a black background color with a little bit see-through */ .prev:hover, .next:hover { background-color: rgba(0,0,0,0.8); } /* Caption text */ .text { color: #f2f2f2; font-size: 15px; padding: 8px 12px; position: absolute; bottom: 8px; width: 100%; text-align: center; } /* Number text (1/3 etc) */ .numbertext { color: #f2f2f2; font-size: 12px; padding: 8px 12px; position: absolute; top: 0; } /* The dots/bullets/indicators */ .dot { cursor: pointer; height: 15px; width: 15px; margin: 0 2px; background-color: #bbb; border-radius: 50%; display: inline-block; transition: background-color 0.6s ease; } .active, .dot:hover { background-color: #717171; } /* Fading animation */ .fade { animation-name: fade; animation-duration: 1.5s; } @keyframes fade { from {opacity: .4} to {opacity: 1} } 1 / 3 Intro 1 / 3 Caption Text 1 / 3 Caption Text 1 / 3 Caption Text 1 / 3 Caption Text 1 / 3 Caption Text 1 / 3 Caption Text 1 / 3 Caption Text 1 / 3 Caption Text \u276e \u276f let slideIndex = 1; showSlides(slideIndex); // Next/previous controls function plusSlides(n) { showSlides(slideIndex += n); } // Thumbnail image controls function currentSlide(n) { showSlides(slideIndex = n); } function showSlides(n) { let i; let slides = document.getElementsByClassName(\"mySlides\"); let dots = document.getElementsByClassName(\"dot\"); if (n > slides.length) {slideIndex = 1} if (n < 1) {slideIndex = slides.length} for (i = 0; i < slides.length; i++) { slides[i].style.display = \"none\"; } for (i = 0; i < dots.length; i++) { dots[i].className = dots[i].className.replace(\" active\", \"\"); } slides[slideIndex-1].style.display = \"block\"; dots[slideIndex-1].className += \" active\"; } Download Slides","title":"Slide Show"},{"location":"Deploying-OCP/Restricted-Network/#overview","text":"Deploying OpenShift in a restricted network (air gap), can be done with the Daffy Restricted Network scripts. This page will primarily focus on the steps to perform an air gap install. However, please note the Proxy Install approach can be a viable option and is a easier approach. Regardless of which one you choose, both install options will result in an OpenShift cluster with restricted network access. Also, note that the air gap install is more complicated and will require a JUMP BOX that has internet access to the Quay Repository. There are two basic options for deployment Option 1. Air gap install Option 2. Proxy install","title":"Overview"},{"location":"Deploying-OCP/Restricted-Network/#proxy-install","text":"OpenShift environments can deny direct access to the internet and instead have an HTTP or HTTPS proxy available. This can be done during an cluster installation. This would allow the cluster to be in a restricted network but gain access to public registries via proxy server. The bastion and cluster you build must also have direct access to the proxy server. This proxy server can then control what access is granted via whitelist. If you are doing proxy install, you only need one bastion and no jump box. This bastion must have access to the restricted network and the proxy server as well. #Airgap - Proxy Install# ###################################################################################### OCP_PROXY_HTTP_PROXY = OCP_PROXY_HTTPS_PROXY = OCP_PROXY_NO_PROXY = Info At this point for a proxy install, you can skip the following steps and move to the last step and install your cluster.","title":"Proxy Install"},{"location":"Deploying-OCP/Restricted-Network/#airgap-install","text":"OpenShift environments can deny direct access to the internet completely. With this method you must replicate or mirror the public registries local in your restricted network. To do this we will use the following terms: Jump box - machine that will have direct access to the internet to mirror (download) public registry software. Bastion - machine that will install the cluster and have direct access to the restricted network where the cluster will run.","title":"Airgap Install"},{"location":"Deploying-OCP/Restricted-Network/#requirements","text":"Jump Box - 4X32 300GB Disk (disk depending on what you plan to mirror) Default will need storage /data/export (100GB) and /mirror/registry (100GB) Bastion - 4X32 300GB Disk (disk depending on what you plan to mirror) Default will need storage /data/import (100GB) and /mirror/registry (100GB)","title":"Requirements"},{"location":"Deploying-OCP/Restricted-Network/#environment-file","text":"You should build your environment file based on the final install you want to build and should include those values in this file as well. The following values are required specifically for airgap install.(Repo Build and Airgap) Variable Name Info Install Type Required default LOCAL_REGISTRY_ENABLED Do you want this to be an air gap install Both Yes false LOCAL_REGISTRY_DNS_NAME This is your jump box DNS name(can Be IP value) Both Yes LOCAL_REGISTRY_IP This is your jump box IP address Both Yes LOCAL_AIRGAP_REGISTRY_DNS_NAME This is your restricted network bastion DNS name(can Be IP value) airgap Yes(if Airgap) LOCAL_AIRGAP_REGISTRY_IP This is your restricted network bastion box IP address airgap Yes(if Airgap) #Local Registry Info ############################### LOCAL_REGISTRY_ENABLED = \"true\" LOCAL_REGISTRY_DNS_NAME = \"<LOCAL IP>\" LOCAL_REGISTRY_IP = \"<LOCAL IP>\" #LOCAL_AIRGAP_REGISTRY_DNS_NAME=\"<Optional DNS Name>\" #LOCAL_AIRGAP_REGISTRY_IP=\"<Optional DNS Name>\"","title":"Environment File"},{"location":"Deploying-OCP/Restricted-Network/#repo-only","text":"If you want to just build a repo on your internet facing bastion, here is only settings you need to mirror. #Local Registry Info ############################### LOCAL_REGISTRY_ENABLED = \"true\" LOCAL_REGISTRY_IP = \"<LOCAL IP>\" #LOCAL_REGISTRY_DNS_NAME=\"<Optional DNS Name>\"","title":"Repo Only"},{"location":"Deploying-OCP/Restricted-Network/#overrides","text":"Current values that you can add to your own environment file to override if needed but not required: Click to expand! #Catalogs to mirror #################### OCP_CATALOG_MIRRORS = \"compliance-operator,container-security-operator,file-integrity-operator,local-storage-operator,ocs-operator\" #Directory Info #################### OCP_REGISTRY_ROOT = \"/mirror/registry\" OCP_AIRGAP_EXPORT = \"${DATA_DIR}/export/airgap\" OCP_AIRGAP_IMPORT = \"${DATA_DIR}/import/airgap\" OCP_AIRGAP_EXPORT_FREE_DISK_SIZE_NEEDED = \"100\" OCP_AIRGAP_IMPORT_FREE_DISK_SIZE_NEEDED = \"100\" #Cert Info #################### CA_CERT_OU = \"ca.${CLUSTER_NAME}.${BASE_DOMAIN}\" LOCAL_REGISTRY_CERTS_FOLDER = \"${DATA_DIR}/${PROJECT_NAME}/certs\" #Registry Info #################### LOCAL_REGISTRY_PORT = \"8443\" LOCAL_OCP_REPOSITORY_NAME = \"ocp4/openshift4\" LOCAL_OLM_MIRROR_REPOSITORY_NAME = \"olm-mirror\"","title":"Overrides"},{"location":"Deploying-OCP/Restricted-Network/#mirror-locally","text":"From your jump box run the following command: /data/daffy/ocp/registry/build.sh <ENVIRONMENT_NAME>","title":"Mirror locally"},{"location":"Deploying-OCP/Restricted-Network/#export-files","text":"After the first command runs, it will display all the files that it created and you will now need to move them to your bastion box in the restricted network. You can do this via portable USB disk, scp, etc. You just need to move these files any way you can to your restricted network bastion.","title":"Export Files"},{"location":"Deploying-OCP/Restricted-Network/#move-files","text":"In our example we will move via scp because our jump box has access to the bastion. This could be via firewall or it has dual NIC cards (Public Nic/Private Nic). ssh <BASTION-IP> mkdir -p /data/import/airgap scp /data/export/airgap/* <BASTION-IP>:/data/import/airgap","title":"Move Files"},{"location":"Deploying-OCP/Restricted-Network/#prepare-bastion","text":"Once all the files are on the bastion in the restricted network, you can run the script that was built from the previous step and copied over. This will untar all files, install all command line tools, and also install daffy locally. Info It does not mirror the registry or build the local registry, but gets the bastion ready for that next step. /data/import/airgap/airgap-prep.sh","title":"Prepare Bastion"},{"location":"Deploying-OCP/Restricted-Network/#build-local-mirror","text":"From your bastion box run the following command: /data/daffy/ocp/registry/build.sh <ENVIRONMENT_NAME>","title":"Build Local Mirror"},{"location":"Deploying-OCP/Restricted-Network/#install-cluster","text":"Now you would just follow the normal steps to build your OpenShift registry. /data/daffy/ocp/build.sh <ENVIRONMENT_NAME>","title":"Install Cluster"},{"location":"Deploying-OCP/Restricted-Network/#demo-video","text":"HTML Video embed","title":"Demo Video"},{"location":"Deploying-OCP/TechZone/","text":"document.title = \"Deploy OCP - TechZone\"; TechZone Install \u00b6 At this point, you have a bastion machine where you have installed the Daffy tool, created your core < ENVIRONMENT_NAME >-env.sh. Depending on if you use TechZone to build your cluster, you may skip Daffy Step 1, which builds your cluster. You will not use the bastion to build your cluster, but follow the steps to have TechZone build your cluster. Once that is done, you would move on with the Daffy process and install your Cloud Paks from your new bastion. Warning For IBM and Business Partners use only Tech Zone AWS/Azure \u00b6 ( prebuilt cluster): https://techzone.ibm.com/collection/third-party-cloud-openshift-clusters Does NOT include bastion with request With this option, you will skip Daffy step 1 (build cluster) as TechZone will build for you (this would fail as you do not have access to create cluster with a TechZone setup) You still need to have a bastion and core values in your < ENVIRONMENT_NAME >-env.sh BASE_DOMAIN is not needed in your environment file OCP_INSTALL_TYPE is needed based on provider you pick(aws-ip or azure-ipi). All other provider info is not needed in your environment file Tech Zone VSphere Gym \u00b6 ( Daffy builds cluster): https://techzone.ibm.com/collection/ocp-gymnasium Includes bastion with request Once you create the request and the VSPhere environment has been provisioned, it will create your own bastion and give you the instructions on how to use Daffy in that environment with the prebuilt /data/daffy/env/vmware-ipi-env.sh To run daffy, you must be full root, do not just sudo the script (sudo /data/daffy/ocp/build.sh vmware-ipi). Run next command first: sudo su - TechZone VMWare/Roks \u00b6 ( prebuilt cluster): https://techzone.ibm.com/collection/5fb3200cec8dd00017c57f20 Does NOT include bastion with request No need for VPN, public direct access to cluster Comes with IBM Storage for ROKS and ODF with VSphere With this option, you will skip Daffy step 1 (build cluster) as TechZone will build for you (this will fail as you do not have access to create cluster with a TechZone setup) Once you create the request, you would follow the same steps as ROKS with Daffy You still need to have a bastion and core values in your < ENVIRONMENT_NAME >-env.sh Extra settings to change in your environment file: ROKS_PROVIDER= techzone #(ROKS only) DAFFY_DEPLOYMENT_TYPE= TechZone BASE_DOMAIN is not needed in your environment file OCP_INSTALL_TYPE is needed based on provider you pick(roks-msp or vsphere-ipi). Installing Cloud Paks","title":"TechZone"},{"location":"Deploying-OCP/TechZone/#techzone-install","text":"At this point, you have a bastion machine where you have installed the Daffy tool, created your core < ENVIRONMENT_NAME >-env.sh. Depending on if you use TechZone to build your cluster, you may skip Daffy Step 1, which builds your cluster. You will not use the bastion to build your cluster, but follow the steps to have TechZone build your cluster. Once that is done, you would move on with the Daffy process and install your Cloud Paks from your new bastion. Warning For IBM and Business Partners use only","title":"TechZone Install"},{"location":"Deploying-OCP/TechZone/#tech-zone-awsazure","text":"( prebuilt cluster): https://techzone.ibm.com/collection/third-party-cloud-openshift-clusters Does NOT include bastion with request With this option, you will skip Daffy step 1 (build cluster) as TechZone will build for you (this would fail as you do not have access to create cluster with a TechZone setup) You still need to have a bastion and core values in your < ENVIRONMENT_NAME >-env.sh BASE_DOMAIN is not needed in your environment file OCP_INSTALL_TYPE is needed based on provider you pick(aws-ip or azure-ipi). All other provider info is not needed in your environment file","title":"Tech Zone AWS/Azure"},{"location":"Deploying-OCP/TechZone/#tech-zone-vsphere-gym","text":"( Daffy builds cluster): https://techzone.ibm.com/collection/ocp-gymnasium Includes bastion with request Once you create the request and the VSPhere environment has been provisioned, it will create your own bastion and give you the instructions on how to use Daffy in that environment with the prebuilt /data/daffy/env/vmware-ipi-env.sh To run daffy, you must be full root, do not just sudo the script (sudo /data/daffy/ocp/build.sh vmware-ipi). Run next command first: sudo su -","title":"Tech Zone VSphere Gym"},{"location":"Deploying-OCP/TechZone/#techzone-vmwareroks","text":"( prebuilt cluster): https://techzone.ibm.com/collection/5fb3200cec8dd00017c57f20 Does NOT include bastion with request No need for VPN, public direct access to cluster Comes with IBM Storage for ROKS and ODF with VSphere With this option, you will skip Daffy step 1 (build cluster) as TechZone will build for you (this will fail as you do not have access to create cluster with a TechZone setup) Once you create the request, you would follow the same steps as ROKS with Daffy You still need to have a bastion and core values in your < ENVIRONMENT_NAME >-env.sh Extra settings to change in your environment file: ROKS_PROVIDER= techzone #(ROKS only) DAFFY_DEPLOYMENT_TYPE= TechZone BASE_DOMAIN is not needed in your environment file OCP_INSTALL_TYPE is needed based on provider you pick(roks-msp or vsphere-ipi). Installing Cloud Paks","title":"TechZone VMWare/Roks"},{"location":"Deploying-OCP/TechZoneTiles/","text":"document.title = \"Deploy OCP - TechZone Tiles\"; TechZone Tile Info - Beta \u00b6 The Purpose of the following TechZone Tiles are to install the full OpenShift/Cloud Pak stack for you. This would enable you to start using your Cloud Pak of choice, without deep skills in OpenShift or Cloud Pak install process. From a Reservetaion to Cloud Pak use, in a few hours of runtime, but 5 min of your time. Sit back, let Daffy/PakInstaller Build the full stack for you. This collection of tiles will build a public facing cluster. No VPN is require or provided More then one user is allowed to access your Cluster and Cloud Pak. The only limitation is your cloud pak and the size that was built. All Access Points via trusted SSL Certs. No Browser Popup You can access the Tech Zone Environment Tiles here: Cloud Pak for Business Automation Cloud Pak for Integration Over all Process \u00b6 Overall Process can take 4 - 8 hours. Of that, it will take 2 to 3 hours to install OpenShift, Cloud Pak base and install/start the services for the cloud pak. You will get email at each major step. Then you will have to wait the 1 -5 hours until your serivces are fully running. Status Email Messages \u00b6 1) After Cluster is installed - OpenShift is up and running, you can logon to the OpenShift Cluster console Email Sample 2) After the Cloud Pak Base is installed - Cloud Pak Name Spaces is create and All Cloud Pak Operators are installed Email Sample 3) After the Cloud Pak Services have be installed - This is that the starting process, servics are not ready yet. This is final email as the Pak Installer automation process is over. Now the cloud pak operators take over and can take another 1 - 5 hours Email Sample TechZone Emails TechZone will also send you emails, at the begging of the reserveration and and the end once the PakInstaller automation is done. You will get a total of 5 emails for this process. Pak Installer Portal \u00b6 To make the process simple, the end result for your TechZone Reservation will be a single link to the Pak Installer Portal. This portal is your one stop for status and connection info for you cluster and the cloud pak. The Portal requires userid/password to connect to as this cluster is a public facing cluster. These pages update every few minutes with any updated information. This portal is where you would go to monitor the status of your servies and to find out when they are ready. You will not get email when they are ready as this can take from 1 - 5 hours to finish. Please use the \"Cloud Pak Status\" tab to monitor the status. Tabs \u00b6 The Pak Installer portal has 5 tabs at the top of the page. Each tab is designed to give you more info for that topic. Tabs Preview Instructions \u00b6 This page to explain the tabs within the Portal. Points to the Daffy Public documentation site from within your local cluster Portal page Bastion \u00b6 Connection info to the bastion(RHEL server). It will gave you the host name, port, userID and password you can use if you want to connect to the bastion server for this new environment. Info This info is not required to be used to run/access your Cloud Pak, but here if you needed it. OpenShift Console \u00b6 Connection info to your new OpenShift Cluster. You can see both ids you can use to connect to the cluster via command line or Web URl. It also shows overall cluster info like version, bastion OS, daffy version, etc. Info This info is not required to be used to run/access your Cloud Pak, but here if you needed it. Cloud Pak Status \u00b6 This page is where can see overall status of the cloud pak and all the sevcies you requeted. The Cloud Pak Status page will auto refresh every few minutes and where you can watch as your services come online. As stated before, this can take 1 -5 hours to get to \"Ready\" state for all services componets. Once services are ready, it will then update the console tab with your console connection info. Cloud Pak Console \u00b6 This page is where can see the connection info for your cloud Pak. This is the page where you will get your URL's, usernames and passwords once the services are up and running. The page will refresh every few minutes. You will not see any values until the CP4B operators have completed there task. Once operators are done, it will populate this page with all of your connection details.","title":"TechZoneTiles"},{"location":"Deploying-OCP/TechZoneTiles/#techzone-tile-info-beta","text":"The Purpose of the following TechZone Tiles are to install the full OpenShift/Cloud Pak stack for you. This would enable you to start using your Cloud Pak of choice, without deep skills in OpenShift or Cloud Pak install process. From a Reservetaion to Cloud Pak use, in a few hours of runtime, but 5 min of your time. Sit back, let Daffy/PakInstaller Build the full stack for you. This collection of tiles will build a public facing cluster. No VPN is require or provided More then one user is allowed to access your Cluster and Cloud Pak. The only limitation is your cloud pak and the size that was built. All Access Points via trusted SSL Certs. No Browser Popup You can access the Tech Zone Environment Tiles here: Cloud Pak for Business Automation Cloud Pak for Integration","title":"TechZone Tile Info - Beta"},{"location":"Deploying-OCP/TechZoneTiles/#over-all-process","text":"Overall Process can take 4 - 8 hours. Of that, it will take 2 to 3 hours to install OpenShift, Cloud Pak base and install/start the services for the cloud pak. You will get email at each major step. Then you will have to wait the 1 -5 hours until your serivces are fully running.","title":"Over all Process"},{"location":"Deploying-OCP/TechZoneTiles/#status-email-messages","text":"1) After Cluster is installed - OpenShift is up and running, you can logon to the OpenShift Cluster console Email Sample 2) After the Cloud Pak Base is installed - Cloud Pak Name Spaces is create and All Cloud Pak Operators are installed Email Sample 3) After the Cloud Pak Services have be installed - This is that the starting process, servics are not ready yet. This is final email as the Pak Installer automation process is over. Now the cloud pak operators take over and can take another 1 - 5 hours Email Sample TechZone Emails TechZone will also send you emails, at the begging of the reserveration and and the end once the PakInstaller automation is done. You will get a total of 5 emails for this process.","title":"Status Email Messages"},{"location":"Deploying-OCP/TechZoneTiles/#pak-installer-portal","text":"To make the process simple, the end result for your TechZone Reservation will be a single link to the Pak Installer Portal. This portal is your one stop for status and connection info for you cluster and the cloud pak. The Portal requires userid/password to connect to as this cluster is a public facing cluster. These pages update every few minutes with any updated information. This portal is where you would go to monitor the status of your servies and to find out when they are ready. You will not get email when they are ready as this can take from 1 - 5 hours to finish. Please use the \"Cloud Pak Status\" tab to monitor the status.","title":"Pak Installer Portal"},{"location":"Deploying-OCP/TechZoneTiles/#tabs","text":"The Pak Installer portal has 5 tabs at the top of the page. Each tab is designed to give you more info for that topic. Tabs Preview","title":"Tabs"},{"location":"Deploying-OCP/TechZoneTiles/#instructions","text":"This page to explain the tabs within the Portal. Points to the Daffy Public documentation site from within your local cluster Portal page","title":"Instructions"},{"location":"Deploying-OCP/TechZoneTiles/#bastion","text":"Connection info to the bastion(RHEL server). It will gave you the host name, port, userID and password you can use if you want to connect to the bastion server for this new environment. Info This info is not required to be used to run/access your Cloud Pak, but here if you needed it.","title":"Bastion"},{"location":"Deploying-OCP/TechZoneTiles/#openshift-console","text":"Connection info to your new OpenShift Cluster. You can see both ids you can use to connect to the cluster via command line or Web URl. It also shows overall cluster info like version, bastion OS, daffy version, etc. Info This info is not required to be used to run/access your Cloud Pak, but here if you needed it.","title":"OpenShift Console"},{"location":"Deploying-OCP/TechZoneTiles/#cloud-pak-status","text":"This page is where can see overall status of the cloud pak and all the sevcies you requeted. The Cloud Pak Status page will auto refresh every few minutes and where you can watch as your services come online. As stated before, this can take 1 -5 hours to get to \"Ready\" state for all services componets. Once services are ready, it will then update the console tab with your console connection info.","title":"Cloud Pak Status"},{"location":"Deploying-OCP/TechZoneTiles/#cloud-pak-console","text":"This page is where can see the connection info for your cloud Pak. This is the page where you will get your URL's, usernames and passwords once the services are up and running. The page will refresh every few minutes. You will not see any values until the CP4B operators have completed there task. Once operators are done, it will populate this page with all of your connection details.","title":"Cloud Pak Console"},{"location":"Deploying-OCP/TechZoneTilesBeta/","text":"document.title = \"Deploy OCP - TechZone Tiles - Beta\"; TechZone Tile Info - Beta \u00b6 The Purpose of the following TechZone Tiles are to install the full OpenShift/Cloud Pak stack for you. This would enable you to start using your Cloud Pak of choice, without deep skills in OpenShift or Cloud Pak install process. From a Reservetaion to Cloud Pak use, in a few hours of runtime, but 5 min of your time. Sit back, let Daffy/PakInstaller Build the full stack for you. This collection of tiles will build a public facing cluster. No VPN is require or provided More then one user is allowed to access your Cluster and Cloud Pak. The only limitation is your cloud pak and the size that was built. All Access Points via trusted SSL Certs. No Browser Popup You can access the Tech Zone Environment Tiles here: Cloud Pak for Business Automation Cloud Pak for Integration Over all Process \u00b6 Overall Process can take 4 - 8 hours. Of that, it will take 2 to 3 hours to install OpenShift, Cloud Pak base and install/start the services for the cloud pak. You will get email at each major step. Then you will have to wait the 1 -5 hours until your serivces are fully running. Status Email Messages \u00b6 1) After Cluster is installed - OpenShift is up and running, you can logon to the OpenShift Cluster console Email Sample 2) After the Cloud Pak Base is installed - Cloud Pak Name Spaces is create and All Cloud Pak Operators are installed Email Sample 3) After the Cloud Pak Services have be installed - This is that the starting process, servics are not ready yet. This is final email as the Pak Installer automation process is over. Now the cloud pak operators take over and can take another 1 - 5 hours Email Sample TechZone Emails TechZone will also send you emails, at the begging of the reserveration and and the end once the PakInstaller automation is done. You will get a total of 5 emails for this process. Pak Installer Portal \u00b6 To make the process simple, the end result for your TechZone Reservation will be a single link to the Pak Installer Portal. This portal is your one stop for status and connection info for you cluster and the cloud pak. The Portal requires userid/password to connect to as this cluster is a public facing cluster. These pages update every few minutes with any updated information. This portal is where you would go to monitor the status of your servies and to find out when they are ready. You will not get email when they are ready as this can take from 1 - 5 hours to finish. Please use the \"Cloud Pak Status\" tab to monitor the status. Tabs \u00b6 The Pak Installer portal has 5 tabs at the top of the page. Each tab is designed to give you more info for that topic. Tabs Preview Instructions \u00b6 This page to explain the tabs within the Portal. Points to the Daffy Public documentation site from within your local cluster Portal page Bastion \u00b6 Connection info to the bastion(RHEL server). It will gave you the host name, port, userID and password you can use if you want to connect to the bastion server for this new environment. Info This info is not required to be used to run/access your Cloud Pak, but here if you needed it. OpenShift Console \u00b6 Connection info to your new OpenShift Cluster. You can see both ids you can use to connect to the cluster via command line or Web URl. It also shows overall cluster info like version, bastion OS, daffy version, etc. Info This info is not required to be used to run/access your Cloud Pak, but here if you needed it. Cloud Pak Status \u00b6 This page is where can see overall status of the cloud pak and all the sevcies you requeted. The Cloud Pak Status page will auto refresh every few minutes and where you can watch as your services come online. As stated before, this can take 1 -5 hours to get to \"Ready\" state for all services componets. Once services are ready, it will then update the console tab with your console connection info. Cloud Pak Console \u00b6 This page is where can see the connection info for your cloud Pak. This is the page where you will get your URL's, usernames and passwords once the services are up and running. The page will refresh every few minutes. You will not see any values until the CP4B operators have completed there task. Once operators are done, it will populate this page with all of your connection details.","title":"TechZoneTilesBeta"},{"location":"Deploying-OCP/TechZoneTilesBeta/#techzone-tile-info-beta","text":"The Purpose of the following TechZone Tiles are to install the full OpenShift/Cloud Pak stack for you. This would enable you to start using your Cloud Pak of choice, without deep skills in OpenShift or Cloud Pak install process. From a Reservetaion to Cloud Pak use, in a few hours of runtime, but 5 min of your time. Sit back, let Daffy/PakInstaller Build the full stack for you. This collection of tiles will build a public facing cluster. No VPN is require or provided More then one user is allowed to access your Cluster and Cloud Pak. The only limitation is your cloud pak and the size that was built. All Access Points via trusted SSL Certs. No Browser Popup You can access the Tech Zone Environment Tiles here: Cloud Pak for Business Automation Cloud Pak for Integration","title":"TechZone Tile Info - Beta"},{"location":"Deploying-OCP/TechZoneTilesBeta/#over-all-process","text":"Overall Process can take 4 - 8 hours. Of that, it will take 2 to 3 hours to install OpenShift, Cloud Pak base and install/start the services for the cloud pak. You will get email at each major step. Then you will have to wait the 1 -5 hours until your serivces are fully running.","title":"Over all Process"},{"location":"Deploying-OCP/TechZoneTilesBeta/#status-email-messages","text":"1) After Cluster is installed - OpenShift is up and running, you can logon to the OpenShift Cluster console Email Sample 2) After the Cloud Pak Base is installed - Cloud Pak Name Spaces is create and All Cloud Pak Operators are installed Email Sample 3) After the Cloud Pak Services have be installed - This is that the starting process, servics are not ready yet. This is final email as the Pak Installer automation process is over. Now the cloud pak operators take over and can take another 1 - 5 hours Email Sample TechZone Emails TechZone will also send you emails, at the begging of the reserveration and and the end once the PakInstaller automation is done. You will get a total of 5 emails for this process.","title":"Status Email Messages"},{"location":"Deploying-OCP/TechZoneTilesBeta/#pak-installer-portal","text":"To make the process simple, the end result for your TechZone Reservation will be a single link to the Pak Installer Portal. This portal is your one stop for status and connection info for you cluster and the cloud pak. The Portal requires userid/password to connect to as this cluster is a public facing cluster. These pages update every few minutes with any updated information. This portal is where you would go to monitor the status of your servies and to find out when they are ready. You will not get email when they are ready as this can take from 1 - 5 hours to finish. Please use the \"Cloud Pak Status\" tab to monitor the status.","title":"Pak Installer Portal"},{"location":"Deploying-OCP/TechZoneTilesBeta/#tabs","text":"The Pak Installer portal has 5 tabs at the top of the page. Each tab is designed to give you more info for that topic. Tabs Preview","title":"Tabs"},{"location":"Deploying-OCP/TechZoneTilesBeta/#instructions","text":"This page to explain the tabs within the Portal. Points to the Daffy Public documentation site from within your local cluster Portal page","title":"Instructions"},{"location":"Deploying-OCP/TechZoneTilesBeta/#bastion","text":"Connection info to the bastion(RHEL server). It will gave you the host name, port, userID and password you can use if you want to connect to the bastion server for this new environment. Info This info is not required to be used to run/access your Cloud Pak, but here if you needed it.","title":"Bastion"},{"location":"Deploying-OCP/TechZoneTilesBeta/#openshift-console","text":"Connection info to your new OpenShift Cluster. You can see both ids you can use to connect to the cluster via command line or Web URl. It also shows overall cluster info like version, bastion OS, daffy version, etc. Info This info is not required to be used to run/access your Cloud Pak, but here if you needed it.","title":"OpenShift Console"},{"location":"Deploying-OCP/TechZoneTilesBeta/#cloud-pak-status","text":"This page is where can see overall status of the cloud pak and all the sevcies you requeted. The Cloud Pak Status page will auto refresh every few minutes and where you can watch as your services come online. As stated before, this can take 1 -5 hours to get to \"Ready\" state for all services componets. Once services are ready, it will then update the console tab with your console connection info.","title":"Cloud Pak Status"},{"location":"Deploying-OCP/TechZoneTilesBeta/#cloud-pak-console","text":"This page is where can see the connection info for your cloud Pak. This is the page where you will get your URL's, usernames and passwords once the services are up and running. The page will refresh every few minutes. You will not see any values until the CP4B operators have completed there task. Once operators are done, it will populate this page with all of your connection details.","title":"Cloud Pak Console"},{"location":"Deploying-OCP/VSphere/","text":"document.title = \"Deploy OCP - VSphere\"; Platform Requirements \u00b6 To install Daffy on VSphere , there are some platform info and requirements that need to be gathered or met. We have a simple doc that you should refer to that list all providers and what would be needed. There is a number of permissions you MUST have as a user on VCenter for deployment of OpenShift on VSphere. Please refer to the requirements document for the specific requirements for IPI or UPI installs on VSphere: https://ibm.box.com/v/DaffyProviderRequirements Finding Provider Details \u00b6 To install Daffy on VSphere , the hardest part can be finding the provider details in the VCenter Console. Some of the variables are easily understood, but a few can be a bit tricky to find. Variable Name Info Install Type Required VSPHERE_DATASTORE This is the name of the VSphere Datastore IPI/UPI Yes VSPHERE_CLUSTER The VSphere cluster is NOT the same as your OpenShift Cluster name. This is variable is referring to the VSphere Cluster name. IPI/UPI Yes VSPHERE_NETWORK1 This is the VSphere VLAN name IPI/UPI Yes VSPHERE_DATACENTER This is the name of the VSphere Datacenter IPI/UPI Yes VSPHERE_FOLDER This is the location of where you will store the NEW VM's IPI/UPI Yes VSPHERE_API_VIP This is an UNUSED IP address that will be utilized by the OpenShift IPI installer to provision the API Virtual IP Address IPI Yes VSPHERE_INGRESS_VIP This is an UNUSED IP address that will be utilized by the OpenShift IPI installer to provision the Ingress Virtual IP Address IPI Yes VSphere UPI Only VSPHERE_ISO_DATASTORE This is the name of the datastore where the the coreos iso is located UPI Yes VSPHERE_ISO_IMAGE_BASE This is the directory within the datastore where the iso image is located UPI Yes BASTION_HOST This is the name of the bastion host, IP or DNS value UPI No BASTION_USER This is non admin id on the bastion host that has authorzation to logon via SSH to bastion UPI Yes OCP_INSTALL_GATEWAY This is gatway to assign new new VMs UPI Yes OCP_FORWARD_DNS This is IP for DNS Server to forward request to UPI Yes OCP_INSTALL_DNS This is IP for the DNS Server that will have OpenShift Node entries UPI Yes OCP_INSTALLBOOTSTRAP_IP This is Bootstrap IP you want to assign UPI Yes OCP_INSTALL_MASTER1_IP This is Master 1 IP you want to assign UPI Yes OCP_INSTALL_MASTER2_IP This is Master 2 IP you want to assign UPI Yes OCP_INSTALL_MASTER3_IP This is Master 3 IP you want to assign UPI Yes OCP_INSTALL_WORKER1_IP This is Worker 1 IP you want to assign UPI Yes OCP_INSTALL_WORKER2_IP This is Worker 2 IP you want to assign UPI Yes OCP_INSTALL_WORKER3_IP This is Worker 3 IP you want to assign UPI Yes OCP_INSTALL_WORKER4_IP This is Worker 4 IP you want to assign UPI Yes OCP_INSTALL_WORKER5_IP This is Worker 5 IP you want to assign UPI Yes OCP_INSTALL_WORKER6_IP This is Worker 6 IP you want to assign UPI Yes Setting up DNS \u00b6 HTML Video embed Environment File \u00b6 Below are the VSphere IPI specific environment variables that must be defined in the /data/daffy/env/< ENVIRONMENT_NAME >-env.sh file. Note You can look in the samples directory on your bastion for example of VSphere install : /data/daffy/env/samples/ vsphere-ipi-env.sh Valid Options: #VSphere Platform Info ######################## VSPHERE_USERNAME = \"userid\" VSPHERE_HOSTNAME = \"vsphere-host-name\" VSPHERE_DATASTORE = \"datastore\" VSPHERE_CLUSTER = \"cluster-name\" . VSPHERE_NETWORK1 = \"vlan-name\" VSPHERE_DATACENTER = \"vsphere-datacenter\" VSPHERE_FOLDER = \"/${VSPHERE_DATACENTER}/vm/${CLUSTER_NAME}\" #IPI Only ######################## OCP_INSTALL_TYPE = \"vsphere-ipi\" VSPHERE_API_VIP = \"xx.xxx.xxx.xxx\" VSPHERE_INGRESS_VIP = \"xx.xxx.xxx\" #UPI Only ######################## #OCP_INSTALL_TYPE=\"vsphere-upi\" #VSPHERE_ISO_DATASTORE=\"iso-datastore\" #VSPHERE_ISO_IMAGE_BASE=\"datastore-directory\" #BASTION_HOST=\"xx.xxx.xxx\" #BASTION_USER=\"bastion\" #OCP_INSTALL_GATEWAY=\"xx.xxx.xxx\" #OCP_FORWARD_DNS=\"xx.xxx.xxx\" #OCP_INSTALL_DNS=${BASTION_HOST} #OCP_NODE_SUBNET_MASK=\"24\" #OCP_INSTALLBOOTSTRAP_IP=\"xx.xxx.xxx\" #OCP_INSTALL_MASTER1_IP=\"xx.xxx.xxx\" #OCP_INSTALL_MASTER2_IP=\"xx.xxx.xxx\" #OCP_INSTALL_MASTER3_IP=\"xx.xxx.xxx\" #OCP_INSTALL_WORKER1_IP=\"xx.xxx.xxx\" #OCP_INSTALL_WORKER2_IP=\"xx.xxx.xxx\" #OCP_INSTALL_WORKER3_IP=\"xx.xxx.xxx\" #OCP_INSTALL_WORKER4_IP=\"xx.xxx.xxx\" #OCP_INSTALL_WORKER5_IP=\"xx.xxx.xxx\" #OCP_INSTALL_WORKER6_IP=\"xx.xxx.xxx\" #Storage Option for OpenShift ######################## #OCP_CREATE_OPENSHIFT_CONTAINER_STORAGE=true Optional: OCP_CREATE_OPENSHIFT_CONTAINER_STORAGE =true If you plan to install a cloud pak and/or need storage, you need to set the flag to setup OCS Storage. Execution \u00b6 To deploy your cluster, run the build.sh script from the /data/daffy/ocp directory: /data/daffy/ocp/build.sh <ENVIRONMENT_NAME> Once your cluster is fully deployed, you can access the help menu which has a number of options. Note <ENVIRONMENT_NAME> is the first part of your name that you used for the <ENVIRONMENT_NAME>-env.sh file Deploying an OpenShift cluster on VSphere using the Daffy scripts (using VSPhere-IPI install type): /data/daffy/ocp/build.sh <ENVIRONMENT_NAME> --help Installing Cloud Paks","title":"VSphere"},{"location":"Deploying-OCP/VSphere/#platform-requirements","text":"To install Daffy on VSphere , there are some platform info and requirements that need to be gathered or met. We have a simple doc that you should refer to that list all providers and what would be needed. There is a number of permissions you MUST have as a user on VCenter for deployment of OpenShift on VSphere. Please refer to the requirements document for the specific requirements for IPI or UPI installs on VSphere: https://ibm.box.com/v/DaffyProviderRequirements","title":"Platform Requirements"},{"location":"Deploying-OCP/VSphere/#finding-provider-details","text":"To install Daffy on VSphere , the hardest part can be finding the provider details in the VCenter Console. Some of the variables are easily understood, but a few can be a bit tricky to find. Variable Name Info Install Type Required VSPHERE_DATASTORE This is the name of the VSphere Datastore IPI/UPI Yes VSPHERE_CLUSTER The VSphere cluster is NOT the same as your OpenShift Cluster name. This is variable is referring to the VSphere Cluster name. IPI/UPI Yes VSPHERE_NETWORK1 This is the VSphere VLAN name IPI/UPI Yes VSPHERE_DATACENTER This is the name of the VSphere Datacenter IPI/UPI Yes VSPHERE_FOLDER This is the location of where you will store the NEW VM's IPI/UPI Yes VSPHERE_API_VIP This is an UNUSED IP address that will be utilized by the OpenShift IPI installer to provision the API Virtual IP Address IPI Yes VSPHERE_INGRESS_VIP This is an UNUSED IP address that will be utilized by the OpenShift IPI installer to provision the Ingress Virtual IP Address IPI Yes VSphere UPI Only VSPHERE_ISO_DATASTORE This is the name of the datastore where the the coreos iso is located UPI Yes VSPHERE_ISO_IMAGE_BASE This is the directory within the datastore where the iso image is located UPI Yes BASTION_HOST This is the name of the bastion host, IP or DNS value UPI No BASTION_USER This is non admin id on the bastion host that has authorzation to logon via SSH to bastion UPI Yes OCP_INSTALL_GATEWAY This is gatway to assign new new VMs UPI Yes OCP_FORWARD_DNS This is IP for DNS Server to forward request to UPI Yes OCP_INSTALL_DNS This is IP for the DNS Server that will have OpenShift Node entries UPI Yes OCP_INSTALLBOOTSTRAP_IP This is Bootstrap IP you want to assign UPI Yes OCP_INSTALL_MASTER1_IP This is Master 1 IP you want to assign UPI Yes OCP_INSTALL_MASTER2_IP This is Master 2 IP you want to assign UPI Yes OCP_INSTALL_MASTER3_IP This is Master 3 IP you want to assign UPI Yes OCP_INSTALL_WORKER1_IP This is Worker 1 IP you want to assign UPI Yes OCP_INSTALL_WORKER2_IP This is Worker 2 IP you want to assign UPI Yes OCP_INSTALL_WORKER3_IP This is Worker 3 IP you want to assign UPI Yes OCP_INSTALL_WORKER4_IP This is Worker 4 IP you want to assign UPI Yes OCP_INSTALL_WORKER5_IP This is Worker 5 IP you want to assign UPI Yes OCP_INSTALL_WORKER6_IP This is Worker 6 IP you want to assign UPI Yes","title":"Finding Provider Details"},{"location":"Deploying-OCP/VSphere/#setting-up-dns","text":"HTML Video embed","title":"Setting up DNS"},{"location":"Deploying-OCP/VSphere/#environment-file","text":"Below are the VSphere IPI specific environment variables that must be defined in the /data/daffy/env/< ENVIRONMENT_NAME >-env.sh file. Note You can look in the samples directory on your bastion for example of VSphere install : /data/daffy/env/samples/ vsphere-ipi-env.sh Valid Options: #VSphere Platform Info ######################## VSPHERE_USERNAME = \"userid\" VSPHERE_HOSTNAME = \"vsphere-host-name\" VSPHERE_DATASTORE = \"datastore\" VSPHERE_CLUSTER = \"cluster-name\" . VSPHERE_NETWORK1 = \"vlan-name\" VSPHERE_DATACENTER = \"vsphere-datacenter\" VSPHERE_FOLDER = \"/${VSPHERE_DATACENTER}/vm/${CLUSTER_NAME}\" #IPI Only ######################## OCP_INSTALL_TYPE = \"vsphere-ipi\" VSPHERE_API_VIP = \"xx.xxx.xxx.xxx\" VSPHERE_INGRESS_VIP = \"xx.xxx.xxx\" #UPI Only ######################## #OCP_INSTALL_TYPE=\"vsphere-upi\" #VSPHERE_ISO_DATASTORE=\"iso-datastore\" #VSPHERE_ISO_IMAGE_BASE=\"datastore-directory\" #BASTION_HOST=\"xx.xxx.xxx\" #BASTION_USER=\"bastion\" #OCP_INSTALL_GATEWAY=\"xx.xxx.xxx\" #OCP_FORWARD_DNS=\"xx.xxx.xxx\" #OCP_INSTALL_DNS=${BASTION_HOST} #OCP_NODE_SUBNET_MASK=\"24\" #OCP_INSTALLBOOTSTRAP_IP=\"xx.xxx.xxx\" #OCP_INSTALL_MASTER1_IP=\"xx.xxx.xxx\" #OCP_INSTALL_MASTER2_IP=\"xx.xxx.xxx\" #OCP_INSTALL_MASTER3_IP=\"xx.xxx.xxx\" #OCP_INSTALL_WORKER1_IP=\"xx.xxx.xxx\" #OCP_INSTALL_WORKER2_IP=\"xx.xxx.xxx\" #OCP_INSTALL_WORKER3_IP=\"xx.xxx.xxx\" #OCP_INSTALL_WORKER4_IP=\"xx.xxx.xxx\" #OCP_INSTALL_WORKER5_IP=\"xx.xxx.xxx\" #OCP_INSTALL_WORKER6_IP=\"xx.xxx.xxx\" #Storage Option for OpenShift ######################## #OCP_CREATE_OPENSHIFT_CONTAINER_STORAGE=true Optional: OCP_CREATE_OPENSHIFT_CONTAINER_STORAGE =true If you plan to install a cloud pak and/or need storage, you need to set the flag to setup OCS Storage.","title":"Environment File"},{"location":"Deploying-OCP/VSphere/#execution","text":"To deploy your cluster, run the build.sh script from the /data/daffy/ocp directory: /data/daffy/ocp/build.sh <ENVIRONMENT_NAME> Once your cluster is fully deployed, you can access the help menu which has a number of options. Note <ENVIRONMENT_NAME> is the first part of your name that you used for the <ENVIRONMENT_NAME>-env.sh file Deploying an OpenShift cluster on VSphere using the Daffy scripts (using VSPhere-IPI install type): /data/daffy/ocp/build.sh <ENVIRONMENT_NAME> --help Installing Cloud Paks","title":"Execution"},{"location":"Deploying-OCP/extra_features/","text":"Build Flags \u00b6 To get a list of the build flags, run the following command and replace the ENVIRONMENT_NAME with the name you gave in your environment file. /data/daffy/ocp/build.sh ENVIRONMENT_NAME --help","title":"Extra features"},{"location":"Deploying-OCP/extra_features/#build-flags","text":"To get a list of the build flags, run the following command and replace the ENVIRONMENT_NAME with the name you gave in your environment file. /data/daffy/ocp/build.sh ENVIRONMENT_NAME --help","title":"Build Flags"},{"location":"Deploying-OCP/kvm-update/","text":"document.title = \"Deploy OCP - KVM\"; KVM Install \u00b6 At this point, you have a bastion machine where you have installed the Daffy tool, created your core <ENVIRONMENT_NAME>-env.sh, and can execute the install of OCP on K ernel-based V irtual M achine (KVM). A KVM deployment differs from other OCP deployments in that a KVM is both a bare metal server and a bastion machine. This bastion is where you install the Daffy tool. Platform Requirements \u00b6 To use Daffy on KVM, there are some platform info and requirements that need to be gathered or met. We have a simple doc that you should refer to that list all providers and what would be needed. For KVM, it breaks down to the following three basic items: Hardware - enough to run the OCP Cluster based on T-Shirt Sizing OS Version - Ubuntu 20.0.4 ( only supported by Daffy ) Permission - full root authority. For IBM Cloud's KVM bastion, you already have full root authority. For detailed list of the above, you can find in the Daffy Provider Requirements. Please review before proceeding: https://ibm.box.com/v/DaffyProviderRequirements Public DNS Setup: You will need to create DNS entries and domain. For the OpenShift install, you need the following: Registered DNS Name - myexample.com DNS Entries - myexample-com api.${CLUSTER_NAME}. myexample.com ---> ${YOUR.BASTION.IP} api-int.${CLUSTER_NAME}. myexample.com ---> ${YOUR.BASTION.IP} *.apps.${CLUSTER_NAME}.myexample.com ---> ${YOUR.BASTION.IP} Setting up DNS for KVM Deployment with IBM Cloud Internet Services: Register domain name. Detailed instructions for domain registration can be found on the IBM Cloud Domain Name Registration page . Create an IBM Cloud Internet Services (CIS) resource. You can find IBM CIS by searching for \"Internet Services\" in the IBM Cloud catalog. IBM CIS will prompt you for your domain name during resource creation. Add the 3 DNS entries previously mentioned. Detailed instructions can be found on the Getting Started on IBM CIS page . Find your IBM CIS service in your resource list and click on it to access the console On the left menu, click on \"Reliability\" On the Reliability page menu, below your domain name and next to Global load balancers, click on \"DNS.\" At the bottom of the page, you will find a list of DNS records. Click on the \"Add button.\" You can now add the 3 required DNS entries for OCP install Environment File \u00b6 Deploying the OpenShift on K ernel-based V irtual M achine only requires two entries to your existing core environment file (< ENVIRONMENT_NAME >-env.sh). Note You can copy the sample file to build your new environment file: cp /data/daffy/env/samples/kvm-upi-env.sh /data/daffy/env/<ENVIRONMENT_NAME>-env.sh Valid Options: OCP_INSTALL_TYPE= kvm-upi BASTION_HOST =\"xxx.xxx.xxx.xxx\" If your host does not have its own public IP address, you need to specify the bastion IP address that the OCP cluster would use to reach your bastion host, i.e. its local IP address you used to connect to the bastion. If you plan to install a cloud pak and/or need storage, you need to set the flag to setup OCS Storage: OCP_CREATE_OPENSHIFT_CONTAINER_STORAGE=true Execution \u00b6 To deploy your OCP cluster to Kernel-based Virtual Machine, run the build.sh script from the /data/daffy/ocp directory: /data/daffy/ocp/build.sh <ENVIRONMENT_NAME> Note <ENVIRONMENT_NAME> is the first part of your name that you used for the <ENVIRONMENT_NAME>-env.sh file Once your cluster is fully deployed you can access the help menu which has a number of options: /data/daffy/ocp/build.sh <ENVIRONMENT_NAME> --help Installing Cloud Paks","title":"Kvm update"},{"location":"Deploying-OCP/kvm-update/#kvm-install","text":"At this point, you have a bastion machine where you have installed the Daffy tool, created your core <ENVIRONMENT_NAME>-env.sh, and can execute the install of OCP on K ernel-based V irtual M achine (KVM). A KVM deployment differs from other OCP deployments in that a KVM is both a bare metal server and a bastion machine. This bastion is where you install the Daffy tool.","title":"KVM Install"},{"location":"Deploying-OCP/kvm-update/#platform-requirements","text":"To use Daffy on KVM, there are some platform info and requirements that need to be gathered or met. We have a simple doc that you should refer to that list all providers and what would be needed. For KVM, it breaks down to the following three basic items: Hardware - enough to run the OCP Cluster based on T-Shirt Sizing OS Version - Ubuntu 20.0.4 ( only supported by Daffy ) Permission - full root authority. For IBM Cloud's KVM bastion, you already have full root authority. For detailed list of the above, you can find in the Daffy Provider Requirements. Please review before proceeding: https://ibm.box.com/v/DaffyProviderRequirements Public DNS Setup: You will need to create DNS entries and domain. For the OpenShift install, you need the following: Registered DNS Name - myexample.com DNS Entries - myexample-com api.${CLUSTER_NAME}. myexample.com ---> ${YOUR.BASTION.IP} api-int.${CLUSTER_NAME}. myexample.com ---> ${YOUR.BASTION.IP} *.apps.${CLUSTER_NAME}.myexample.com ---> ${YOUR.BASTION.IP} Setting up DNS for KVM Deployment with IBM Cloud Internet Services: Register domain name. Detailed instructions for domain registration can be found on the IBM Cloud Domain Name Registration page . Create an IBM Cloud Internet Services (CIS) resource. You can find IBM CIS by searching for \"Internet Services\" in the IBM Cloud catalog. IBM CIS will prompt you for your domain name during resource creation. Add the 3 DNS entries previously mentioned. Detailed instructions can be found on the Getting Started on IBM CIS page . Find your IBM CIS service in your resource list and click on it to access the console On the left menu, click on \"Reliability\" On the Reliability page menu, below your domain name and next to Global load balancers, click on \"DNS.\" At the bottom of the page, you will find a list of DNS records. Click on the \"Add button.\" You can now add the 3 required DNS entries for OCP install","title":"Platform Requirements"},{"location":"Deploying-OCP/kvm-update/#environment-file","text":"Deploying the OpenShift on K ernel-based V irtual M achine only requires two entries to your existing core environment file (< ENVIRONMENT_NAME >-env.sh). Note You can copy the sample file to build your new environment file: cp /data/daffy/env/samples/kvm-upi-env.sh /data/daffy/env/<ENVIRONMENT_NAME>-env.sh Valid Options: OCP_INSTALL_TYPE= kvm-upi BASTION_HOST =\"xxx.xxx.xxx.xxx\" If your host does not have its own public IP address, you need to specify the bastion IP address that the OCP cluster would use to reach your bastion host, i.e. its local IP address you used to connect to the bastion. If you plan to install a cloud pak and/or need storage, you need to set the flag to setup OCS Storage: OCP_CREATE_OPENSHIFT_CONTAINER_STORAGE=true","title":"Environment File"},{"location":"Deploying-OCP/kvm-update/#execution","text":"To deploy your OCP cluster to Kernel-based Virtual Machine, run the build.sh script from the /data/daffy/ocp directory: /data/daffy/ocp/build.sh <ENVIRONMENT_NAME> Note <ENVIRONMENT_NAME> is the first part of your name that you used for the <ENVIRONMENT_NAME>-env.sh file Once your cluster is fully deployed you can access the help menu which has a number of options: /data/daffy/ocp/build.sh <ENVIRONMENT_NAME> --help Installing Cloud Paks","title":"Execution"},{"location":"Deploying-OCP/kvm/","text":"document.title = \"Deploy OCP - KVM\"; KVM Install \u00b6 At this point, you have a bastion machine where you have installed the Daffy tool, created your core environment-name -env.sh, and can execute the install of OCP on ROKS. Platform Requirements \u00b6 To use Daffy on K ernel-based V irtual M achine, there are some platform info and requirements that need to be gathered or met. We have a simple doc that you should refer to that list all providers and what would be needed. For KVM, it breaks down to the following three basic items: Hardware - enough to run the OCP Cluster based on T-Shirt Sizing OS Version - Ubuntu 20.0.4 ( only supported by Daffy ) Permission - full root authority For detailed list of the above, you can find in the Daffy Provider Requirements. Please review before proceeding. https://ibm.box.com/v/DaffyProviderRequirements Public DNS Setup: You will need to create a DNS entries and domain. For the OpenShift install, you need the following: Registered DNS Name - myexample.com DNS Entries - myexample-com api.${CLUSTER_NAME}. myexample.com ---> ${YOUR.BASTION.IP} api-int.${CLUSTER_NAME}. myexample.com ---> ${YOUR.BASTION.IP} *.apps.${CLUSTER_NAME}.myexample.com ---> ${YOUR.BASTION.IP} Setting up DNS for KVM Deployment with OpenShift: INSERT VIDEO Here Environment File \u00b6 Deploying the OpenShift on K ernel-based V irtual M achine only requires three entries to your existing core environment file (< ENVIRONMENT_NAME >-env.sh) plus a local service account file. Note: you can look in the samples directory on your bastion for example of K ernel-based V irtual M achine install : /data/daffy/env/samples/kvm-upi-env.sh You can copy the sample file to build your new environment file. cp /data/daffy/env/samples/kvm-upi-env.sh /data/daffy/env/ -env.sh Valid Options: OCP_INSTALL_TYPE= kvm-upi Optional: BASTION_HOST =\"xxx.xxx.xxx.xxx\" If your host does not have its own public IP address, you need to specify the bastion IP address that the OCP cluster would use to reach your bastion host, i.e. its local IP address you used to connect to the bastion. If you plan to install a cloud pak and/or need storage, you need to set the flag to setup OCS Storage: OCP_CREATE_OPENSHIFT_CONTAINER_STORAGE= true Execution \u00b6 To deploy your OCP cluster to Kernel-based Virtual Machine, run the build.sh script from the /data/daffy/ocp directory /data/daffy/ocp/build.sh <ENVIRONMENT_NAME> Once your cluster is fully deployed you can access the help menu which has a number of options. Note: <ENVIRONMENT_NAME> is the first part of your name that you used for the <ENVIRONMENT_NAME>-env.sh file /data/daffy/ocp/build.sh <ENVIRONMENT_NAME> --help Here is a full example for deploying OpenShift on Kernel-based Virtual Machine with the Daffy process. Installing Cloud Paks","title":"KVM"},{"location":"Deploying-OCP/kvm/#kvm-install","text":"At this point, you have a bastion machine where you have installed the Daffy tool, created your core environment-name -env.sh, and can execute the install of OCP on ROKS.","title":"KVM Install"},{"location":"Deploying-OCP/kvm/#platform-requirements","text":"To use Daffy on K ernel-based V irtual M achine, there are some platform info and requirements that need to be gathered or met. We have a simple doc that you should refer to that list all providers and what would be needed. For KVM, it breaks down to the following three basic items: Hardware - enough to run the OCP Cluster based on T-Shirt Sizing OS Version - Ubuntu 20.0.4 ( only supported by Daffy ) Permission - full root authority For detailed list of the above, you can find in the Daffy Provider Requirements. Please review before proceeding. https://ibm.box.com/v/DaffyProviderRequirements Public DNS Setup: You will need to create a DNS entries and domain. For the OpenShift install, you need the following: Registered DNS Name - myexample.com DNS Entries - myexample-com api.${CLUSTER_NAME}. myexample.com ---> ${YOUR.BASTION.IP} api-int.${CLUSTER_NAME}. myexample.com ---> ${YOUR.BASTION.IP} *.apps.${CLUSTER_NAME}.myexample.com ---> ${YOUR.BASTION.IP} Setting up DNS for KVM Deployment with OpenShift: INSERT VIDEO Here","title":"Platform Requirements"},{"location":"Deploying-OCP/kvm/#environment-file","text":"Deploying the OpenShift on K ernel-based V irtual M achine only requires three entries to your existing core environment file (< ENVIRONMENT_NAME >-env.sh) plus a local service account file. Note: you can look in the samples directory on your bastion for example of K ernel-based V irtual M achine install : /data/daffy/env/samples/kvm-upi-env.sh You can copy the sample file to build your new environment file. cp /data/daffy/env/samples/kvm-upi-env.sh /data/daffy/env/ -env.sh Valid Options: OCP_INSTALL_TYPE= kvm-upi Optional: BASTION_HOST =\"xxx.xxx.xxx.xxx\" If your host does not have its own public IP address, you need to specify the bastion IP address that the OCP cluster would use to reach your bastion host, i.e. its local IP address you used to connect to the bastion. If you plan to install a cloud pak and/or need storage, you need to set the flag to setup OCS Storage: OCP_CREATE_OPENSHIFT_CONTAINER_STORAGE= true","title":"Environment File"},{"location":"Deploying-OCP/kvm/#execution","text":"To deploy your OCP cluster to Kernel-based Virtual Machine, run the build.sh script from the /data/daffy/ocp directory /data/daffy/ocp/build.sh <ENVIRONMENT_NAME> Once your cluster is fully deployed you can access the help menu which has a number of options. Note: <ENVIRONMENT_NAME> is the first part of your name that you used for the <ENVIRONMENT_NAME>-env.sh file /data/daffy/ocp/build.sh <ENVIRONMENT_NAME> --help Here is a full example for deploying OpenShift on Kernel-based Virtual Machine with the Daffy process. Installing Cloud Paks","title":"Execution"},{"location":"More/CommonErrors/","text":"document.title = \"Common Errors\"; Failed to watch *v1.ClusterVersion? \u00b6 The following error can happen on any platform. Its not major error, more of an FYI. Just means that during the cluster setup, the install program was disconnected. Most of the time the install is fine, just a few hiccups. You can ignore this error. E0908 07:05:20.266253 2142422 reflector.go:138] k8s.io/client-go/tools/watch/informerwatcher.go:146: Failed to watch *v1.ClusterVersion: failed to list *v1.ClusterVersion: Get \"https://api.lambda02.daffy-installer.com:6443/apis/config.openshift.io/v1/clusterversions?fieldSelector=metadata.name%3Dversion&limit=500&resourceVersion=0\": EOF W0908 07:05:31.158887 2142422 reflector.go:324] k8s.io/client-go/tools/watch/informerwatcher.go:146: failed to list *v1.ClusterVersion: Get \"https://api.lambda02.daffy-installer.com:6443/apis/config.openshift.io/v1/clusterversions?fieldSelector=metadata.name%3Dversion&limit=500&resourceVersion=0\": EOF I0908 07:05:31.159042 2142422 trace.go:205] Trace[1375718625]: \"Reflector ListAndWatch\" name:k8s.io/client-go/tools/watch/informerwatcher.go:146 (08-Sep-2022 07:05:21.098) (total time: 10060ms): Trace[1375718625]: ---\"Objects listed\" error:Get \"https://api.lambda02.daffy-installer.com:6443/apis/config.openshift.io/v1/clusterversions?fieldSelector=metadata.name%3Dversion&limit=500&resourceVersion=0\": EOF 10060ms (07:05:31.158) Trace[1375718625]: [10.060656161s] [10.060656161s] END FAILED vCPUs needed \u00b6 What does this mean? Precheck IBM Cloud VPC quota (LOG /data/daffy/log/cpdata/ocp/ibmcloud-vpc-quota.log ) ################################################################ ibmcloud resource group-create daffy-quota-test ibmcloud is target --gen 2 ibmcloud is vpc-create daffy-quota-test-vpc \u2014resource-group-name daffy-quota-test ibmcloud is subnet-create daffy-quota-test-subnet daffy-quota-test-vpc us-south-3 --ipv4-cidr-block 10.240.128.0/18 --resource-group-name daffy-quota-test \u274c FAILED vCPUs needed 176 The IBM VPC zone that you're trying to deploy to does not have enough quota of VPC to have a successful deployment. VPC quota is based on the region so you have two options: Deployed to a new zone Request via IBM cloud ticket to increase your quota to 500 https://cloud.ibm.com/docs/vpc?topic=vpc-quotas","title":"Common Errors"},{"location":"More/CommonErrors/#failed-to-watch-v1clusterversion","text":"The following error can happen on any platform. Its not major error, more of an FYI. Just means that during the cluster setup, the install program was disconnected. Most of the time the install is fine, just a few hiccups. You can ignore this error. E0908 07:05:20.266253 2142422 reflector.go:138] k8s.io/client-go/tools/watch/informerwatcher.go:146: Failed to watch *v1.ClusterVersion: failed to list *v1.ClusterVersion: Get \"https://api.lambda02.daffy-installer.com:6443/apis/config.openshift.io/v1/clusterversions?fieldSelector=metadata.name%3Dversion&limit=500&resourceVersion=0\": EOF W0908 07:05:31.158887 2142422 reflector.go:324] k8s.io/client-go/tools/watch/informerwatcher.go:146: failed to list *v1.ClusterVersion: Get \"https://api.lambda02.daffy-installer.com:6443/apis/config.openshift.io/v1/clusterversions?fieldSelector=metadata.name%3Dversion&limit=500&resourceVersion=0\": EOF I0908 07:05:31.159042 2142422 trace.go:205] Trace[1375718625]: \"Reflector ListAndWatch\" name:k8s.io/client-go/tools/watch/informerwatcher.go:146 (08-Sep-2022 07:05:21.098) (total time: 10060ms): Trace[1375718625]: ---\"Objects listed\" error:Get \"https://api.lambda02.daffy-installer.com:6443/apis/config.openshift.io/v1/clusterversions?fieldSelector=metadata.name%3Dversion&limit=500&resourceVersion=0\": EOF 10060ms (07:05:31.158) Trace[1375718625]: [10.060656161s] [10.060656161s] END","title":"Failed to watch *v1.ClusterVersion?"},{"location":"More/CommonErrors/#failed-vcpus-needed","text":"What does this mean? Precheck IBM Cloud VPC quota (LOG /data/daffy/log/cpdata/ocp/ibmcloud-vpc-quota.log ) ################################################################ ibmcloud resource group-create daffy-quota-test ibmcloud is target --gen 2 ibmcloud is vpc-create daffy-quota-test-vpc \u2014resource-group-name daffy-quota-test ibmcloud is subnet-create daffy-quota-test-subnet daffy-quota-test-vpc us-south-3 --ipv4-cidr-block 10.240.128.0/18 --resource-group-name daffy-quota-test \u274c FAILED vCPUs needed 176 The IBM VPC zone that you're trying to deploy to does not have enough quota of VPC to have a successful deployment. VPC quota is based on the region so you have two options: Deployed to a new zone Request via IBM cloud ticket to increase your quota to 500 https://cloud.ibm.com/docs/vpc?topic=vpc-quotas","title":"FAILED vCPUs needed"},{"location":"More/Glossary/","text":"document.title = \"Glossary\"; Glossary \u00b6 Bastion \u00b6 A bastion host is a server whose purpose is to provide access to a private/public network from an external/internal network, such as the Internet or private lan. We use this primarily for the install and tempory management of the OpenShift Cluster. The server will have install tools, access to destired openshift cluster network and may be removed post install. Operator catalogs \u00b6 An Operator catalog is a repository of metadata that Operator Lifecycle Manager (OLM) can query to discover and install Operators and their dependencies on a cluster. OLM always installs Operators from the latest version of a catalog. As of OpenShift Container Platform 4.6, Red Hat-provided catalogs are distributed using index images. An index image, based on the Operator bundle format, is a containerized snapshot of a catalog. It is an immutable artifact that contains the database of pointers to a set of Operator manifest content. A catalog can reference an index image to source its content for OLM on the cluster. As catalogs are updated, the latest versions of Operators change, and older versions may be removed or altered. In addition, when OLM runs on an OpenShift Container Platform cluster in a restricted network environment, it is unable to access the catalogs directly from the internet to pull the latest content. As a cluster administrator, you can create your own custom index image, either based on a Red Hat-provided catalog or from scratch, which can be used to source the catalog content on the cluster. Creating and updating your own index image provides a method for customizing the set of Operators available on the cluster, while also avoiding the aforementioned restricted network environment issues. Cloud Pak \u00b6 IBM Cloud Paks are packaged based on solution domains and harness the combined power of container technology and IBM enterprise expertise to help organizations solve their most pressing challenges: IBM Cloud Pak\u00ae for Data: Unify cloud storage and simplify the collection, organization and analysis of data. IBM Cloud Pak\u00ae for Business Automation: Automate business operations to achieve better performance. IBM Cloud Pak\u00ae for Watson AIOps: Place AI at the core of your IT operations tool chain. Automate operations management decisions while resolving real-world operations management scenarios to deliver actionable insights. IBM Cloud Pak\u00ae for Integration: Automate application and data flows to improve client experiences. Connect your applications and data wherever they live. Get new tools for automated integrations based on APIs that extend capability and modernize flexibility for ongoing adaption. IBM Cloud Pak\u00ae for Security: Generate deeper insights into threats, and orchestrate actions for scalability and automated responses. IBM Cloud Pak\u00ae for Network Automation: Automate networks to deliver zero-touch operations. Cloud Provider \u00b6 A cloud service provider is a third-party company offering a cloud-based platform, infrastructure, application, or storage services. Much like a homeowner would pay for a utility such as electricity or gas, companies typically have to pay only for the amount of cloud services they use, as business demands require. Besides the pay-per-use model, cloud service providers also give companies a wide range of benefits. Businesses can take advantage of scalability and flexibility by not being limited to physical constraints of on-premises servers, the reliability of multiple data centers with multiple redundancies, customization by configuring servers to your preferences, and responsive load balancing that can easily respond to changing demands. Though businesses should also evaluate security considerations of storing information in the cloud to ensure industry-recommended access and compliance management configurations and practices are enacted and met. Cluster \u00b6 Multiple computing nodes or hosts that work together to support an application or middleware such as a database. A cluster is a group of inter-connected computers or hosts that work together to support applications and middleware (e.g. databases). In a cluster, each computer is referred to as a \u201cnode\u201d. Unlike grid computers, where each node performs a different task, computer clusters assign the same task to each node. Nodes in a cluster are usually connected to each other through high-speed local area networks. Each node runs its own instance of an operating system. A computer cluster may range from a simple two-node system connecting two personal computers to a supercomputer with a cluster architecture. Computer clusters are often used for cost-effective high-performance computing (HPC) and high availability (HA). If a single component fails in a computer cluster, the other nodes continue to provide uninterrupted processing. A computer cluster can provide faster processing speed, larger storage capacity, better data integrity, greater reliability and wider availability of resources. Computer clusters are usually dedicated to specific functions, such as load balancing, high availability, high performance or large-scale processing. Control Plane \u00b6 In network routing, the control plane is the part of the router architecture that is concerned with drawing the network topology, or the information in a routing table that defines what to do with incoming packets. Control plane functions, such as participating in routing protocols, run in the architectural control element.[1] In most cases, the routing table contains a list of destination addresses and the outgoing interface(s) associated with each. Control plane logic also can identify certain packets to be discarded, as well as preferential treatment of certain packets for which a high quality of service is defined by such mechanisms as differentiated services. Depending on the specific router implementation, there may be a separate forwarding information base that is populated by the control plane, but used by the high-speed forwarding plane to look up packets and decide how to handle them. In computing, the control plane is the part of the software that configures and shuts down the data plane.[2] By contrast, the data plane is the part of the software that processes the data requests.[3] The data plane is also sometimes referred to as the forwarding plane. The distinction has proven useful in the networking field where it originated, as it separates the concerns: the data plane is optimized for speed of processing, and for simplicity and regularity. The control plane is optimized for customizability, handling policies, handling exceptional situations, and in general facilitating and simplifying the data plane processing.[4] [5] The conceptual separation of the data plane from the control plane has been done for years.[6] An early example is Unix, where the basic file operations are open, close for the control plane and read write for the data plane.[7] Control Plane Domain Name System (DNS) \u00b6 The Domain Name System (DNS) is the hierarchical and decentralized naming system used to identify computers, servr ices, and other resources reachable through the Internet or other Internet Protocol (IP) networks. The resource records contained in the DNS associate domain names with other forms of information. These are most commonly used to map human-friendly domain names to the numerical IP addresses computers need to locate services and devices using the underlying network protocols, but have been extended over time to perform many other functions as well. In a nutshell, its a system where your local computer can call other computers on the internet to translate a website name to a computer IP address. Think of it as the internet phone book. Domain Name System DNS Registrar/Registry \u00b6 A domain name registrar is a company that manages the reservation of Internet domain names. A domain name registrar must be accredited by a generic top-level domain (gTLD) registry or a country code top-level domain (ccTLD) registry. A registrar operates in accordance with the guidelines of the designated domain name registries. In a nutshell, its a company where you can buy and register a website name. Like \"mywebsite.com\" You would now own that name and can have that name redirected to any computer IP address. A domain name registry is a database of all domain names and the associated registrant information in the top level domains of the Domain Name System (DNS) of the Internet that enables third party entities to request administrative control of a domain name. Most registries operate on the top-level and second-level of the DNS. In a nutshell, its where website names are stored and has a mapping from name to IP Address. Like \"www.mywebsite.com\" points to 169.45.45.55 Domain Name Registrar Enviornment Name/File \u00b6 You may see reference to , this is what you name your environment and the base name of the file to store your details for that environment. Example gamm01 -env.sh is your file where gamma01 is your Best practice, but not required, is to name your environment the same as your cluster as this is the core of your environment. Ingress Operator \u00b6 The Ingress Operator makes it possible for external clients to access your service by deploying and managing one or more HAProxy-based Ingress Controllers to handle routing. You can use the Ingress Operator to route traffic by specifying OpenShift Container Platform Route and Kubernetes Ingress resources. Configurations within the Ingress Controller, such as the ability to define endpointPublishingStrategy type and internal load balancing, provide ways to publish Ingress Controller endpoints. In a nutshell, it allows external access to your cluster and all the running pods/services/applications. Its basically an inbound router for your OpenShift Cluster. Ingress Operator Install Type UPI \u00b6 User-provisioned Infrastructure (UPI) provides the ablith to deploy the Openshift container platform (OCP) on existing infrastuure that you created. The user, will create the VMs, networks, loadbalancer, etc. You must create all require infrastrue before you can start the openshift install process. Most users use terraform to build these resources. With Daffy, we call it a UPI, but daffy is the user in the perspective. Daffy will build all the infrastrure components for you. From the user point of view, with daffy, this performs like an IPI install. Install Type IPI \u00b6 Installer-provisioned Infrastructure (IPI) provides a full-stack installation and setup of the Openshift container platform (OCP). It creates Bootstrapping node which will take care deploying the cluster. It will create the VMs, networks, loadbalancer, etc. It creates all require infrastrue during the install for you. With he corrent promission on the provider, this the easiest install. Instal Type MSP \u00b6 Daffy coined this acronym. Managed Service Provider(MSP). This is when a provider does the heavy lifting of install of Openshift container platform (OCP) and also the management of the running cluster. This includes ROSA(AWS), ROKS(IBM) and ARO(Azure). Daffy will call the exposed API from the provider to provision the cluster for you. Master Node \u00b6 The master node is responsible for running several Kubernetes processes that are absolutely necessary to run and manage the cluster properly: API Server: This is essentially the entry-point to the Kubernates cluster, which itself is a container. This is the process that allows communication between different Kubernetes clients and the cluster. The clients include the UI, if we are using the Kubernetes Dashboard, the API if we are running scripts, or the command-line tool. All these clients talk to the API Server to interact with the cluster. Controller Manager: This keeps track of the state of the cluster. It keeps an eye on the cluster and checks whether a node needs to be repaired or restarted. Scheduler: Scheduler ensures proper pod placement on the worker nodes based on several factors such as the available resources and the current load on the cluster. etcd: This is the key-value storage responsible for holding the state of the cluster at any given time. etcd has the configuration information and status data of each node in the cluster. etcd snapshots allow us to recover the whole cluster state, hence it is used in backing up and restoring a cluster. Namespace \u00b6 What is a namespace in OpenShift? Namespaces. A Kubernetes namespace provides a mechanism to scope resources in a cluster. In OpenShift Online, a project is a Kubernetes namespace with additional annotations. Namespaces provide a unique scope for: Named resources to avoid basic naming collisions. Referance Node \u00b6 A node is a virtual or bare-metal machine in a Kubernetes cluster. Worker nodes host your application containers, grouped as pods. The control plane nodes run services that are required to control the Kubernetes cluster. In OpenShift Container Platform, the control plane nodes contain more than just the Kubernetes services for managing the OpenShift Container Platform cluster. Referance Operator \u00b6 Red Hat\u00ae OpenShift\u00ae Operators automate the creation, configuration, and management of instances of Kubernetes-native applications. Operators provide automation at every level of the stack\u2014from managing the parts that make up the platform all the way to applications that are provided as a managed service. Referance Pod \u00b6 Pods are the smallest deployable units of computing that you can create and manage in Kubernetes. A Pod (as in a pod of whales or pea pod) is a group of one or more containers, with shared storage and network resources, and a specification for how to run the containers. A Pod's contents are always co-located and co-scheduled, and run in a shared context. A Pod models an application-specific \"logical host\" it contains one or more application containers which are relatively tightly coupled. In non-cloud contexts, applications executed on the same physical or virtual machine are analogous to cloud applications executed on the same logical host. As well as application containers, a Pod can contain init containers that run during Pod startup. You can also inject ephemeral containers for debugging if your cluster offers this. Service \u00b6 A Kubernetes service serves as an internal load balancer. It identifies a set of replicated pods in order to proxy the connections it receives to them. Backing pods can be added to or removed from a service arbitrarily while the service remains consistently available, enabling anything that depends on the service to refer to it at a consistent address. The default service clusterIP addresses are from the OpenShift Online internal network and they are used to permit pods to access each other. Referance Subscription \u00b6 With OpenShift, you need to have a subscription to deploy and run the cluster. This subscription is tied to a billing account for Yearly bililng and payment. You cluster is tied to the subscription via the pull secret that is stored in the cluster. At installed time you supplied this but can change this post install at anytime. Storage Class \u00b6 A StorageClass provides a way for administrators to describe the \"classes\" of storage they offer. Different classes might map to quality-of-service levels, or to backup policies, or to arbitrary policies determined by the cluster administrators. Kubernetes itself is unopinionated about what classes represent. This concept is sometimes called \"profiles\" in other storage systems. Worker Node \u00b6 The worker nodes are the part of the Kubernetes clusters which actually execute the containers and applications on them. They have two main components, the Kubelet Service and the Kube-proxy Service. Kubelet Service: Each worker node has a Kubelet process running on it that allows the cluster to talk to each other and execute some tasks on the worker nodes, such as running application processes. It listens for instructions from the Api Server and manages containers running on the node. Kube-proxy Service: The Kube-Proxy Service is responsible for enabling communication between services within the cluster. These worker nodes have docker containers for each application running on them. There may be a different number of containers running on each node depending on the distribution of the workload. Worker nodes are generally more powerful than master nodes because they have to run hundreds of clusters on them. However, master nodes hold more significance because they manage the distribution of workload and the state of the cluster.","title":"Glossary"},{"location":"More/Glossary/#glossary","text":"","title":"Glossary"},{"location":"More/Glossary/#bastion","text":"A bastion host is a server whose purpose is to provide access to a private/public network from an external/internal network, such as the Internet or private lan. We use this primarily for the install and tempory management of the OpenShift Cluster. The server will have install tools, access to destired openshift cluster network and may be removed post install.","title":"Bastion"},{"location":"More/Glossary/#operator-catalogs","text":"An Operator catalog is a repository of metadata that Operator Lifecycle Manager (OLM) can query to discover and install Operators and their dependencies on a cluster. OLM always installs Operators from the latest version of a catalog. As of OpenShift Container Platform 4.6, Red Hat-provided catalogs are distributed using index images. An index image, based on the Operator bundle format, is a containerized snapshot of a catalog. It is an immutable artifact that contains the database of pointers to a set of Operator manifest content. A catalog can reference an index image to source its content for OLM on the cluster. As catalogs are updated, the latest versions of Operators change, and older versions may be removed or altered. In addition, when OLM runs on an OpenShift Container Platform cluster in a restricted network environment, it is unable to access the catalogs directly from the internet to pull the latest content. As a cluster administrator, you can create your own custom index image, either based on a Red Hat-provided catalog or from scratch, which can be used to source the catalog content on the cluster. Creating and updating your own index image provides a method for customizing the set of Operators available on the cluster, while also avoiding the aforementioned restricted network environment issues.","title":"Operator catalogs"},{"location":"More/Glossary/#cloud-pak","text":"IBM Cloud Paks are packaged based on solution domains and harness the combined power of container technology and IBM enterprise expertise to help organizations solve their most pressing challenges: IBM Cloud Pak\u00ae for Data: Unify cloud storage and simplify the collection, organization and analysis of data. IBM Cloud Pak\u00ae for Business Automation: Automate business operations to achieve better performance. IBM Cloud Pak\u00ae for Watson AIOps: Place AI at the core of your IT operations tool chain. Automate operations management decisions while resolving real-world operations management scenarios to deliver actionable insights. IBM Cloud Pak\u00ae for Integration: Automate application and data flows to improve client experiences. Connect your applications and data wherever they live. Get new tools for automated integrations based on APIs that extend capability and modernize flexibility for ongoing adaption. IBM Cloud Pak\u00ae for Security: Generate deeper insights into threats, and orchestrate actions for scalability and automated responses. IBM Cloud Pak\u00ae for Network Automation: Automate networks to deliver zero-touch operations.","title":"Cloud Pak"},{"location":"More/Glossary/#cloud-provider","text":"A cloud service provider is a third-party company offering a cloud-based platform, infrastructure, application, or storage services. Much like a homeowner would pay for a utility such as electricity or gas, companies typically have to pay only for the amount of cloud services they use, as business demands require. Besides the pay-per-use model, cloud service providers also give companies a wide range of benefits. Businesses can take advantage of scalability and flexibility by not being limited to physical constraints of on-premises servers, the reliability of multiple data centers with multiple redundancies, customization by configuring servers to your preferences, and responsive load balancing that can easily respond to changing demands. Though businesses should also evaluate security considerations of storing information in the cloud to ensure industry-recommended access and compliance management configurations and practices are enacted and met.","title":"Cloud Provider"},{"location":"More/Glossary/#cluster","text":"Multiple computing nodes or hosts that work together to support an application or middleware such as a database. A cluster is a group of inter-connected computers or hosts that work together to support applications and middleware (e.g. databases). In a cluster, each computer is referred to as a \u201cnode\u201d. Unlike grid computers, where each node performs a different task, computer clusters assign the same task to each node. Nodes in a cluster are usually connected to each other through high-speed local area networks. Each node runs its own instance of an operating system. A computer cluster may range from a simple two-node system connecting two personal computers to a supercomputer with a cluster architecture. Computer clusters are often used for cost-effective high-performance computing (HPC) and high availability (HA). If a single component fails in a computer cluster, the other nodes continue to provide uninterrupted processing. A computer cluster can provide faster processing speed, larger storage capacity, better data integrity, greater reliability and wider availability of resources. Computer clusters are usually dedicated to specific functions, such as load balancing, high availability, high performance or large-scale processing.","title":"Cluster"},{"location":"More/Glossary/#control-plane","text":"In network routing, the control plane is the part of the router architecture that is concerned with drawing the network topology, or the information in a routing table that defines what to do with incoming packets. Control plane functions, such as participating in routing protocols, run in the architectural control element.[1] In most cases, the routing table contains a list of destination addresses and the outgoing interface(s) associated with each. Control plane logic also can identify certain packets to be discarded, as well as preferential treatment of certain packets for which a high quality of service is defined by such mechanisms as differentiated services. Depending on the specific router implementation, there may be a separate forwarding information base that is populated by the control plane, but used by the high-speed forwarding plane to look up packets and decide how to handle them. In computing, the control plane is the part of the software that configures and shuts down the data plane.[2] By contrast, the data plane is the part of the software that processes the data requests.[3] The data plane is also sometimes referred to as the forwarding plane. The distinction has proven useful in the networking field where it originated, as it separates the concerns: the data plane is optimized for speed of processing, and for simplicity and regularity. The control plane is optimized for customizability, handling policies, handling exceptional situations, and in general facilitating and simplifying the data plane processing.[4] [5] The conceptual separation of the data plane from the control plane has been done for years.[6] An early example is Unix, where the basic file operations are open, close for the control plane and read write for the data plane.[7] Control Plane","title":"Control Plane"},{"location":"More/Glossary/#domain-name-system-dns","text":"The Domain Name System (DNS) is the hierarchical and decentralized naming system used to identify computers, servr ices, and other resources reachable through the Internet or other Internet Protocol (IP) networks. The resource records contained in the DNS associate domain names with other forms of information. These are most commonly used to map human-friendly domain names to the numerical IP addresses computers need to locate services and devices using the underlying network protocols, but have been extended over time to perform many other functions as well. In a nutshell, its a system where your local computer can call other computers on the internet to translate a website name to a computer IP address. Think of it as the internet phone book. Domain Name System","title":"Domain Name System (DNS)"},{"location":"More/Glossary/#dns-registrarregistry","text":"A domain name registrar is a company that manages the reservation of Internet domain names. A domain name registrar must be accredited by a generic top-level domain (gTLD) registry or a country code top-level domain (ccTLD) registry. A registrar operates in accordance with the guidelines of the designated domain name registries. In a nutshell, its a company where you can buy and register a website name. Like \"mywebsite.com\" You would now own that name and can have that name redirected to any computer IP address. A domain name registry is a database of all domain names and the associated registrant information in the top level domains of the Domain Name System (DNS) of the Internet that enables third party entities to request administrative control of a domain name. Most registries operate on the top-level and second-level of the DNS. In a nutshell, its where website names are stored and has a mapping from name to IP Address. Like \"www.mywebsite.com\" points to 169.45.45.55 Domain Name Registrar","title":"DNS Registrar/Registry"},{"location":"More/Glossary/#enviornment-namefile","text":"You may see reference to , this is what you name your environment and the base name of the file to store your details for that environment. Example gamm01 -env.sh is your file where gamma01 is your Best practice, but not required, is to name your environment the same as your cluster as this is the core of your environment.","title":"Enviornment Name/File"},{"location":"More/Glossary/#ingress-operator","text":"The Ingress Operator makes it possible for external clients to access your service by deploying and managing one or more HAProxy-based Ingress Controllers to handle routing. You can use the Ingress Operator to route traffic by specifying OpenShift Container Platform Route and Kubernetes Ingress resources. Configurations within the Ingress Controller, such as the ability to define endpointPublishingStrategy type and internal load balancing, provide ways to publish Ingress Controller endpoints. In a nutshell, it allows external access to your cluster and all the running pods/services/applications. Its basically an inbound router for your OpenShift Cluster. Ingress Operator","title":"Ingress Operator"},{"location":"More/Glossary/#install-type-upi","text":"User-provisioned Infrastructure (UPI) provides the ablith to deploy the Openshift container platform (OCP) on existing infrastuure that you created. The user, will create the VMs, networks, loadbalancer, etc. You must create all require infrastrue before you can start the openshift install process. Most users use terraform to build these resources. With Daffy, we call it a UPI, but daffy is the user in the perspective. Daffy will build all the infrastrure components for you. From the user point of view, with daffy, this performs like an IPI install.","title":"Install Type UPI"},{"location":"More/Glossary/#install-type-ipi","text":"Installer-provisioned Infrastructure (IPI) provides a full-stack installation and setup of the Openshift container platform (OCP). It creates Bootstrapping node which will take care deploying the cluster. It will create the VMs, networks, loadbalancer, etc. It creates all require infrastrue during the install for you. With he corrent promission on the provider, this the easiest install.","title":"Install Type IPI"},{"location":"More/Glossary/#instal-type-msp","text":"Daffy coined this acronym. Managed Service Provider(MSP). This is when a provider does the heavy lifting of install of Openshift container platform (OCP) and also the management of the running cluster. This includes ROSA(AWS), ROKS(IBM) and ARO(Azure). Daffy will call the exposed API from the provider to provision the cluster for you.","title":"Instal Type MSP"},{"location":"More/Glossary/#master-node","text":"The master node is responsible for running several Kubernetes processes that are absolutely necessary to run and manage the cluster properly: API Server: This is essentially the entry-point to the Kubernates cluster, which itself is a container. This is the process that allows communication between different Kubernetes clients and the cluster. The clients include the UI, if we are using the Kubernetes Dashboard, the API if we are running scripts, or the command-line tool. All these clients talk to the API Server to interact with the cluster. Controller Manager: This keeps track of the state of the cluster. It keeps an eye on the cluster and checks whether a node needs to be repaired or restarted. Scheduler: Scheduler ensures proper pod placement on the worker nodes based on several factors such as the available resources and the current load on the cluster. etcd: This is the key-value storage responsible for holding the state of the cluster at any given time. etcd has the configuration information and status data of each node in the cluster. etcd snapshots allow us to recover the whole cluster state, hence it is used in backing up and restoring a cluster.","title":"Master Node"},{"location":"More/Glossary/#namespace","text":"What is a namespace in OpenShift? Namespaces. A Kubernetes namespace provides a mechanism to scope resources in a cluster. In OpenShift Online, a project is a Kubernetes namespace with additional annotations. Namespaces provide a unique scope for: Named resources to avoid basic naming collisions. Referance","title":"Namespace"},{"location":"More/Glossary/#node","text":"A node is a virtual or bare-metal machine in a Kubernetes cluster. Worker nodes host your application containers, grouped as pods. The control plane nodes run services that are required to control the Kubernetes cluster. In OpenShift Container Platform, the control plane nodes contain more than just the Kubernetes services for managing the OpenShift Container Platform cluster. Referance","title":"Node"},{"location":"More/Glossary/#operator","text":"Red Hat\u00ae OpenShift\u00ae Operators automate the creation, configuration, and management of instances of Kubernetes-native applications. Operators provide automation at every level of the stack\u2014from managing the parts that make up the platform all the way to applications that are provided as a managed service. Referance","title":"Operator"},{"location":"More/Glossary/#pod","text":"Pods are the smallest deployable units of computing that you can create and manage in Kubernetes. A Pod (as in a pod of whales or pea pod) is a group of one or more containers, with shared storage and network resources, and a specification for how to run the containers. A Pod's contents are always co-located and co-scheduled, and run in a shared context. A Pod models an application-specific \"logical host\" it contains one or more application containers which are relatively tightly coupled. In non-cloud contexts, applications executed on the same physical or virtual machine are analogous to cloud applications executed on the same logical host. As well as application containers, a Pod can contain init containers that run during Pod startup. You can also inject ephemeral containers for debugging if your cluster offers this.","title":"Pod"},{"location":"More/Glossary/#service","text":"A Kubernetes service serves as an internal load balancer. It identifies a set of replicated pods in order to proxy the connections it receives to them. Backing pods can be added to or removed from a service arbitrarily while the service remains consistently available, enabling anything that depends on the service to refer to it at a consistent address. The default service clusterIP addresses are from the OpenShift Online internal network and they are used to permit pods to access each other. Referance","title":"Service"},{"location":"More/Glossary/#subscription","text":"With OpenShift, you need to have a subscription to deploy and run the cluster. This subscription is tied to a billing account for Yearly bililng and payment. You cluster is tied to the subscription via the pull secret that is stored in the cluster. At installed time you supplied this but can change this post install at anytime.","title":"Subscription"},{"location":"More/Glossary/#storage-class","text":"A StorageClass provides a way for administrators to describe the \"classes\" of storage they offer. Different classes might map to quality-of-service levels, or to backup policies, or to arbitrary policies determined by the cluster administrators. Kubernetes itself is unopinionated about what classes represent. This concept is sometimes called \"profiles\" in other storage systems.","title":"Storage Class"},{"location":"More/Glossary/#worker-node","text":"The worker nodes are the part of the Kubernetes clusters which actually execute the containers and applications on them. They have two main components, the Kubelet Service and the Kube-proxy Service. Kubelet Service: Each worker node has a Kubelet process running on it that allows the cluster to talk to each other and execute some tasks on the worker nodes, such as running application processes. It listens for instructions from the Api Server and manages containers running on the node. Kube-proxy Service: The Kube-Proxy Service is responsible for enabling communication between services within the cluster. These worker nodes have docker containers for each application running on them. There may be a different number of containers running on each node depending on the distribution of the workload. Worker nodes are generally more powerful than master nodes because they have to run hundreds of clusters on them. However, master nodes hold more significance because they manage the distribution of workload and the state of the cluster.","title":"Worker Node"},{"location":"More/secure-server-ssh-only-login/","text":"Enable SSH Key ONLY Login - Disable Password \u00b6 It is highly recommended that you disable password login for root access and ONLY allow login with ssh key. To do that follow these steps. Warning As a recommmendation, it is advised that you have a second session open while you are making these chagnes. Once you enable the above changes you will no longer be able to log into the server using passwords. If your ssh key configuration is not corrrect, you will not be able to log in. Leave the second session open until you verify that you can in fact log in with your ssh key. Setup SSH \u00b6 create new authorized_keys file mkdir -p /root/.ssh touch authorized_keys verify you can log in with your ssh key Edit the ssh configuration \u00b6 Edit the ssh configuratioh file vim /etc/ssh/sshd_config Modify these to values. ChallengeResponseAuthentication no PasswordAuthentication no Reload the SSH Config \u00b6 Execute the reload command to enable the changes(as root user). Ubuntu systemctl restart ssh.service RHEL systemctl restart sshd.service","title":"Secure your Bastion"},{"location":"More/secure-server-ssh-only-login/#enable-ssh-key-only-login-disable-password","text":"It is highly recommended that you disable password login for root access and ONLY allow login with ssh key. To do that follow these steps. Warning As a recommmendation, it is advised that you have a second session open while you are making these chagnes. Once you enable the above changes you will no longer be able to log into the server using passwords. If your ssh key configuration is not corrrect, you will not be able to log in. Leave the second session open until you verify that you can in fact log in with your ssh key.","title":"Enable SSH Key ONLY Login - Disable Password"},{"location":"More/secure-server-ssh-only-login/#setup-ssh","text":"create new authorized_keys file mkdir -p /root/.ssh touch authorized_keys verify you can log in with your ssh key","title":"Setup SSH"},{"location":"More/secure-server-ssh-only-login/#edit-the-ssh-configuration","text":"Edit the ssh configuratioh file vim /etc/ssh/sshd_config Modify these to values. ChallengeResponseAuthentication no PasswordAuthentication no","title":"Edit the ssh configuration"},{"location":"More/secure-server-ssh-only-login/#reload-the-ssh-config","text":"Execute the reload command to enable the changes(as root user). Ubuntu systemctl restart ssh.service RHEL systemctl restart sshd.service","title":"Reload the SSH Config"},{"location":"More/FAQ/","text":"document.title = \"FAQ\"; Why the name Daffy? \u00b6 Its a great name and mascot. What is your favorite Cloud Pak? \u00b6 Cloud Pak for Business Automation Can this be run on the XBOX platform? \u00b6 No, sorry. Daffy Slack Channels: \u00b6 #daffy-user-group I put invalid or wrong Red Hat Pull Secret during the OCP install process, how do you fix? \u00b6 You can run the security-cleanup.sh script to remove existing pull secret. /data/daffy/security-cleanup.sh --pullSecret Follow all the instructions from the above script, then you can run daffy install process again I put invalid or wrong IBM Entitlement during the CP install process, how do you fix? \u00b6 /data/daffy/security-cleanup.sh --ibm Follow all the instructions from the above script, then you can run daffy install process again How can I create my own bastion? \u00b6 We have a document outlining how to create your own bastion in two ways A paid IBM Cloud Account IBM Technology Zone Create Bastion Steps ROKS unbound immediate PersistentVolumeClaims \u00b6 This means the cluster has requested storage from MSP provider(IBM ROKS) and it is still provisioning it. It can take a few minutes or hours at times or sometimes never get provisioned. If it is stuck for more then 60 minutes, best option is to delete the stuck pod requesting storage or you can run the daffy rebuild process to get it working in a few hours. What does rebuild.sh do? \u00b6 Rebuild starts over with your cluster using your environment file. Delete Cluster and all resources it created Install new Cluster Install new Cloud Pak Install all services for your Cloud Pak rebuild.sh requires your env file, that drives all the info it would need. As you can see, it will destroy everything and give you new env, nothing will be saved in cluster.","title":"FAQ"},{"location":"More/FAQ/#why-the-name-daffy","text":"Its a great name and mascot.","title":"Why the name Daffy?"},{"location":"More/FAQ/#what-is-your-favorite-cloud-pak","text":"Cloud Pak for Business Automation","title":"What is your favorite Cloud Pak?"},{"location":"More/FAQ/#can-this-be-run-on-the-xbox-platform","text":"No, sorry.","title":"Can this be run on the XBOX platform?"},{"location":"More/FAQ/#daffy-slack-channels","text":"#daffy-user-group","title":"Daffy Slack Channels:"},{"location":"More/FAQ/#i-put-invalid-or-wrong-red-hat-pull-secret-during-the-ocp-install-process-how-do-you-fix","text":"You can run the security-cleanup.sh script to remove existing pull secret. /data/daffy/security-cleanup.sh --pullSecret Follow all the instructions from the above script, then you can run daffy install process again","title":"I put invalid or wrong Red Hat Pull Secret during the OCP install process, how do you fix?"},{"location":"More/FAQ/#i-put-invalid-or-wrong-ibm-entitlement-during-the-cp-install-process-how-do-you-fix","text":"/data/daffy/security-cleanup.sh --ibm Follow all the instructions from the above script, then you can run daffy install process again","title":"I put invalid or wrong IBM Entitlement during the CP install process, how do you fix?"},{"location":"More/FAQ/#how-can-i-create-my-own-bastion","text":"We have a document outlining how to create your own bastion in two ways A paid IBM Cloud Account IBM Technology Zone Create Bastion Steps","title":"How can I create my own bastion?"},{"location":"More/FAQ/#roks-unbound-immediate-persistentvolumeclaims","text":"This means the cluster has requested storage from MSP provider(IBM ROKS) and it is still provisioning it. It can take a few minutes or hours at times or sometimes never get provisioned. If it is stuck for more then 60 minutes, best option is to delete the stuck pod requesting storage or you can run the daffy rebuild process to get it working in a few hours.","title":"ROKS unbound immediate PersistentVolumeClaims"},{"location":"More/FAQ/#what-does-rebuildsh-do","text":"Rebuild starts over with your cluster using your environment file. Delete Cluster and all resources it created Install new Cluster Install new Cloud Pak Install all services for your Cloud Pak rebuild.sh requires your env file, that drives all the info it would need. As you can see, it will destroy everything and give you new env, nothing will be saved in cluster.","title":"What does rebuild.sh do?"},{"location":"More/FAQ/Deployment-Estimates/","text":"document.title = \"Deployment Estimates\"; Note Below are some estimates for Daffy T-Shirt sizes running on each cloud platform. These are rough examples, customer accounts may have more/less discounts then the estimates we obtained. These numbers are used to help prepare some high level cost estimates for POC's for OpenShift and IBM Cloud Paks. Google Cloud Platform \u00b6 Calculator T-Shirt Size 1 Hour 1 Day 1 Week 1 Month Large $6.69 $160.56 $1,123.92 $4,816.80 Estimate on 03-06-2022 Amazon Web Services \u00b6 Calculator T-Shirt Size 1 Hour 1 Day 1 Week 1 Month Large Coming Coming Coming Coming Azure \u00b6 Calculator T-Shirt Size 1 Hour 1 Day 1 Week 1 Month Large Coming Coming Coming Coming Red Hat OpenShift Kubernetes Services on IBM Cloud \u00b6 Calculator T-Shirt Size 1 Hour 1 Day 1 Week 1 Month Large $5.70 $136.80 $957.60 $4,104.00 Estimate on 03-07-2022","title":"Deployment Estimates"},{"location":"More/FAQ/Deployment-Estimates/#google-cloud-platform","text":"Calculator T-Shirt Size 1 Hour 1 Day 1 Week 1 Month Large $6.69 $160.56 $1,123.92 $4,816.80 Estimate on 03-06-2022","title":"Google Cloud Platform"},{"location":"More/FAQ/Deployment-Estimates/#amazon-web-services","text":"Calculator T-Shirt Size 1 Hour 1 Day 1 Week 1 Month Large Coming Coming Coming Coming","title":"Amazon Web Services"},{"location":"More/FAQ/Deployment-Estimates/#azure","text":"Calculator T-Shirt Size 1 Hour 1 Day 1 Week 1 Month Large Coming Coming Coming Coming","title":"Azure"},{"location":"More/FAQ/Deployment-Estimates/#red-hat-openshift-kubernetes-services-on-ibm-cloud","text":"Calculator T-Shirt Size 1 Hour 1 Day 1 Week 1 Month Large $5.70 $136.80 $957.60 $4,104.00 Estimate on 03-07-2022","title":"Red Hat OpenShift Kubernetes Services on IBM Cloud"},{"location":"More/FAQ/IBM-Entitlement-Keys/","text":"document.title = \"IBM Entitlement\"; To run software from the IBM Entitled Registry, you must supply your entitlement key as a Kubernetes pull secret. Below are links for customers to obtain trial keys. Components Length Link Cloud Pak for Data 60 days https://www.ibm.com/account/reg/us-en/signup?formid=urx-42212 Cloud Pak for Business Automation 60 days https://www.ibm.com/account/reg/us-en/signup?formid=urx-44505 Cloud Pak for Integration 60 days https://www.ibm.com/account/reg/us-en/signup?formid=urx-46640 Cloud Pak for AIOps 60 days https://www.ibm.com/account/reg/us-en/signup?formid=urx-51074 Cloud Pak for Security 60 days PENDING","title":"IBM Entitlement Keys"},{"location":"Overview/","text":"document.title = \"Overview\"; Demo Video OpenShift install on Azure with Cloud Pak for Data and Watson Knowledge Catalog HTML Video embed","title":"Index"},{"location":"Overview/Process/","text":"document.title = \"Process\"; Where Do I Start \u00b6 The daffy tool allows you to start at any step. Before you start, you need to see what your goal is. First step is to get the Prereq and Core steps done. Prereq Core Steps Then you need to decide your end goal and what you want daffy to assist with. I need to build full stack (OpenShift, Cloud Pak Operators and Cloud Pak Services) Run Step 1, 2 and 3 Do you need to build just the OpenShift Cluster? Run Step 1 Do you need to deploy just the Cloud Pak Operators? Run Step 2 Do you need to deploy just cloud Pak Services? Run Step 3 Step 1 \u00b6 With Step one you will need information for your provider. This includes provider ids, secrets, regions, etc. The items that daffy will need to create your cluster in the given provider. In the end you are asking daffy to do a full install of OpenShift in the provider, so it needs all the info necessary to access the provider and build out infrastructure. Deploying OCP Step 2 \u00b6 If you are starting at step 2, you would NOT need provider info(provider ids, secrets, regions, etc.). The only thing step 2 needs is OpenShift cluster info. Access to cluster and cluster and Cloud Pak details. If you used step 1, you already have access to cluster. But if you are staring at step 2, you first need to logon on to your cluster via the oc login command from your bastion host where you plan to run daffy. Deploying Cloud-Paks Step 3 \u00b6 If you are starting at step 3, you would NOT need provider info(provider ids, secrets, regions, etc.). The only thing step 3 needs is OpenShift cluster info. Access to cluster and cluster and Cloud Pak details. But if you are staring at step 3, you first need to logon on to your cluster via the oc login command from your bastion host where you plan to run daffy. Deploying Cloud-Paks","title":"Process"},{"location":"Overview/Process/#where-do-i-start","text":"The daffy tool allows you to start at any step. Before you start, you need to see what your goal is. First step is to get the Prereq and Core steps done. Prereq Core Steps Then you need to decide your end goal and what you want daffy to assist with. I need to build full stack (OpenShift, Cloud Pak Operators and Cloud Pak Services) Run Step 1, 2 and 3 Do you need to build just the OpenShift Cluster? Run Step 1 Do you need to deploy just the Cloud Pak Operators? Run Step 2 Do you need to deploy just cloud Pak Services? Run Step 3","title":"Where Do I Start"},{"location":"Overview/Process/#step-1","text":"With Step one you will need information for your provider. This includes provider ids, secrets, regions, etc. The items that daffy will need to create your cluster in the given provider. In the end you are asking daffy to do a full install of OpenShift in the provider, so it needs all the info necessary to access the provider and build out infrastructure. Deploying OCP","title":"Step 1"},{"location":"Overview/Process/#step-2","text":"If you are starting at step 2, you would NOT need provider info(provider ids, secrets, regions, etc.). The only thing step 2 needs is OpenShift cluster info. Access to cluster and cluster and Cloud Pak details. If you used step 1, you already have access to cluster. But if you are staring at step 2, you first need to logon on to your cluster via the oc login command from your bastion host where you plan to run daffy. Deploying Cloud-Paks","title":"Step 2"},{"location":"Overview/Process/#step-3","text":"If you are starting at step 3, you would NOT need provider info(provider ids, secrets, regions, etc.). The only thing step 3 needs is OpenShift cluster info. Access to cluster and cluster and Cloud Pak details. But if you are staring at step 3, you first need to logon on to your cluster via the oc login command from your bastion host where you plan to run daffy. Deploying Cloud-Paks","title":"Step 3"},{"location":"Overview/Tools-Installed/","text":"document.title = \"Tools Installed\"; * { box-sizing: border-box; } .row { margin-left:-5px; margin-right:-5px; } .column { float: left; width: 50%; padding: 5px; } /* Clearfix (clear floats) */ .row::after { content: \"\"; clear: both; display: table; } table { border-collapse: collapse; border-spacing: 0; width: 100%; border: 1px solid #ddd; } th, td { text-align: left; padding: 16px; } tr:nth-child(even) { background-color: #f2f2f2; } Throughout the Daffy process, it will install all support tools it would need. Depending on the step of Daffy you are running and feature it is using, the tools will be different. If the tool is present already and the correct version, it will not be installed. Core Tool When oc always openshfit-install always kubectl always nmon always curl always nano always vim always tree always wget always jq always dnsutils always openssh-client always grepcidr always acl always(RHEL) dos2unix always nfs-kernel-server NFS Option only nfs-common NFS Option only openssl always Cloud CLI Tool When awscli AWS Install only azure-cli Azure Install only gcloud GCP Install only cloudctl CP4D/CP4BA python2 cloudctl Flag python3 cloudctl Flag pip2 cloudctl Flag pip3 cloudctl Flag ibmcloud ROKS/IBM-IPI/IBM DNS/Secrets Manager ibm-cp-automation CP4BA Install Only podman All Cloud Paks VSphere Tool When rhcos-4.6.XX-x86_64-vmware.x86_64.ova VSphere IPI only rhcos-4.7.XX-x86_64-vmware.x86_64.ova VSphere IPI only rhcos-4.9.XX-x86_64-vmware.x86_64.ova VSphere IPI only rhcos-4.10.XX-x86_64-vmware.x86_64.ova VSphere IPI only govc VSphere Install only Airgap Tool When mirror-registry build local registry only rhcos-4.6.XX-x86_64-live.x86_64.iso build local registry only rhcos-4.7.XX-x86_64-live.x86_64.iso build local registry only rhcos-4.9.XX-x86_64-live.x86_64.iso build local registry only rhcos-4.10.XX-x86_64-live.x86_64.iso build local registry only UPI Tool When haproxy KVM or VSphere install Only dnsmasq KVM or VSphere install Only KVM Tool When lvm2 KVM install Only bridge-utils KVM install Only qemu-kvm KVM install Only virtinst KVM install Only libvirt-daemon KVM install Only virt-manager KVM install Only cifs-utils KVM install Only libosinfo-bin KVM install Only uvtool KVM install Only matchbox KVM install Only rhcos-live-initramfs.x86_64.img KVM install Only rhcos-live-kernel-x86_64 KVM install Only rhcos-live-rootfs.x86_64.img KVM install Only KVM Dashboard Tool When apache2 VMDashboard Only php VMDashboard Only libapache2-mod-php VMDashboard Only php-mysql VMDashboard Only php-xml VMDashboard Only php-libvirt-php VMDashboard Only python VMDashboard Only RPA Server Tool When mssql2019 RPA Server openldap RPA Server CP4D Tool When cpd-cli CP4D 4.5.X","title":"Tools Installed"},{"location":"Security/","text":"document.title = \"Security\"; Security \u00b6 IBM Secrets Manager SSL Certificates Overview \u00b6 Trusted certificates can be used to create secure connections to a server via the Internet. A certificate is essential in order to circumvent a malicious party which happens to be on the route to a target server which acts as if it were the target. Such a scenario is commonly referred to as a man-in-the-middle attack. The client uses the CA certificate to authenticate the CA signature on the server certificate, as part of the authorizations before launching a secure connection. Usually, client software\u2014for example, browsers\u2014include a set of trusted CA certificates. This makes sense, as many users need to trust their client software. A malicious or compromised client can skip any security check and still fool its users into believing otherwise. Useful Links \u00b6 SSL Certificates IBM Secrets Manager","title":"Index"},{"location":"Security/#security","text":"IBM Secrets Manager SSL Certificates","title":"Security"},{"location":"Security/#overview","text":"Trusted certificates can be used to create secure connections to a server via the Internet. A certificate is essential in order to circumvent a malicious party which happens to be on the route to a target server which acts as if it were the target. Such a scenario is commonly referred to as a man-in-the-middle attack. The client uses the CA certificate to authenticate the CA signature on the server certificate, as part of the authorizations before launching a secure connection. Usually, client software\u2014for example, browsers\u2014include a set of trusted CA certificates. This makes sense, as many users need to trust their client software. A malicious or compromised client can skip any security check and still fool its users into believing otherwise.","title":"Overview"},{"location":"Security/#useful-links","text":"SSL Certificates IBM Secrets Manager","title":"Useful Links"},{"location":"Security/IBMSecretsManager/","text":"document.title = \"IBM Cloud Secrets Manager\"; IBM Cloud Secrets Manager \u00b6 Assumptions \u00b6 With IBM Cloud Secrets Manager, you can create, lease, and centrally manage secrets that are used in IBM Cloud services or your custom-built applications. Secrets are stored in a dedicated instance of Secrets Manager, built on open source HashiCorp Vault. For daffy to be able to download your certs from IBM Secrets Manager, we just need a few variables. Below are the variables you will need and how to obtain them from the IBM Cloud Web Portal IBM . Info We will not show you how to setup IBM Secrets Manager, Cert Authority or request your certs. Please follow the standard IBM Doc listed in the useful links below. Environment variables \u00b6 Variable Name Info Required IBM_SECRET_CERT_API_KEY The API key that can access your Secrets Yes(Will Prompt at runtime if the other three are present) IBM_SECRET_SERVICE_API_URL The host name for your Secret Service Yes IBM_SECRET_CERT_ID_INGRESS_API The ID for your API Cert Yes IBM_SECRET_CERT_ID_INGRESS_APPS The ID for your APPS Cert Yes Building Certs \u00b6 When you build your certs, you have two options. You can build a cert for each URL (*.apps & api ) Apps Cert Common Name - *.apps.${CLUSTER}.${BASE_DOMAIN} API Cert Common Name - api.${CLUSTER}.${BASE_DOMAIN} You can build a single cert with both URLs as alt names of the cert Common Name - ${CLUSTER}.${BASE_DOMAIN} Alt Names - ${CLUSTER}.${BASE_DOMAIN}, *.apps .${CLUSTER}.${BASE_DOMAIN}, api .${CLUSTER}.${BASE_DOMAIN} Info If you build one cert, you would just use the same cert ID for both apps and api values in env file Find Your Values \u00b6 Cert API Key \u00b6 IBM_SECRET_CERT_API_KEY In your IBM Cloud account, you need to create/use an API key that has authority to access your secrets. IBM Cloud API Keys Create Key Service API Url \u00b6 IBM_SECRET_SERVICE_API_URL This is the Service API Url for your Secretes Manger Instance Get Service URL Cert ID Ingress API \u00b6 IBM_SECRET_CERT_ID_INGRESS_API Each Cert you create has a unique ID, we will use that ID to download the cert via the command line. Secret (Cert) Show Details Secret (Cert) ID Cert ID Ingress APPS \u00b6 IBM_SECRET_CERT_ID_INGRESS_APPS Each Cert you create has a unique ID, we will use that ID to download the cert via the command line. Secret (Cert) Show Details Secret (Cert) ID There is one important fact about the apps cert. It needs to be a wild card certs *.apps.${CLUSTER_NAME}.${BASE_DOMAIN} Apps Cert Common Name Execution \u00b6 Cluster Install Time \u00b6 If you have your required Environment variables in your file, during the cluster install, it will pick them up and download the certs to the correct location. /data/daffy/certs/${CLUSTER_NAME} Info If you are missing any of the variables, the cluster install process will continue and not download your certs, no errors or warnings. You can run as post cluster install step. Post Cluster Install \u00b6 If you already have a cluster up and running, you can run the build.sh post flag to install download your certs. Because this is explicit call to download certs, it will precheck to see all of your variables are there. If not it will give you error and not continue. /data/daffy/ocp/build.sh <ENVIRONMENT_NAME> --downloadIBMCloudIngressCerts Useful Links \u00b6 Getting started with Secrets Manager IBM Cloud Secrets Manger cli","title":"IBM Cloud Secrets Manager"},{"location":"Security/IBMSecretsManager/#ibm-cloud-secrets-manager","text":"","title":"IBM Cloud Secrets Manager"},{"location":"Security/IBMSecretsManager/#assumptions","text":"With IBM Cloud Secrets Manager, you can create, lease, and centrally manage secrets that are used in IBM Cloud services or your custom-built applications. Secrets are stored in a dedicated instance of Secrets Manager, built on open source HashiCorp Vault. For daffy to be able to download your certs from IBM Secrets Manager, we just need a few variables. Below are the variables you will need and how to obtain them from the IBM Cloud Web Portal IBM . Info We will not show you how to setup IBM Secrets Manager, Cert Authority or request your certs. Please follow the standard IBM Doc listed in the useful links below.","title":"Assumptions"},{"location":"Security/IBMSecretsManager/#environment-variables","text":"Variable Name Info Required IBM_SECRET_CERT_API_KEY The API key that can access your Secrets Yes(Will Prompt at runtime if the other three are present) IBM_SECRET_SERVICE_API_URL The host name for your Secret Service Yes IBM_SECRET_CERT_ID_INGRESS_API The ID for your API Cert Yes IBM_SECRET_CERT_ID_INGRESS_APPS The ID for your APPS Cert Yes","title":"Environment variables"},{"location":"Security/IBMSecretsManager/#building-certs","text":"When you build your certs, you have two options. You can build a cert for each URL (*.apps & api ) Apps Cert Common Name - *.apps.${CLUSTER}.${BASE_DOMAIN} API Cert Common Name - api.${CLUSTER}.${BASE_DOMAIN} You can build a single cert with both URLs as alt names of the cert Common Name - ${CLUSTER}.${BASE_DOMAIN} Alt Names - ${CLUSTER}.${BASE_DOMAIN}, *.apps .${CLUSTER}.${BASE_DOMAIN}, api .${CLUSTER}.${BASE_DOMAIN} Info If you build one cert, you would just use the same cert ID for both apps and api values in env file","title":"Building Certs"},{"location":"Security/IBMSecretsManager/#find-your-values","text":"","title":"Find Your Values"},{"location":"Security/IBMSecretsManager/#cert-api-key","text":"IBM_SECRET_CERT_API_KEY In your IBM Cloud account, you need to create/use an API key that has authority to access your secrets. IBM Cloud API Keys Create Key","title":"Cert API Key"},{"location":"Security/IBMSecretsManager/#service-api-url","text":"IBM_SECRET_SERVICE_API_URL This is the Service API Url for your Secretes Manger Instance Get Service URL","title":"Service API Url"},{"location":"Security/IBMSecretsManager/#cert-id-ingress-api","text":"IBM_SECRET_CERT_ID_INGRESS_API Each Cert you create has a unique ID, we will use that ID to download the cert via the command line. Secret (Cert) Show Details Secret (Cert) ID","title":"Cert ID Ingress API"},{"location":"Security/IBMSecretsManager/#cert-id-ingress-apps","text":"IBM_SECRET_CERT_ID_INGRESS_APPS Each Cert you create has a unique ID, we will use that ID to download the cert via the command line. Secret (Cert) Show Details Secret (Cert) ID There is one important fact about the apps cert. It needs to be a wild card certs *.apps.${CLUSTER_NAME}.${BASE_DOMAIN} Apps Cert Common Name","title":"Cert ID Ingress APPS"},{"location":"Security/IBMSecretsManager/#execution","text":"","title":"Execution"},{"location":"Security/IBMSecretsManager/#cluster-install-time","text":"If you have your required Environment variables in your file, during the cluster install, it will pick them up and download the certs to the correct location. /data/daffy/certs/${CLUSTER_NAME} Info If you are missing any of the variables, the cluster install process will continue and not download your certs, no errors or warnings. You can run as post cluster install step.","title":"Cluster Install Time"},{"location":"Security/IBMSecretsManager/#post-cluster-install","text":"If you already have a cluster up and running, you can run the build.sh post flag to install download your certs. Because this is explicit call to download certs, it will precheck to see all of your variables are there. If not it will give you error and not continue. /data/daffy/ocp/build.sh <ENVIRONMENT_NAME> --downloadIBMCloudIngressCerts","title":"Post Cluster Install"},{"location":"Security/IBMSecretsManager/#useful-links","text":"Getting started with Secrets Manager IBM Cloud Secrets Manger cli","title":"Useful Links"},{"location":"Security/SSLCertificates/","text":"document.title = \"SSL Certificates\"; SSL Certificates \u00b6 Cluster \u00b6 The cluster Ingress Operator makes it possible for external clients to access your service by deploying and managing one or more HAProxy-based Ingress Controllers to handle routing. You can use the Ingress Operator to route traffic by specifying OpenShift Container Platform Route and Kubernetes Ingress resources. Configurations within the Ingress Controller, such as the ability to define endpointPublishingStrategy type and internal load balancing, provide ways to publish Ingress Controller endpoints. By default Openshift will create self-signed, untrusted certs in the cluster. But if you want to install a trusted cert from a Trusted Certificate Authority(CA), this will allow you to do that. Daffy supports any valid cert from any Certificate Authority(CA), public or private. Just needs to be in the PEM format. For ingress there are two major urls. *.apps.${CLUSTER_NAME}.${BASE_DOMAIN} api.${CLUSTER_NAME}.${BASE_DOMAIN} Assumptions \u00b6 Must have valid Certificate in PEM format(apps and api) Must have private key from the cert(apps and api) Must have trust chain cert form the CA that issued the cert( apps only) The apps Ingress cert must have wild card Common Name *.apps.yourcluster.acme.com When you build your certs, you have two options. You can build a cert for each URL (*.apps & api ) Apps Cert Common Name - *.apps.${CLUSTER}.${BASE_DOMAIN} API Cert Common Name - api.${CLUSTER}.${BASE_DOMAIN} You can build a single cert with both URLs as alt names of the cert Common Name - ${CLUSTER}.${BASE_DOMAIN} Alt Names - ${CLUSTER}.${BASE_DOMAIN}, *.apps .${CLUSTER}.${BASE_DOMAIN}, api .${CLUSTER}.${BASE_DOMAIN} Info If you build one cert, you would just duplicate the cert/key files with each name for apps and api Location of certs \u00b6 Apps ingress When the daffy process runs, it will look in the certs folder for your certs. If it finds all three certs, it will apply them to the cluster. /data/daffy/certs/${CLUSTER_NAME}/ apps .${CLUSTER_NAME}.${BASE_DOMAIN} .crt /data/daffy/certs/${CLUSTER_NAME}/ apps .${CLUSTER_NAME}.${BASE_DOMAIN} .key /data/daffy/certs/${CLUSTER_NAME}/${OCP_TRUSTE_CA_FILE} .pem API ingress When the daffy process runs, it will look in the certs folder for your certs. If it finds both certs, it will apply them to the cluster. /data/daffy/certs/${CLUSTER_NAME}/ api .${CLUSTER_NAME}.${BASE_DOMAIN} .crt /data/daffy/certs/${CLUSTER_NAME}/ api .${CLUSTER_NAME}.${BASE_DOMAIN} .key Environment variables \u00b6 Variable Name Info Default value Required OCP_TRUSTE_CA_NAME The display name of your CA Cert in OpenShift lets-encrypt No OCP_TRUSTE_CA_FILE The file name of your CA Cert lets-encrypt.pem No Execution \u00b6 Cluster Install Time \u00b6 If you have your required certs in the location, during the cluster install, it will pick them up and install them after the cluster has been setup and running. Info If only some of your certs are there or none, the cluster install process will continue, no errors or warnings. Post Cluster Install \u00b6 If you already have a cluster up and running, you can run the build.sh post flag to install your certs. Because this is explicit call to install certs, it will precheck to see all of your certs are there and correct location. If not it will give you error and not continue. You can run as post cluster install step. /data/daffy/ocp/build.sh <ENVIRONMENT_NAME> --updateIngressCerts CP4BA \u00b6 There are two main ingress access points for CP4BA. cp-console(icp-management-ingress) cpd(ibm-nginx-svc). By default, these are setup with self-signed certs. If you want to replace them with trusted certs from any major CA, this process will allow that. For ingress there is a base urls. *.apps.${CLUSTER_NAME}.${BASE_DOMAIN} Assumptions \u00b6 Must have valid Certificate in PEM format Must have private key from the cert Must have trust chain cert form the CA that issued the cert The apps Ingress cert must have wild card Common Name *.apps.${CLUSTER_NAME}.${BASE_DOMAIN} Location of certs \u00b6 CP4BA ingress When the daffy process runs, it will look in the certs folder for your certs. If it finds all three certs, it will apply them to the cluster. /data/daffy/certs/${CLUSTER_NAME}/ apps .${CLUSTER_NAME}.${BASE_DOMAIN} .crt /data/daffy/certs/${CLUSTER_NAME}/ apps .${CLUSTER_NAME}.${BASE_DOMAIN} .key /data/daffy/certs/${CLUSTER_NAME}/${OCP_TRUSTE_CA_FILE} .pem Environment variables \u00b6 Variable Name Info Default value Required OCP_TRUSTE_CA_NAME The display name of your CA Cert in OpenShift lets-encrypt No OCP_TRUSTE_CA_FILE The file name of your CA Cert lets-encrypt.pem No Execution \u00b6 Post CP4BA Install Time \u00b6 For you to run this, the cloud pak must be up and running. This process can only updated existing pods and secrets. /data/daffy/cp4ba/build.sh <ENVIRONMENT_NAME> --updateIngressCerts Useful Links \u00b6 PEM Certificate OpenShift Ingress Api OpenShift Ingress Apps Common Services Ingress CP4BA Services Ingress","title":"SSL Certificates"},{"location":"Security/SSLCertificates/#ssl-certificates","text":"","title":"SSL Certificates"},{"location":"Security/SSLCertificates/#cluster","text":"The cluster Ingress Operator makes it possible for external clients to access your service by deploying and managing one or more HAProxy-based Ingress Controllers to handle routing. You can use the Ingress Operator to route traffic by specifying OpenShift Container Platform Route and Kubernetes Ingress resources. Configurations within the Ingress Controller, such as the ability to define endpointPublishingStrategy type and internal load balancing, provide ways to publish Ingress Controller endpoints. By default Openshift will create self-signed, untrusted certs in the cluster. But if you want to install a trusted cert from a Trusted Certificate Authority(CA), this will allow you to do that. Daffy supports any valid cert from any Certificate Authority(CA), public or private. Just needs to be in the PEM format. For ingress there are two major urls. *.apps.${CLUSTER_NAME}.${BASE_DOMAIN} api.${CLUSTER_NAME}.${BASE_DOMAIN}","title":"Cluster"},{"location":"Security/SSLCertificates/#assumptions","text":"Must have valid Certificate in PEM format(apps and api) Must have private key from the cert(apps and api) Must have trust chain cert form the CA that issued the cert( apps only) The apps Ingress cert must have wild card Common Name *.apps.yourcluster.acme.com When you build your certs, you have two options. You can build a cert for each URL (*.apps & api ) Apps Cert Common Name - *.apps.${CLUSTER}.${BASE_DOMAIN} API Cert Common Name - api.${CLUSTER}.${BASE_DOMAIN} You can build a single cert with both URLs as alt names of the cert Common Name - ${CLUSTER}.${BASE_DOMAIN} Alt Names - ${CLUSTER}.${BASE_DOMAIN}, *.apps .${CLUSTER}.${BASE_DOMAIN}, api .${CLUSTER}.${BASE_DOMAIN} Info If you build one cert, you would just duplicate the cert/key files with each name for apps and api","title":"Assumptions"},{"location":"Security/SSLCertificates/#location-of-certs","text":"Apps ingress When the daffy process runs, it will look in the certs folder for your certs. If it finds all three certs, it will apply them to the cluster. /data/daffy/certs/${CLUSTER_NAME}/ apps .${CLUSTER_NAME}.${BASE_DOMAIN} .crt /data/daffy/certs/${CLUSTER_NAME}/ apps .${CLUSTER_NAME}.${BASE_DOMAIN} .key /data/daffy/certs/${CLUSTER_NAME}/${OCP_TRUSTE_CA_FILE} .pem API ingress When the daffy process runs, it will look in the certs folder for your certs. If it finds both certs, it will apply them to the cluster. /data/daffy/certs/${CLUSTER_NAME}/ api .${CLUSTER_NAME}.${BASE_DOMAIN} .crt /data/daffy/certs/${CLUSTER_NAME}/ api .${CLUSTER_NAME}.${BASE_DOMAIN} .key","title":"Location of certs"},{"location":"Security/SSLCertificates/#environment-variables","text":"Variable Name Info Default value Required OCP_TRUSTE_CA_NAME The display name of your CA Cert in OpenShift lets-encrypt No OCP_TRUSTE_CA_FILE The file name of your CA Cert lets-encrypt.pem No","title":"Environment variables"},{"location":"Security/SSLCertificates/#execution","text":"","title":"Execution"},{"location":"Security/SSLCertificates/#cluster-install-time","text":"If you have your required certs in the location, during the cluster install, it will pick them up and install them after the cluster has been setup and running. Info If only some of your certs are there or none, the cluster install process will continue, no errors or warnings.","title":"Cluster Install Time"},{"location":"Security/SSLCertificates/#post-cluster-install","text":"If you already have a cluster up and running, you can run the build.sh post flag to install your certs. Because this is explicit call to install certs, it will precheck to see all of your certs are there and correct location. If not it will give you error and not continue. You can run as post cluster install step. /data/daffy/ocp/build.sh <ENVIRONMENT_NAME> --updateIngressCerts","title":"Post Cluster Install"},{"location":"Security/SSLCertificates/#cp4ba","text":"There are two main ingress access points for CP4BA. cp-console(icp-management-ingress) cpd(ibm-nginx-svc). By default, these are setup with self-signed certs. If you want to replace them with trusted certs from any major CA, this process will allow that. For ingress there is a base urls. *.apps.${CLUSTER_NAME}.${BASE_DOMAIN}","title":"CP4BA"},{"location":"Security/SSLCertificates/#assumptions_1","text":"Must have valid Certificate in PEM format Must have private key from the cert Must have trust chain cert form the CA that issued the cert The apps Ingress cert must have wild card Common Name *.apps.${CLUSTER_NAME}.${BASE_DOMAIN}","title":"Assumptions"},{"location":"Security/SSLCertificates/#location-of-certs_1","text":"CP4BA ingress When the daffy process runs, it will look in the certs folder for your certs. If it finds all three certs, it will apply them to the cluster. /data/daffy/certs/${CLUSTER_NAME}/ apps .${CLUSTER_NAME}.${BASE_DOMAIN} .crt /data/daffy/certs/${CLUSTER_NAME}/ apps .${CLUSTER_NAME}.${BASE_DOMAIN} .key /data/daffy/certs/${CLUSTER_NAME}/${OCP_TRUSTE_CA_FILE} .pem","title":"Location of certs"},{"location":"Security/SSLCertificates/#environment-variables_1","text":"Variable Name Info Default value Required OCP_TRUSTE_CA_NAME The display name of your CA Cert in OpenShift lets-encrypt No OCP_TRUSTE_CA_FILE The file name of your CA Cert lets-encrypt.pem No","title":"Environment variables"},{"location":"Security/SSLCertificates/#execution_1","text":"","title":"Execution"},{"location":"Security/SSLCertificates/#post-cp4ba-install-time","text":"For you to run this, the cloud pak must be up and running. This process can only updated existing pods and secrets. /data/daffy/cp4ba/build.sh <ENVIRONMENT_NAME> --updateIngressCerts","title":"Post CP4BA Install Time"},{"location":"Security/SSLCertificates/#useful-links","text":"PEM Certificate OpenShift Ingress Api OpenShift Ingress Apps Common Services Ingress CP4BA Services Ingress","title":"Useful Links"},{"location":"Supporting-Software/","text":"document.title = \"Supporting Software\"; Supporting software Instructions on other software you can install Bastion IBM DB2 IBM LDAP Turbonomics Instana","title":"Index"},{"location":"Supporting-Software/Bastion/","text":"document.title = \"Supporting Software - Bastion\"; What is a bastion server? \u00b6 A bastion host is a server whose purpose is to provide access to a private network from an external network, such as the Internet. Because of its exposure to potential attack, a bastion host must minimize the chances of penetration. Openshift uses a bastion to help create a running cluster. A bastion can be reused for multiple clusters. In some scenarios for POC purposes such as User Provisioned Infrastructure (UPI), the bastion can be used as the proxy to the cluster. Bastion servers can be installed anywhere. This guide assumes the bastion server is Ubuntu 20.04 Minimal and will be in the IBM Cloud. Requirements for completing this task is to have an IBMid, an IBM cloud account, and a local SSH key. For more information, go to Daffy Prerequisites. Important Regardless of where your bastion is hosted, if it is accessable on the public internet it is HIGHLY recommended that you disable root user password authentication and ONLY allow SSH Key login. Please refer to the instructions here for more information on how to do that. Detailed below are the instructions to build your own bastion to do an IPI or MSP install. IBM Cloud \u00b6 First, open a web browser and go to http://cloud.ibm.com Enter your id and click 'Continue' Once logged in, select 'Catalog' in the top menu bar Once the Catalog is loaded, select 'Compute' or search the catalog for Virtual Servers. Select 'Virtual Server for Classic' Alternative: Skip step 3 and search for \"virtual server\", choosing \"virtual server for classic\". Both options achieve the same thing. Fill out the details. (Public, hostname can be anything, and so can domain \u2013 feel free to leave what is there originally for your domain). Choose your billing method based on needs to be either Hourly or Monthly (~$40/mo.) and choose a Location. Scroll down and fill out the remainder of the information. Choose a server type and select your SSH key so you can login. Finally, make sure you have the Ubuntu 20.04 Operating System selected. Note: You can use any other available tool to create a key if needed Click the agreement on the right-hand pane and select 'Create' This will take you to a device page. You can search for your bastion you have created. Once your server is done provisioning and has a start date, you can login to it via Termius using the Public IP address. **If connecting to a VPN to connect to the network, you will use the Private IP address, but Public will be used more frequently. Create a new host in Termius: use your SSH key as the password , use root as the username , input the Public IP Address from the device list as your host's address, and create a label. **If you don't use a SSH Key, you can go into the details of the bastion you created by double clicking on it and going to the passwords section. This password will not show until provisioning is complete. Once you have connected your bastion to Termius , install Daffy to the terminal of your newly created host. IBM Technology Zone \u00b6 An alternative to creating a bastion using a paid IBM Cloud account is to use IBM Technology Zone. There are three options with TechZone Warning For internal IBM or Business partner use only Use this link to navigate to IBM Technology Zone: https://techzone.ibm.com/collection/base-images Scroll down to environments, and choose IBM Virtual Server Instance (Classic) From there, complete your reservation. Make sure to fill out items 1 \u2013 4 , leaving the other fields blank .","title":"Bastion"},{"location":"Supporting-Software/Bastion/#what-is-a-bastion-server","text":"A bastion host is a server whose purpose is to provide access to a private network from an external network, such as the Internet. Because of its exposure to potential attack, a bastion host must minimize the chances of penetration. Openshift uses a bastion to help create a running cluster. A bastion can be reused for multiple clusters. In some scenarios for POC purposes such as User Provisioned Infrastructure (UPI), the bastion can be used as the proxy to the cluster. Bastion servers can be installed anywhere. This guide assumes the bastion server is Ubuntu 20.04 Minimal and will be in the IBM Cloud. Requirements for completing this task is to have an IBMid, an IBM cloud account, and a local SSH key. For more information, go to Daffy Prerequisites. Important Regardless of where your bastion is hosted, if it is accessable on the public internet it is HIGHLY recommended that you disable root user password authentication and ONLY allow SSH Key login. Please refer to the instructions here for more information on how to do that. Detailed below are the instructions to build your own bastion to do an IPI or MSP install.","title":"What is a bastion server?"},{"location":"Supporting-Software/Bastion/#ibm-cloud","text":"First, open a web browser and go to http://cloud.ibm.com Enter your id and click 'Continue' Once logged in, select 'Catalog' in the top menu bar Once the Catalog is loaded, select 'Compute' or search the catalog for Virtual Servers. Select 'Virtual Server for Classic' Alternative: Skip step 3 and search for \"virtual server\", choosing \"virtual server for classic\". Both options achieve the same thing. Fill out the details. (Public, hostname can be anything, and so can domain \u2013 feel free to leave what is there originally for your domain). Choose your billing method based on needs to be either Hourly or Monthly (~$40/mo.) and choose a Location. Scroll down and fill out the remainder of the information. Choose a server type and select your SSH key so you can login. Finally, make sure you have the Ubuntu 20.04 Operating System selected. Note: You can use any other available tool to create a key if needed Click the agreement on the right-hand pane and select 'Create' This will take you to a device page. You can search for your bastion you have created. Once your server is done provisioning and has a start date, you can login to it via Termius using the Public IP address. **If connecting to a VPN to connect to the network, you will use the Private IP address, but Public will be used more frequently. Create a new host in Termius: use your SSH key as the password , use root as the username , input the Public IP Address from the device list as your host's address, and create a label. **If you don't use a SSH Key, you can go into the details of the bastion you created by double clicking on it and going to the passwords section. This password will not show until provisioning is complete. Once you have connected your bastion to Termius , install Daffy to the terminal of your newly created host.","title":"IBM Cloud"},{"location":"Supporting-Software/Bastion/#ibm-technology-zone","text":"An alternative to creating a bastion using a paid IBM Cloud account is to use IBM Technology Zone. There are three options with TechZone Warning For internal IBM or Business partner use only Use this link to navigate to IBM Technology Zone: https://techzone.ibm.com/collection/base-images Scroll down to environments, and choose IBM Virtual Server Instance (Classic) From there, complete your reservation. Make sure to fill out items 1 \u2013 4 , leaving the other fields blank .","title":"IBM Technology Zone"},{"location":"Supporting-Software/DB2/","text":"document.title = \"Supporting Software - DB2\"; With the Daffy system you can now install DB2 on a Linux server. RHEL 8.x or Ubuntu 20.0.4. You will need to size the server based on your needs for the Database. Below are the steps you would need to get the software, install it and check on the status of the install. Important If you are looking to install DB2 to support IDS(IBM LDAP), you need to have RHEL 8.X. IDS will only run on RHEL. You can not have UBuntu for IDS. IDS does not support it, its not a daffy issue. Obtain Software \u00b6 Before you install DB2, you will need to download the DB2 Software and place the binary's on your Linux server. 1) Customer can download the software via the Passport Advantage site. 2) Tech Sellers can download it from IBM Internal DSW Downloads . You will need to search for the part number CC1U0ML , this is the DB2 Software that will be used during the install. Once you get the software, you will need to upload it to the Linux server where you plan to install it. By default, it will need to be in the following location: /data/software/db2 You can override this base location ( /data/software ) by overriding it in your environment file. SOFTWARE_INSTALLS_DIR= Install DB2 \u00b6 Installing DB2 needs one new values to your environment file (< ENVIRONMENT_NAME >-env.sh). DB2_VERSION With this value, the daffy engine will be able to install the version of DB2. Valid Options: DB2_VERSION: 11.5 DB2_VERSION=\"11.5\" Once you have the requires values in your environment file and the sofware was placed on the Linux server, you are ready to kick off the install. Below is the command to kick off the install. The process should take approx 10 min to install. /data/daffy/db2/build.sh <ENVIRONMENT_NAME> Help \u00b6 There are a few options that the Daffy DB2 installer has to help with install and post install. To see the options you can run the following command: /data/daffy/db2/build.sh <ENVIRONMENT_NAME> --help If you need to test is you have correct info to install DB2 on this host, You can run the following command /data/daffy/db2/build.sh <ENVIRONMENT_NAME> --precheck If you need to display connect info for DB2, user name password port, etc. You can run the following command /data/daffy/db2/build.sh <ENVIRONMENT_NAME> --console","title":"DB2"},{"location":"Supporting-Software/DB2/#obtain-software","text":"Before you install DB2, you will need to download the DB2 Software and place the binary's on your Linux server. 1) Customer can download the software via the Passport Advantage site. 2) Tech Sellers can download it from IBM Internal DSW Downloads . You will need to search for the part number CC1U0ML , this is the DB2 Software that will be used during the install. Once you get the software, you will need to upload it to the Linux server where you plan to install it. By default, it will need to be in the following location: /data/software/db2 You can override this base location ( /data/software ) by overriding it in your environment file. SOFTWARE_INSTALLS_DIR=","title":"Obtain Software"},{"location":"Supporting-Software/DB2/#install-db2","text":"Installing DB2 needs one new values to your environment file (< ENVIRONMENT_NAME >-env.sh). DB2_VERSION With this value, the daffy engine will be able to install the version of DB2. Valid Options: DB2_VERSION: 11.5 DB2_VERSION=\"11.5\" Once you have the requires values in your environment file and the sofware was placed on the Linux server, you are ready to kick off the install. Below is the command to kick off the install. The process should take approx 10 min to install. /data/daffy/db2/build.sh <ENVIRONMENT_NAME>","title":"Install DB2"},{"location":"Supporting-Software/DB2/#help","text":"There are a few options that the Daffy DB2 installer has to help with install and post install. To see the options you can run the following command: /data/daffy/db2/build.sh <ENVIRONMENT_NAME> --help If you need to test is you have correct info to install DB2 on this host, You can run the following command /data/daffy/db2/build.sh <ENVIRONMENT_NAME> --precheck If you need to display connect info for DB2, user name password port, etc. You can run the following command /data/daffy/db2/build.sh <ENVIRONMENT_NAME> --console","title":"Help"},{"location":"Supporting-Software/IBM-LDAP/","text":"document.title = \"Supporting Software - IBM LDAP\"; With the Daffy system you can now install IBM LDAP server on a Linux server. IDS only supports RHEL 8.x. You will need to size the server based on your needs for the Database. Below are the steps you would need to get the software, install it and check on the status of the install. Important To install LDAP, you need to have DB2 installed locally. Instructions here Obtain Software \u00b6 Before you install IBM LDAP you will need to download the IBM LDAP Software and place the binary's on your Linux server. 1) Customer can download the software via the Passport Advantage site. 2) Tech Sellers can download it from IBM Internal DSW Downloads . You will need to search for two part numbers CN487ML and CN4VJML , this is the IBM LDAP Software that will be used during the install. Once you get the software, you will need to upload it to the Linux server where you plan to install it. By default, it will need to be in the following location: /data/software/ldap You can override this base location (/data/software) by overriding it in your environment file. SOFTWARE_INSTALLS_DIR= Install LDAP \u00b6 Installing LDAP needs one new values to your environment file (< ENVIRONMENT_NAME >-env.sh). IDS_VERSION= With this value, the daffy engine will be able to install the version of IDS(LDAP). ** The IBM LDAP Software is only supported on IBM RHEL. Valid Options: IDS_VERSION 6.4 IDS_VERSION=\"6.4\" /data/daffy/ldap/build.sh <ENVIRONMENT_NAME> Help \u00b6 There are a few options that the Daffy IBM LDAP installer has to help with install and post install. To see the options you can run the following command: /data/daffy/ldap/build.sh <ENVIRONMENT_NAME> --help If you need to test is you have correct info to install IBM LDAP on this host, You can run the following command /data/daffy/ldap/build.sh <ENVIRONMENT_NAME> --precheck If you need to display connect info for IBM LDAP , user name password port, etc. You can run the following command /data/daffy/ldap/build.sh <ENVIRONMENT_NAME> --console","title":"IBM LDAP"},{"location":"Supporting-Software/IBM-LDAP/#obtain-software","text":"Before you install IBM LDAP you will need to download the IBM LDAP Software and place the binary's on your Linux server. 1) Customer can download the software via the Passport Advantage site. 2) Tech Sellers can download it from IBM Internal DSW Downloads . You will need to search for two part numbers CN487ML and CN4VJML , this is the IBM LDAP Software that will be used during the install. Once you get the software, you will need to upload it to the Linux server where you plan to install it. By default, it will need to be in the following location: /data/software/ldap You can override this base location (/data/software) by overriding it in your environment file. SOFTWARE_INSTALLS_DIR=","title":"Obtain Software"},{"location":"Supporting-Software/IBM-LDAP/#install-ldap","text":"Installing LDAP needs one new values to your environment file (< ENVIRONMENT_NAME >-env.sh). IDS_VERSION= With this value, the daffy engine will be able to install the version of IDS(LDAP). ** The IBM LDAP Software is only supported on IBM RHEL. Valid Options: IDS_VERSION 6.4 IDS_VERSION=\"6.4\" /data/daffy/ldap/build.sh <ENVIRONMENT_NAME>","title":"Install LDAP"},{"location":"Supporting-Software/IBM-LDAP/#help","text":"There are a few options that the Daffy IBM LDAP installer has to help with install and post install. To see the options you can run the following command: /data/daffy/ldap/build.sh <ENVIRONMENT_NAME> --help If you need to test is you have correct info to install IBM LDAP on this host, You can run the following command /data/daffy/ldap/build.sh <ENVIRONMENT_NAME> --precheck If you need to display connect info for IBM LDAP , user name password port, etc. You can run the following command /data/daffy/ldap/build.sh <ENVIRONMENT_NAME> --console","title":"Help"},{"location":"Supporting-Software/Instana/","text":"document.title = \"Supporting Software - Instana\"; Obtain Software \u00b6 Obtain Key \u00b6 Install \u00b6 Help \u00b6","title":"Instana"},{"location":"Supporting-Software/Instana/#obtain-software","text":"","title":"Obtain Software"},{"location":"Supporting-Software/Instana/#obtain-key","text":"","title":"Obtain Key"},{"location":"Supporting-Software/Instana/#install","text":"","title":"Install"},{"location":"Supporting-Software/Instana/#help","text":"","title":"Help"},{"location":"Supporting-Software/Turbonomics/","text":"document.title = \"Supporting Software - Turbonomics\"; Turbonomic \u00b6 Attention You will be required to provide a license key to fully enable the Turbonomic Platform. When the platform installation is complete, you will be presented with a URL where you will need to configure the administrator password and provide a valid Turbonomic license key. Info The default admin username for the Turbonomic Platform is: administrator and you will have the ability to set the password when you configure the server after install. KubeTurbo is the Turbo Metric Collector designed to send metrics and usage data from the Kubernetes environment to a Turbonomic Platform instance. Installing KubeTurbo will require you to provide the Turbonomic Platform topology processor URL and the administrator password. Turbonomic Platform \u00b6 Turbonomic is an Application Resource Management (ARM) tool used to identify and automate critical actions that proactively deliver the most efficient use of compute, storage and network resources to your apps at every layer of the stack. Continuously, in real time and without human intervention. REQUIRED ENVIRONMENT VARIABLES TURBO_PLATFORM_VERSION = \"8.5.4\" Kubeturbo \u00b6 Kubeturbo is an optional service that you can deploy to OpenShift. This will collect metrics from the OpenShift environment and send it to the Turbo Platform for display in the dashboard. You may configure the Kubeturbo to collect metrics and send data to an external Turbo Plaftorm, by overriding the values below. OPTIONAL ENVIRONMENT VARIABLES Info TURBO_PLATFORM_URL will be the \"topology-processor\" OpenShift ROUTE URL (If using the Turbo Platform Deployed by Daffy). If the Turbo Platform was deployed without DAFFY, the URL may be the nginx endpoint. TURBO_PLATFORM_URL = \"https://topology-processor-turbo.apps.yourdomain.net\" Info TURBO_PLATFORM_USERNAME Default value is: \"administrator\" TURBO_PLATFORM_USERNAME = \"administrator\" Info TURBO_KUBE_CLUSTER_NAME is the label of your cluster. This will be defaulted to the cluster name you specified in the env file. TURBO_KUBE_CLUSTER_NAME = \"gamma03\" Executing Scripts \u00b6 Turbo Platform KubeTurbo Deploy Turbo Platform /data/daffy/turbo/build.sh <env-prefix> Deploy KubeTurbo /data/daffy/turbo/service.sh <env-prefix>","title":"Turbonomics"},{"location":"Supporting-Software/Turbonomics/#turbonomic","text":"Attention You will be required to provide a license key to fully enable the Turbonomic Platform. When the platform installation is complete, you will be presented with a URL where you will need to configure the administrator password and provide a valid Turbonomic license key. Info The default admin username for the Turbonomic Platform is: administrator and you will have the ability to set the password when you configure the server after install. KubeTurbo is the Turbo Metric Collector designed to send metrics and usage data from the Kubernetes environment to a Turbonomic Platform instance. Installing KubeTurbo will require you to provide the Turbonomic Platform topology processor URL and the administrator password.","title":"Turbonomic"},{"location":"Supporting-Software/Turbonomics/#turbonomic-platform","text":"Turbonomic is an Application Resource Management (ARM) tool used to identify and automate critical actions that proactively deliver the most efficient use of compute, storage and network resources to your apps at every layer of the stack. Continuously, in real time and without human intervention. REQUIRED ENVIRONMENT VARIABLES TURBO_PLATFORM_VERSION = \"8.5.4\"","title":"Turbonomic Platform"},{"location":"Supporting-Software/Turbonomics/#kubeturbo","text":"Kubeturbo is an optional service that you can deploy to OpenShift. This will collect metrics from the OpenShift environment and send it to the Turbo Platform for display in the dashboard. You may configure the Kubeturbo to collect metrics and send data to an external Turbo Plaftorm, by overriding the values below. OPTIONAL ENVIRONMENT VARIABLES Info TURBO_PLATFORM_URL will be the \"topology-processor\" OpenShift ROUTE URL (If using the Turbo Platform Deployed by Daffy). If the Turbo Platform was deployed without DAFFY, the URL may be the nginx endpoint. TURBO_PLATFORM_URL = \"https://topology-processor-turbo.apps.yourdomain.net\" Info TURBO_PLATFORM_USERNAME Default value is: \"administrator\" TURBO_PLATFORM_USERNAME = \"administrator\" Info TURBO_KUBE_CLUSTER_NAME is the label of your cluster. This will be defaulted to the cluster name you specified in the env file. TURBO_KUBE_CLUSTER_NAME = \"gamma03\"","title":"Kubeturbo"},{"location":"Supporting-Software/Turbonomics/#executing-scripts","text":"Turbo Platform KubeTurbo Deploy Turbo Platform /data/daffy/turbo/build.sh <env-prefix> Deploy KubeTurbo /data/daffy/turbo/service.sh <env-prefix>","title":"Executing Scripts"},{"location":"Team-Notes/","text":"document.title = \"Team Notes\"; MKDOCS Install & Configuration This site uses MKDocs for the publishing of the \"GitHub Pages\" site You MUST install both MKDocs and the Material Theme. Here is how you setup your machine to edit the documentation hosted on the Github Pages Site. Details of these steps are below. Outline of the steps you will take Install MKdocs Install the MKdocs Theme (Material) Get a Personal Access Token from GitHub Clone this repository Make your changes (Feel free to reach out to Dave Krier or Kyle Dawson for help on how to use MKDocs) Deploy your changes Install MKDocs \u00b6 Here is the installation documentation for MKDocs Install the MKDocs theme Install guidance is on the Material theme page. Material Theme - Quick Start Material Theme Documentation \u00b6 pip3 install mkdocs-material You can check the version of Material you currently have installed with this command. python3 -m pip list If you have any issues with the pip install, you may need to perform an uninstall and reinstall. pip3 uninstall mkdocs-material pip3 install mkdocs-material pip3 uninstall mkdocs pip3 install mkdocs Link to the Material Theme Documentation You need a \"Personal Access Token\" from Github. (This is super simple). \u00b6 Why do I need this? You need this because you need to be able to push changes back into the github repository. Here is a Github doc page you can look at for help if the steps below do not make sense. Login to Github. Click on your user icon in the upper right hand corner Select Settings --> Developer Settings --> Personal Access Tokens Generate a new token with only these prevliages Repo:Status Repo-Deployment Public-Repo Take note of the access key (It's the only time you will see it) Important You will use this Access Token as your password when using the gh-deploy mkdocs command. Clone this repo in Github. \u00b6 I assume you will all know how to do that!! MKDocs User Guide - Modifying the site \u00b6 MKDocs Documentation Deploying your changes. Navigate to your cloned repo directory in a terminal window. You must be inside the /daffydoc directory to run the following commands. Build the files using the build command mkdocs build Deploy the newly built files to the Github Pages site Note This is where you will use the access token from Github. Use the access token instead of your password. mkdocs gh-deploy Update mkdocs \u00b6 pip3 install -U mkdocs","title":"Index"},{"location":"Team-Notes/#install-mkdocs","text":"Here is the installation documentation for MKDocs Install the MKDocs theme Install guidance is on the Material theme page. Material Theme - Quick Start","title":"Install MKDocs"},{"location":"Team-Notes/#material-theme-documentation","text":"pip3 install mkdocs-material You can check the version of Material you currently have installed with this command. python3 -m pip list If you have any issues with the pip install, you may need to perform an uninstall and reinstall. pip3 uninstall mkdocs-material pip3 install mkdocs-material pip3 uninstall mkdocs pip3 install mkdocs Link to the Material Theme Documentation","title":"Material Theme Documentation"},{"location":"Team-Notes/#you-need-a-personal-access-token-from-github-this-is-super-simple","text":"Why do I need this? You need this because you need to be able to push changes back into the github repository. Here is a Github doc page you can look at for help if the steps below do not make sense. Login to Github. Click on your user icon in the upper right hand corner Select Settings --> Developer Settings --> Personal Access Tokens Generate a new token with only these prevliages Repo:Status Repo-Deployment Public-Repo Take note of the access key (It's the only time you will see it) Important You will use this Access Token as your password when using the gh-deploy mkdocs command.","title":"You need a \"Personal Access Token\" from Github. (This is super simple)."},{"location":"Team-Notes/#clone-this-repo-in-github","text":"I assume you will all know how to do that!!","title":"Clone this repo in Github."},{"location":"Team-Notes/#mkdocs-user-guide-modifying-the-site","text":"MKDocs Documentation Deploying your changes. Navigate to your cloned repo directory in a terminal window. You must be inside the /daffydoc directory to run the following commands. Build the files using the build command mkdocs build Deploy the newly built files to the Github Pages site Note This is where you will use the access token from Github. Use the access token instead of your password. mkdocs gh-deploy","title":"MKDocs User Guide - Modifying the site"},{"location":"Team-Notes/#update-mkdocs","text":"pip3 install -U mkdocs","title":"Update mkdocs"},{"location":"Tips-and-Tricks/Common-Commands/","text":"document.title = \"Common Commands\"; This page lists some of the most commonly used Daffy commands for installation, troubleshooting, and cleanup. Precheck Environment \u00b6 By adding --precheck to any of the following build/service commands in daffy. It will run all prechecks for that step and then exit without preforming the full process Info This will not work on the all-in-one command /data/daffy/build.sh --precheck Precheck OpenShift Build / data / daffy / ocp / build.sh < env - name > -- precheck And will work on all Cloud Pak commands as well: Precheck CP4D Build / data / daffy / cp4d / build.sh < env - name > -- precheck Precheck CP4D Service / data / daffy / cp4d / service.sh < env - name > -- precheck Delete Environment \u00b6 Run the following command to delete all resources created by Daffy, including but not limited to the OpenShift cluster, Cloud Paks, services, VM systems, and tools. /data/daffy/cleanup.sh <env-name> Daffy Versioning \u00b6 Upgrade \u00b6 Run the following command to upgrade Daffy to the newest release. cd /data ; /data/daffy/refresh.sh Downgrade \u00b6 Run the following command to install older version of Daffy. This will output a list of previous releases to select and install. cd /data ; /data/daffy/refresh.sh --list Example Output Warning If you already downgraded to an older version and want to install another older version, you must first upgrade to the latest release and then downgrade to the other version you want. Install Older \u00b6 If you want to install an older version from the begining, you can with this command: curl http://get.daffy-installer.com/download-scripts/daffy-init.sh | sudo -E bash -s -- v2023-01-11.tar Version name format Just supply the name of the tar version you want to install. Format v ${YEAR} -${MONTH} -${DAY} .tar Get Version \u00b6 Run the following command to view the current Daffy version. /data/daffy/version.sh OCP Build Help \u00b6 The OCP build process has several flags, to see all the flags, you can run the --help /data/daffy/ocp/build.sh <env_name> --help All Build Flags --precheck \"This will just do a quick precheck to see if the environment is ready for a build\" --tshirtSize \"This will display what the current TShirt size would be with this environment\" --createIBMCloudDNSEntries \"This will create your Public DNS enries in IBM Cloud\" --approveVCenterCert \"This will download VCenter certs and add to host trust store.\" --removeIBMCloudDNSEntries \"This will remove your Public DNS enries in IBM Cloud\" --displayOCPDNSRequirements \"This will display what DNS records that need to be created\" --displayOCPLoadBalanceRules \"This will display the Load Balnacer rules you will need\" --runOpenShiftInstallWaitBoot \"This will run openshift-install (wait-for boot)\" --approveCSR \"This will approve all pending CSR request\" --runOpenShiftInstallWaitInstall \"This will run openshift-install (wait-for install)\" --updateIngress \"This will get new IBM Cert and update the main ingress certs/secret\" --createOpenShiftContainerStorage \"This will create OpenShift Container Storage\" --createNFSServer \"This will create local NFS Server\" --createNFSDisk \"This will create local disk for NFS Server\" --configureLocalStorge \"This will create local stroage\" --createVMDashboard \"This will create the VM Dashboard Web UI\" --createImageRegistry \"This will create the OpenShift Image Registry\" --console \"This will display Admin Console Info\" --status \"This will display cluster info\" --installOpenShiftTools \"This will install oc, kubectl and openshift-install tools\" --displayVSpherePermissionsNeeded \"This will display all permissions needed for VSphere Install\" --validateReserverLookupDNS \"This will validate PTR records for UPI installs\" --applyFix \"This will apply a given fix to your Daffy env, pass the fix number to command\" --deleteAllFailedPods \"This will delete all pods in Failed state.\" --MACinstallOCPTools \"This will install oc and kubectl tools locally on your Mac\" --installRHACM \"This will install Red Hat Advanced Cluster Management\" --CephStatus \"This will show the status of ceph using ceph tools inside of OpenShift Storage\" --createAdminAccount \"This will create local htaccess admin account\" --help | --? | ? | -? | help | -help \"This help menu\" Examples To show OpenShift Console info, ID/Password and URL /data/daffy/ocp/build.sh <env_name> --console To show OpenShift status /data/daffy/ocp/build.sh <env_name> --status To show OpenShift Node sizes based on your environment file /data/daffy/ocp/build.sh <env_name> --tshirtSize ODF Existing Cluster \u00b6 If you have existing cluster, you can run the follow command to setup OpenShift Data Foundation on your existing cluster. /data/daffy/ocp/build.sh <env_name> --createOpenShiftContainerStorage Daffy Tools \u00b6 Run the following command to install other tools you might need. /data/daffy/tools.sh Tools Flags Daffy Tools --prepareHost This will run the prepareHost for daffy --mustGather This will run the mustGather for daffy --installOC This will install the oc command line tool --installOpenShiftInstall This will install the openshift-install command line tool --installAWS This will install the aws commandline tool --installAzure This will install the azure commandline tool --installGCloud This will install the gcloud commandline tool --installGOVC This will install the govc commandline tool(VMware) --installCloudCTL This will install the cloudctl for CP4D --installCP4DCloudCLI This will install the CP4D Cloud CLI utility --installCPDCTL This will install the Cloud Pak 4 Data (CPDCTL) utility\" Gather Logs \u00b6 Run the following command to package all Daffy logs into a tar file. When troubleshooting, please send the gathered logs at /tmp/daffy/daffy_mustgather.tar.gz to the Daffy team. /data/daffy/tools.sh --mustGather Security Cleanup \u00b6 Run the following command to clear all security (or individual) credentials. /data/daffy/security-cleanup.sh Additional Cleanup Flags --all This will cleanup all security including RH Pull Secret, SSH key, and IBM Entitlement keys --aws This will cleanup the aws security information --azure This will cleanup the azure security information --gcp This will cleanup the gcp credential information --ibm This will cleanup the IBM Entitlement Key and ibmcloud sesssion info --vsphere This will cleanup vsphere information --pullSecret This will cleanup your RedHat Pull Secret --ssh This will cleanup your local ssh key --ibm-ipi This will cleanup IBM API Key for use with IBM-ipi --roks This will cleanup IBM ROKS keys and login information --turbo This will cleanup your local ssh key --help|--?|?|-?|help|-help This help menu","title":"Common Commands"},{"location":"Tips-and-Tricks/Common-Commands/#precheck-environment","text":"By adding --precheck to any of the following build/service commands in daffy. It will run all prechecks for that step and then exit without preforming the full process Info This will not work on the all-in-one command /data/daffy/build.sh --precheck Precheck OpenShift Build / data / daffy / ocp / build.sh < env - name > -- precheck And will work on all Cloud Pak commands as well: Precheck CP4D Build / data / daffy / cp4d / build.sh < env - name > -- precheck Precheck CP4D Service / data / daffy / cp4d / service.sh < env - name > -- precheck","title":"Precheck Environment"},{"location":"Tips-and-Tricks/Common-Commands/#delete-environment","text":"Run the following command to delete all resources created by Daffy, including but not limited to the OpenShift cluster, Cloud Paks, services, VM systems, and tools. /data/daffy/cleanup.sh <env-name>","title":"Delete Environment"},{"location":"Tips-and-Tricks/Common-Commands/#daffy-versioning","text":"","title":"Daffy Versioning"},{"location":"Tips-and-Tricks/Common-Commands/#upgrade","text":"Run the following command to upgrade Daffy to the newest release. cd /data ; /data/daffy/refresh.sh","title":"Upgrade"},{"location":"Tips-and-Tricks/Common-Commands/#downgrade","text":"Run the following command to install older version of Daffy. This will output a list of previous releases to select and install. cd /data ; /data/daffy/refresh.sh --list Example Output Warning If you already downgraded to an older version and want to install another older version, you must first upgrade to the latest release and then downgrade to the other version you want.","title":"Downgrade"},{"location":"Tips-and-Tricks/Common-Commands/#install-older","text":"If you want to install an older version from the begining, you can with this command: curl http://get.daffy-installer.com/download-scripts/daffy-init.sh | sudo -E bash -s -- v2023-01-11.tar Version name format Just supply the name of the tar version you want to install. Format v ${YEAR} -${MONTH} -${DAY} .tar","title":"Install Older"},{"location":"Tips-and-Tricks/Common-Commands/#get-version","text":"Run the following command to view the current Daffy version. /data/daffy/version.sh","title":"Get Version"},{"location":"Tips-and-Tricks/Common-Commands/#ocp-build-help","text":"The OCP build process has several flags, to see all the flags, you can run the --help /data/daffy/ocp/build.sh <env_name> --help All Build Flags --precheck \"This will just do a quick precheck to see if the environment is ready for a build\" --tshirtSize \"This will display what the current TShirt size would be with this environment\" --createIBMCloudDNSEntries \"This will create your Public DNS enries in IBM Cloud\" --approveVCenterCert \"This will download VCenter certs and add to host trust store.\" --removeIBMCloudDNSEntries \"This will remove your Public DNS enries in IBM Cloud\" --displayOCPDNSRequirements \"This will display what DNS records that need to be created\" --displayOCPLoadBalanceRules \"This will display the Load Balnacer rules you will need\" --runOpenShiftInstallWaitBoot \"This will run openshift-install (wait-for boot)\" --approveCSR \"This will approve all pending CSR request\" --runOpenShiftInstallWaitInstall \"This will run openshift-install (wait-for install)\" --updateIngress \"This will get new IBM Cert and update the main ingress certs/secret\" --createOpenShiftContainerStorage \"This will create OpenShift Container Storage\" --createNFSServer \"This will create local NFS Server\" --createNFSDisk \"This will create local disk for NFS Server\" --configureLocalStorge \"This will create local stroage\" --createVMDashboard \"This will create the VM Dashboard Web UI\" --createImageRegistry \"This will create the OpenShift Image Registry\" --console \"This will display Admin Console Info\" --status \"This will display cluster info\" --installOpenShiftTools \"This will install oc, kubectl and openshift-install tools\" --displayVSpherePermissionsNeeded \"This will display all permissions needed for VSphere Install\" --validateReserverLookupDNS \"This will validate PTR records for UPI installs\" --applyFix \"This will apply a given fix to your Daffy env, pass the fix number to command\" --deleteAllFailedPods \"This will delete all pods in Failed state.\" --MACinstallOCPTools \"This will install oc and kubectl tools locally on your Mac\" --installRHACM \"This will install Red Hat Advanced Cluster Management\" --CephStatus \"This will show the status of ceph using ceph tools inside of OpenShift Storage\" --createAdminAccount \"This will create local htaccess admin account\" --help | --? | ? | -? | help | -help \"This help menu\" Examples To show OpenShift Console info, ID/Password and URL /data/daffy/ocp/build.sh <env_name> --console To show OpenShift status /data/daffy/ocp/build.sh <env_name> --status To show OpenShift Node sizes based on your environment file /data/daffy/ocp/build.sh <env_name> --tshirtSize","title":"OCP Build Help"},{"location":"Tips-and-Tricks/Common-Commands/#odf-existing-cluster","text":"If you have existing cluster, you can run the follow command to setup OpenShift Data Foundation on your existing cluster. /data/daffy/ocp/build.sh <env_name> --createOpenShiftContainerStorage","title":"ODF Existing Cluster"},{"location":"Tips-and-Tricks/Common-Commands/#daffy-tools","text":"Run the following command to install other tools you might need. /data/daffy/tools.sh Tools Flags Daffy Tools --prepareHost This will run the prepareHost for daffy --mustGather This will run the mustGather for daffy --installOC This will install the oc command line tool --installOpenShiftInstall This will install the openshift-install command line tool --installAWS This will install the aws commandline tool --installAzure This will install the azure commandline tool --installGCloud This will install the gcloud commandline tool --installGOVC This will install the govc commandline tool(VMware) --installCloudCTL This will install the cloudctl for CP4D --installCP4DCloudCLI This will install the CP4D Cloud CLI utility --installCPDCTL This will install the Cloud Pak 4 Data (CPDCTL) utility\"","title":"Daffy Tools"},{"location":"Tips-and-Tricks/Common-Commands/#gather-logs","text":"Run the following command to package all Daffy logs into a tar file. When troubleshooting, please send the gathered logs at /tmp/daffy/daffy_mustgather.tar.gz to the Daffy team. /data/daffy/tools.sh --mustGather","title":"Gather Logs"},{"location":"Tips-and-Tricks/Common-Commands/#security-cleanup","text":"Run the following command to clear all security (or individual) credentials. /data/daffy/security-cleanup.sh Additional Cleanup Flags --all This will cleanup all security including RH Pull Secret, SSH key, and IBM Entitlement keys --aws This will cleanup the aws security information --azure This will cleanup the azure security information --gcp This will cleanup the gcp credential information --ibm This will cleanup the IBM Entitlement Key and ibmcloud sesssion info --vsphere This will cleanup vsphere information --pullSecret This will cleanup your RedHat Pull Secret --ssh This will cleanup your local ssh key --ibm-ipi This will cleanup IBM API Key for use with IBM-ipi --roks This will cleanup IBM ROKS keys and login information --turbo This will cleanup your local ssh key --help|--?|?|-?|help|-help This help menu","title":"Security Cleanup"},{"location":"Tips-and-Tricks/Common-Issues/","text":"document.title = \"Tips and Tricks - Common Issues\"; Common Issues & Tips \u00b6 If you run into issues with your install, please take the time to read the output of the script. In most cases, the output will give you a clear indication of what the problem is and in some cases how to fix it. When reading the output during an error, always read messages starting from the TOP of your console output. Daffy will try to find as many errors as it can, so ALWAYS start at the TOP of your output. Most cases, the issue is a simple typo or a missing variable in the environment file. Please double check your environment file for any errors. Run the /data/daffy/refresh.sh script to download the latest version of the daffy scripts. Run the security-cleanup.sh script to remove all the sensitive data and the next time you run, the scripts will prompt you for the necessary input. /data/daffy/security-cleanup.sh","title":"Common Issues"},{"location":"Tips-and-Tricks/Common-Issues/#common-issues-tips","text":"If you run into issues with your install, please take the time to read the output of the script. In most cases, the output will give you a clear indication of what the problem is and in some cases how to fix it. When reading the output during an error, always read messages starting from the TOP of your console output. Daffy will try to find as many errors as it can, so ALWAYS start at the TOP of your output. Most cases, the issue is a simple typo or a missing variable in the environment file. Please double check your environment file for any errors. Run the /data/daffy/refresh.sh script to download the latest version of the daffy scripts. Run the security-cleanup.sh script to remove all the sensitive data and the next time you run, the scripts will prompt you for the necessary input. /data/daffy/security-cleanup.sh","title":"Common Issues &amp; Tips"},{"location":"Tips-and-Tricks/Edit-Files/","text":"document.title = \"Tips and Tricks - Edit Files\"; Editing Files \u00b6 Basic vi commands you will use to edit your environment file: \u00b6 Use the arrows on your keyboard to go to location you want to edit, you can not use your mouse. Esc starts all modes and Esc will end all modes. The Esc character is your friend!!!!!! Modes: Esc - X = deletes one character where you cursor is. Once done Esc Esc - I = puts you in insert mode, just start typing what you want to add. Once done Esc Esc - D - D = delete entire line cursor is on. Once done Esc Esc - U = will undo your last action. Once done Esc Once done, the following will save your file: Esc - : - W - Q - Enter (lower case w and q) If you messed up the file and you want to exit and not save your file: Esc - : - Q - ! - Enter (lower case q) VI Cheat-sheet - https://www.atmos.albany.edu/daes/atmclasses/atm350/vi_cheat_sheet.pdf Nano Cheat-sheet https://itsfoss.com/nano-editor-guide/","title":"Edit Files"},{"location":"Tips-and-Tricks/Edit-Files/#editing-files","text":"","title":"Editing Files"},{"location":"Tips-and-Tricks/Edit-Files/#basic-vi-commands-you-will-use-to-edit-your-environment-file","text":"Use the arrows on your keyboard to go to location you want to edit, you can not use your mouse. Esc starts all modes and Esc will end all modes. The Esc character is your friend!!!!!! Modes: Esc - X = deletes one character where you cursor is. Once done Esc Esc - I = puts you in insert mode, just start typing what you want to add. Once done Esc Esc - D - D = delete entire line cursor is on. Once done Esc Esc - U = will undo your last action. Once done Esc Once done, the following will save your file: Esc - : - W - Q - Enter (lower case w and q) If you messed up the file and you want to exit and not save your file: Esc - : - Q - ! - Enter (lower case q) VI Cheat-sheet - https://www.atmos.albany.edu/daes/atmclasses/atm350/vi_cheat_sheet.pdf Nano Cheat-sheet https://itsfoss.com/nano-editor-guide/","title":"Basic vi commands you will use to edit your environment file:"},{"location":"Tips-and-Tricks/Environment-File/","text":"document.title = \"Tips and Tricks - Environment File\"; Your environment file name must follow simple rules: \u00b6 All lower case no spaces Alphanumeric or - (no special characters) Must end in -env.sh Environment name is the prefix for the file name Example: /data/daffy/ocp/build.sh myenv =====> /data/daffy/env/ myenv -env.sh is the file that daffy would use The # at the beginning of the line will mark line as comment only All Names in file must be UPPER_CASE All true/false values must be lower case","title":"Environment File"},{"location":"Tips-and-Tricks/Environment-File/#your-environment-file-name-must-follow-simple-rules","text":"All lower case no spaces Alphanumeric or - (no special characters) Must end in -env.sh Environment name is the prefix for the file name Example: /data/daffy/ocp/build.sh myenv =====> /data/daffy/env/ myenv -env.sh is the file that daffy would use The # at the beginning of the line will mark line as comment only All Names in file must be UPPER_CASE All true/false values must be lower case","title":"Your environment file name must follow simple rules:"},{"location":"Tips-and-Tricks/Upgrade/","text":"document.title = \"Upgrade Daffy\"; Updating Daffy \u00b6 There are two ways to update existing daffy. Both process will replace the code of daffy but leave your environment files as is. It will require you to accept the daffy warranty. Upgrade \u00b6 1) Run the Refresh script in the daffy home directory /data/daffy/refresh.sh Screenshot Downgrade \u00b6 2) If you want to install older version of daffy from existing installed daffy: /data/daffy/refresh.sh --list This will give you a list of older version of daffy to downgrade to: Screenshot Warning If you have downgrade to older version of daffy and want to install another older version, you need to first upgrade to the lasted version then downgrade the other version you want. Install Older \u00b6 If you want to install an older version from the begining, you can with this command: curl http://get.daffy-installer.com/download-scripts/daffy-init.sh | sudo -E bash -s -- v2023-01-11.tar Version name format Just supply the name of the tar version you want to install. Format v ${YEAR} -${MONTH} -${DAY} .tar","title":"Daffy Upgrade"},{"location":"Tips-and-Tricks/Upgrade/#updating-daffy","text":"There are two ways to update existing daffy. Both process will replace the code of daffy but leave your environment files as is. It will require you to accept the daffy warranty.","title":"Updating Daffy"},{"location":"Tips-and-Tricks/Upgrade/#upgrade","text":"1) Run the Refresh script in the daffy home directory /data/daffy/refresh.sh Screenshot","title":"Upgrade"},{"location":"Tips-and-Tricks/Upgrade/#downgrade","text":"2) If you want to install older version of daffy from existing installed daffy: /data/daffy/refresh.sh --list This will give you a list of older version of daffy to downgrade to: Screenshot Warning If you have downgrade to older version of daffy and want to install another older version, you need to first upgrade to the lasted version then downgrade the other version you want.","title":"Downgrade"},{"location":"Tips-and-Tricks/Upgrade/#install-older","text":"If you want to install an older version from the begining, you can with this command: curl http://get.daffy-installer.com/download-scripts/daffy-init.sh | sudo -E bash -s -- v2023-01-11.tar Version name format Just supply the name of the tar version you want to install. Format v ${YEAR} -${MONTH} -${DAY} .tar","title":"Install Older"},{"location":"Tips-and-Tricks/occommands/","text":"document.title = \"OC Commands\"; Get Login token \u00b6 At certain parts of the install of cloud paks, you may need to get a login token and give to daffy. This will show you how to do this. 1) Login to our cluster UI Screenshot 2) Top right of your screen, select the drop down menu for your user Screenshot 3) From the menu, select the \"Copy login command\" Screenshot 4) This will open a new browser tab, click the \"Display Token\" link Screenshot 5) It will now show the token command. copy the oc login section only Screenshot 6) Once you copied the oc command, go back to your bastion and past it into terminal console. You will see a successful login after the command. Screenshot Get Cluster Name \u00b6 Sometimes you will use existing cluster for daffy, but daffy still needs to match your cluster name in the environment file with your runtime. There is an oc command that will display your cluster name. oc describe infrastructure / cluster | grep \"Infrastructure Name\" Get Worker Nodes \u00b6 Daffy will require your environment file to match your runtime environment. One area it checks is the number of worker nodes you have. They must match. Here is a command you can run to get the your worker nodes for your running cluster oc get nodes | grep \"worker\" Other Helpful Sites \u00b6 OpenShif Tips Site","title":"OC Commands"},{"location":"Tips-and-Tricks/occommands/#get-login-token","text":"At certain parts of the install of cloud paks, you may need to get a login token and give to daffy. This will show you how to do this. 1) Login to our cluster UI Screenshot 2) Top right of your screen, select the drop down menu for your user Screenshot 3) From the menu, select the \"Copy login command\" Screenshot 4) This will open a new browser tab, click the \"Display Token\" link Screenshot 5) It will now show the token command. copy the oc login section only Screenshot 6) Once you copied the oc command, go back to your bastion and past it into terminal console. You will see a successful login after the command. Screenshot","title":"Get Login token"},{"location":"Tips-and-Tricks/occommands/#get-cluster-name","text":"Sometimes you will use existing cluster for daffy, but daffy still needs to match your cluster name in the environment file with your runtime. There is an oc command that will display your cluster name. oc describe infrastructure / cluster | grep \"Infrastructure Name\"","title":"Get Cluster Name"},{"location":"Tips-and-Tricks/occommands/#get-worker-nodes","text":"Daffy will require your environment file to match your runtime environment. One area it checks is the number of worker nodes you have. They must match. Here is a command you can run to get the your worker nodes for your running cluster oc get nodes | grep \"worker\"","title":"Get Worker Nodes"},{"location":"Tips-and-Tricks/occommands/#other-helpful-sites","text":"OpenShif Tips Site","title":"Other Helpful Sites"},{"location":"overrides/","text":"document.title = \"Overrides\"; Warning There is less error checking on overrides that you add, so make sure you understand the override you are adding and its valid. Daffy allows you to override many settings. Most common installs do not need to do any overrides. Overrides are simple, just add the override to your own Environment file with the value you want to override.","title":"Index"},{"location":"overrides/daffy/","text":"document.title = \"Overrides - Daffy\"; General \u00b6 DEBUG \u00b6 If set to true, this will tell the daffy system to stop at the beginning of each major section of the process. It will allow you to view each step before it executes. You will have to hit any key on your keyboard to continue each step. If not set in your environment file or set to any other value the true, it will disable debug. This is great option for your first install to slowly go thru each step to understand what the daffy process is doing. Info Must be all lower case true Variable Name Default Value Valid Options DEBUG false true or false DEBUG = \"true\"","title":"Daffy"},{"location":"overrides/daffy/#general","text":"","title":"General"},{"location":"overrides/daffy/#debug","text":"If set to true, this will tell the daffy system to stop at the beginning of each major section of the process. It will allow you to view each step before it executes. You will have to hit any key on your keyboard to continue each step. If not set in your environment file or set to any other value the true, it will disable debug. This is great option for your first install to slowly go thru each step to understand what the daffy process is doing. Info Must be all lower case true Variable Name Default Value Valid Options DEBUG false true or false DEBUG = \"true\"","title":"DEBUG"},{"location":"overrides/ocp/","text":"document.title = \"Overrides - OpenShift\"; Worker Node Sizing \u00b6 Default values are based on TShirt Sizing. To override the default number of workers, just add the value you want to build in your environment file. Variable Name Install Type Default Value Valid Options VM_NUMBER_OF_WORKERS_MIN VM_TSHIRT_SIZE=\" Min \" 3 any Number VM_NUMBER_OF_WORKERS_LARGE VM_TSHIRT_SIZE=\" Large \" 3(+3 if ODF true) any Number #VM_NUMBER_OF_WORKERS_MIN=\"4\" VM_NUMBER_OF_WORKERS_LARGE = \"8\" Master Node Sizing \u00b6 Default values are based on TShirt Sizing. To override the default number of masters, just add the value you want to build in your environment file. Variable Name Install Type Default Value Valid Options VM_NUMBER_OF_MASTERS_MIN VM_TSHIRT_SIZE=\" Min \" 3 any Number VM_NUMBER_OF_MASTERS_LARGE VM_TSHIRT_SIZE=\" Large \" 3 any Number #VM_NUMBER_OF_MASTERS_MIN=\"1\" VM_NUMBER_OF_MASTERS_LARGE = \"4\" Masters Schedulable \u00b6 This will tell the cluster if you want the master nodes to preform workload. Should it be considered part of the other worker nodes. Variable Name Default Value Valid Options OCP_MASTER_NODES_SCHEDULABLE false true/false OCP_MASTER_NODES_SCHEDULABLE = \"true\" TShirt Sizing \u00b6 If you want to see what the current TShirt sizing is or what values you can override in your env file, just run the following command. It will show you current values and the names you can override in your file. / data / daffy / ocp / build.sh < env_name > -- tshirtSize Example of GCP: Current T - Shirt Sizing Info ################################################################ Setting VM T - Shirt Size to Large Google Cloud Platform : Bootstrap Node type = n1 - standard -2 GCP_MACHINE_TYPE_BOOTSTRAP_CPU_LARGE = \"2\" Master Nodes : -------------------------------------------------- GCP_MACHINE_TYPE_MASTER_LARGE = \"n1-standard-8\" VM_NUMBER_OF_MASTERS_LARGE = \"3\" GCP_MACHINE_TYPE_MASTER_CPU_LARGE = \"8\" Worker Nodes : -------------------------------------------------- GCP_MACHINE_TYPE_WORKER_LARGE = \"n1-standard-16\" VM_NUMBER_OF_WORKERS_LARGE = \"6\" GCP_MACHINE_TYPE_WORKER_CPU_LARGE = \"16\" OpenShift Storage Cluster Storage ( Block ) : -------------------------------------------------- VM_WORKER_DISK2_LARGE = \"50G\" OpenShift Storage Cluster Storage ( Block ) : -------------------------------------------------- VM_WORKER_DISK3_LARGE = \"200G\"","title":"OpenShift"},{"location":"overrides/ocp/#worker-node-sizing","text":"Default values are based on TShirt Sizing. To override the default number of workers, just add the value you want to build in your environment file. Variable Name Install Type Default Value Valid Options VM_NUMBER_OF_WORKERS_MIN VM_TSHIRT_SIZE=\" Min \" 3 any Number VM_NUMBER_OF_WORKERS_LARGE VM_TSHIRT_SIZE=\" Large \" 3(+3 if ODF true) any Number #VM_NUMBER_OF_WORKERS_MIN=\"4\" VM_NUMBER_OF_WORKERS_LARGE = \"8\"","title":"Worker Node Sizing"},{"location":"overrides/ocp/#master-node-sizing","text":"Default values are based on TShirt Sizing. To override the default number of masters, just add the value you want to build in your environment file. Variable Name Install Type Default Value Valid Options VM_NUMBER_OF_MASTERS_MIN VM_TSHIRT_SIZE=\" Min \" 3 any Number VM_NUMBER_OF_MASTERS_LARGE VM_TSHIRT_SIZE=\" Large \" 3 any Number #VM_NUMBER_OF_MASTERS_MIN=\"1\" VM_NUMBER_OF_MASTERS_LARGE = \"4\"","title":"Master Node Sizing"},{"location":"overrides/ocp/#masters-schedulable","text":"This will tell the cluster if you want the master nodes to preform workload. Should it be considered part of the other worker nodes. Variable Name Default Value Valid Options OCP_MASTER_NODES_SCHEDULABLE false true/false OCP_MASTER_NODES_SCHEDULABLE = \"true\"","title":"Masters Schedulable"},{"location":"overrides/ocp/#tshirt-sizing","text":"If you want to see what the current TShirt sizing is or what values you can override in your env file, just run the following command. It will show you current values and the names you can override in your file. / data / daffy / ocp / build.sh < env_name > -- tshirtSize Example of GCP: Current T - Shirt Sizing Info ################################################################ Setting VM T - Shirt Size to Large Google Cloud Platform : Bootstrap Node type = n1 - standard -2 GCP_MACHINE_TYPE_BOOTSTRAP_CPU_LARGE = \"2\" Master Nodes : -------------------------------------------------- GCP_MACHINE_TYPE_MASTER_LARGE = \"n1-standard-8\" VM_NUMBER_OF_MASTERS_LARGE = \"3\" GCP_MACHINE_TYPE_MASTER_CPU_LARGE = \"8\" Worker Nodes : -------------------------------------------------- GCP_MACHINE_TYPE_WORKER_LARGE = \"n1-standard-16\" VM_NUMBER_OF_WORKERS_LARGE = \"6\" GCP_MACHINE_TYPE_WORKER_CPU_LARGE = \"16\" OpenShift Storage Cluster Storage ( Block ) : -------------------------------------------------- VM_WORKER_DISK2_LARGE = \"50G\" OpenShift Storage Cluster Storage ( Block ) : -------------------------------------------------- VM_WORKER_DISK3_LARGE = \"200G\"","title":"TShirt Sizing"},{"location":"overrides/cloudpaks/","text":"document.title = \"Overrides - Cloud Paks\"; Cloud Pak Overrides \u00b6 In progress !!!!!","title":"Index"},{"location":"overrides/cloudpaks/#cloud-pak-overrides","text":"In progress !!!!!","title":"Cloud Pak Overrides"},{"location":"overrides/cloudpaks/cp4ba/","text":"document.title = \"Overrides - CP4BA\"; CP4BA Overrides \u00b6 Storage \u00b6 Based on the platform you delopy to, a default storge will be picked. If you want to override any, just add to your environment file based on names below and new stroage class name that is in the cluster. There are 6 storage class variables that are set: Variable Name Storage Type Info CP4BA_AUTO_STORAGE_CLASS_OCP_BLOCK Block Storage This is block storage , NFS is not block storage CP4BA_AUTO_STORAGE_CLASS_FAST_ROKS File Storage CP4BA_AUTO_STORAGE_CLASS_OCP File Storage CP4BA_AUTO_STORAGE_CLASS_OCP_SLOW File Storage CP4BA_AUTO_STORAGE_CLASS_OCP_MEDIUM File Storage CP4BA_AUTO_STORAGE_CLASS_OCP_FAST File Storage ROKS \u00b6 CP4BA_AUTO_STORAGE_CLASS_FAST_ROKS = \"ibmc-file-gold-gid\" CP4BA_AUTO_STORAGE_CLASS_OCP = \"ibmc-file-gold-gid\" CP4BA_AUTO_STORAGE_CLASS_OCP_BLOCK = \"ibmc-block-gold\" CP4BA_AUTO_STORAGE_CLASS_OCP_SLOW = \"ibmc-file-bronze-gid\" CP4BA_AUTO_STORAGE_CLASS_OCP_MEDIUM = \"ibmc-file-silver-gid\" CP4BA_AUTO_STORAGE_CLASS_OCP_FAST = \"ibmc-file-gold-gid\" ROKS VPC2 \u00b6 CP4BA_AUTO_STORAGE_CLASS_FAST_ROKS = \"ocs-storagecluster-cephfs\" CP4BA_AUTO_STORAGE_CLASS_OCP = \"ocs-storagecluster-cephfs\" CP4BA_AUTO_STORAGE_CLASS_OCP_BLOCK = \"ocs-storagecluster-ceph-rbd\" CP4BA_AUTO_STORAGE_CLASS_OCP_SLOW = \"ocs-storagecluster-cephfs\" CP4BA_AUTO_STORAGE_CLASS_OCP_MEDIUM = \"ocs-storagecluster-cephfs\" CP4BA_AUTO_STORAGE_CLASS_OCP_FAST = \"ocs-storagecluster-cephfs\" ROSA \u00b6 CP4BA_AUTO_PLATFORM = OCP CP4BA_AUTO_STORAGE_CLASS_FAST_ROKS = \"efs-nfs-client\" CP4BA_AUTO_STORAGE_CLASS_OCP = \"efs-nfs-client\" CP4BA_AUTO_STORAGE_CLASS_OCP_BLOCK = \"gp3\" CP4BA_AUTO_STORAGE_CLASS_OCP_SLOW = \"efs-nfs-client\" CP4BA_AUTO_STORAGE_CLASS_OCP_MEDIUM = \"efs-nfs-client\" CP4BA_AUTO_STORAGE_CLASS_OCP_FAST = \"efs-nfs-client\" All Others \u00b6 CP4BA_AUTO_STORAGE_CLASS_OCP = \"ocs-storagecluster-cephfs\" CP4BA_AUTO_STORAGE_CLASS_OCP_BLOCK = \"ocs-storagecluster-ceph-rbd\" CP4BA_AUTO_STORAGE_CLASS_OCP_SLOW = \"ocs-storagecluster-cephfs\" CP4BA_AUTO_STORAGE_CLASS_OCP_MEDIUM = \"ocs-storagecluster-cephfs\" CP4BA_AUTO_STORAGE_CLASS_OCP_FAST = \"ocs-storagecluster-cephfs\" Namespace \u00b6 A default namespace will be used, but if you want to override it, just update the following variable based on your deployment type and your new name in your environment file Starter \u00b6 CP4BA_AUTO_NAMESPACE_STARTER = \"cp4ba-starter\" Production \u00b6 CP4BA_DEPLOYMENT_PRODUCTION_DECISIONS_NAMESPACE = \"cp4ba-decisions\" CP4BA_DEPLOYMENT_PRODUCTION_CONTENT_NAMESPACE = \"cp4ba-content\" CP4BA_DEPLOYMENT_PRODUCTION_WORKFLOW_RUNTIME_NAMESPACE = \"cp4ba-workflow-runtime\" License \u00b6 By default, daffy will use a non-production license value during deployment. If you wish to change this, add the following varaible and update based on need to your environment file. Variable Name Options CP4BA_DEPLOYMENT_LICENSE non-production or production CP4BA_DEPLOYMENT_LICENSE = \"non-production\" ODM Production \u00b6 LDAP Overrides \u00b6 If you have existing LDAP Server and want to integrate with the ODM production deployment, below are the dafault values you can override to point to a new LDAP Server. Just add variable name and new value to your environment file. CP4BA_DEPLOYMENT_PRODUCTION_DECISIONS_LDAP_OU = \"${PROJECT_NAME}\" CP4BA_DEPLOYMENT_PRODUCTION_DECISIONS_LDAP_TYPE = \"IBM Security Directory Server\" CP4BA_DEPLOYMENT_PRODUCTION_DECISIONS_LDAP_SERVER = \"\" CP4BA_DEPLOYMENT_PRODUCTION_DECISIONS_LDAP_PORT = \"389\" CP4BA_DEPLOYMENT_PRODUCTION_DECISIONS_ADMIN_USER = \"odmAdmin\" CP4BA_DEPLOYMENT_PRODUCTION_DECISIONS_LDAP_BIND_SECRET = \"ldap-bind-secret\" CP4BA_DEPLOYMENT_PRODUCTION_DECISIONS_LDAP_BASE_DN = \"ou=${PROJECT_NAME},ou=odm,dc=ibm,dc=com\" CP4BA_DEPLOYMENT_PRODUCTION_DECISIONS_LDAP_SSL_ENABLED = \"false\" CP4BA_DEPLOYMENT_PRODUCTION_DECISIONS_LDAP_SECRET_NAME = \"ldap-ssl-cert\" CP4BA_DEPLOYMENT_PRODUCTION_DECISIONS_LDAP_USER_NAME_ATTRIBUTE = \"*:uid\" CP4BA_DEPLOYMENT_PRODUCTION_DECISIONS_LDAP_USER_DISPLAY_NAME_ATTR = \"uid\" CP4BA_DEPLOYMENT_PRODUCTION_DECISIONS_LDAP_GROUP_BASE_DN = \"ou=Groups,ou=${PROJECT_NAME},ou=odm,dc=ibm,dc=com\" CP4BA_DEPLOYMENT_PRODUCTION_DECISIONS_LDAP_GROUP_NAME_ATTRIBUTE = \"*:cn\" CP4BA_DEPLOYMENT_PRODUCTION_DECISIONS_LDAP_GROUP_DISPLAY_NAME_ATTR = \"cn\" CP4BA_DEPLOYMENT_PRODUCTION_DECISIONS_LDAP_GROUP_MEMBERSHIP_SEARCH_FILTER = \"(|(\\&(objectclass=groupofnames)(member={0}))(\\&amp;(objectclass=groupofuniquenames)(uniquemember={0})))\" CP4BA_DEPLOYMENT_PRODUCTION_DECISIONS_LDAP_GROUP_MEMBER_ID_MAP = \"groupofnames:member\" CP4BA_DEPLOYMENT_PRODUCTION_DECISIONS_LDAP_TDS_LC_USER_FILTER = \"(\\&amp;(cn=%v)(objectclass=person))\" CP4BA_DEPLOYMENT_PRODUCTION_DECISIONS_LDAP_TDS_LC_GROUP_FILTER = \"(\\&amp;(cn=%v)(|(objectclass=groupofnames)(objectclass=groupofuniquenames)(objectclass=groupofurls)))\"","title":"Business Automation"},{"location":"overrides/cloudpaks/cp4ba/#cp4ba-overrides","text":"","title":"CP4BA Overrides"},{"location":"overrides/cloudpaks/cp4ba/#storage","text":"Based on the platform you delopy to, a default storge will be picked. If you want to override any, just add to your environment file based on names below and new stroage class name that is in the cluster. There are 6 storage class variables that are set: Variable Name Storage Type Info CP4BA_AUTO_STORAGE_CLASS_OCP_BLOCK Block Storage This is block storage , NFS is not block storage CP4BA_AUTO_STORAGE_CLASS_FAST_ROKS File Storage CP4BA_AUTO_STORAGE_CLASS_OCP File Storage CP4BA_AUTO_STORAGE_CLASS_OCP_SLOW File Storage CP4BA_AUTO_STORAGE_CLASS_OCP_MEDIUM File Storage CP4BA_AUTO_STORAGE_CLASS_OCP_FAST File Storage","title":"Storage"},{"location":"overrides/cloudpaks/cp4ba/#roks","text":"CP4BA_AUTO_STORAGE_CLASS_FAST_ROKS = \"ibmc-file-gold-gid\" CP4BA_AUTO_STORAGE_CLASS_OCP = \"ibmc-file-gold-gid\" CP4BA_AUTO_STORAGE_CLASS_OCP_BLOCK = \"ibmc-block-gold\" CP4BA_AUTO_STORAGE_CLASS_OCP_SLOW = \"ibmc-file-bronze-gid\" CP4BA_AUTO_STORAGE_CLASS_OCP_MEDIUM = \"ibmc-file-silver-gid\" CP4BA_AUTO_STORAGE_CLASS_OCP_FAST = \"ibmc-file-gold-gid\"","title":"ROKS"},{"location":"overrides/cloudpaks/cp4ba/#roks-vpc2","text":"CP4BA_AUTO_STORAGE_CLASS_FAST_ROKS = \"ocs-storagecluster-cephfs\" CP4BA_AUTO_STORAGE_CLASS_OCP = \"ocs-storagecluster-cephfs\" CP4BA_AUTO_STORAGE_CLASS_OCP_BLOCK = \"ocs-storagecluster-ceph-rbd\" CP4BA_AUTO_STORAGE_CLASS_OCP_SLOW = \"ocs-storagecluster-cephfs\" CP4BA_AUTO_STORAGE_CLASS_OCP_MEDIUM = \"ocs-storagecluster-cephfs\" CP4BA_AUTO_STORAGE_CLASS_OCP_FAST = \"ocs-storagecluster-cephfs\"","title":"ROKS VPC2"},{"location":"overrides/cloudpaks/cp4ba/#rosa","text":"CP4BA_AUTO_PLATFORM = OCP CP4BA_AUTO_STORAGE_CLASS_FAST_ROKS = \"efs-nfs-client\" CP4BA_AUTO_STORAGE_CLASS_OCP = \"efs-nfs-client\" CP4BA_AUTO_STORAGE_CLASS_OCP_BLOCK = \"gp3\" CP4BA_AUTO_STORAGE_CLASS_OCP_SLOW = \"efs-nfs-client\" CP4BA_AUTO_STORAGE_CLASS_OCP_MEDIUM = \"efs-nfs-client\" CP4BA_AUTO_STORAGE_CLASS_OCP_FAST = \"efs-nfs-client\"","title":"ROSA"},{"location":"overrides/cloudpaks/cp4ba/#all-others","text":"CP4BA_AUTO_STORAGE_CLASS_OCP = \"ocs-storagecluster-cephfs\" CP4BA_AUTO_STORAGE_CLASS_OCP_BLOCK = \"ocs-storagecluster-ceph-rbd\" CP4BA_AUTO_STORAGE_CLASS_OCP_SLOW = \"ocs-storagecluster-cephfs\" CP4BA_AUTO_STORAGE_CLASS_OCP_MEDIUM = \"ocs-storagecluster-cephfs\" CP4BA_AUTO_STORAGE_CLASS_OCP_FAST = \"ocs-storagecluster-cephfs\"","title":"All Others"},{"location":"overrides/cloudpaks/cp4ba/#namespace","text":"A default namespace will be used, but if you want to override it, just update the following variable based on your deployment type and your new name in your environment file","title":"Namespace"},{"location":"overrides/cloudpaks/cp4ba/#starter","text":"CP4BA_AUTO_NAMESPACE_STARTER = \"cp4ba-starter\"","title":"Starter"},{"location":"overrides/cloudpaks/cp4ba/#production","text":"CP4BA_DEPLOYMENT_PRODUCTION_DECISIONS_NAMESPACE = \"cp4ba-decisions\" CP4BA_DEPLOYMENT_PRODUCTION_CONTENT_NAMESPACE = \"cp4ba-content\" CP4BA_DEPLOYMENT_PRODUCTION_WORKFLOW_RUNTIME_NAMESPACE = \"cp4ba-workflow-runtime\"","title":"Production"},{"location":"overrides/cloudpaks/cp4ba/#license","text":"By default, daffy will use a non-production license value during deployment. If you wish to change this, add the following varaible and update based on need to your environment file. Variable Name Options CP4BA_DEPLOYMENT_LICENSE non-production or production CP4BA_DEPLOYMENT_LICENSE = \"non-production\"","title":"License"},{"location":"overrides/cloudpaks/cp4ba/#odm-production","text":"","title":"ODM Production"},{"location":"overrides/cloudpaks/cp4ba/#ldap-overrides","text":"If you have existing LDAP Server and want to integrate with the ODM production deployment, below are the dafault values you can override to point to a new LDAP Server. Just add variable name and new value to your environment file. CP4BA_DEPLOYMENT_PRODUCTION_DECISIONS_LDAP_OU = \"${PROJECT_NAME}\" CP4BA_DEPLOYMENT_PRODUCTION_DECISIONS_LDAP_TYPE = \"IBM Security Directory Server\" CP4BA_DEPLOYMENT_PRODUCTION_DECISIONS_LDAP_SERVER = \"\" CP4BA_DEPLOYMENT_PRODUCTION_DECISIONS_LDAP_PORT = \"389\" CP4BA_DEPLOYMENT_PRODUCTION_DECISIONS_ADMIN_USER = \"odmAdmin\" CP4BA_DEPLOYMENT_PRODUCTION_DECISIONS_LDAP_BIND_SECRET = \"ldap-bind-secret\" CP4BA_DEPLOYMENT_PRODUCTION_DECISIONS_LDAP_BASE_DN = \"ou=${PROJECT_NAME},ou=odm,dc=ibm,dc=com\" CP4BA_DEPLOYMENT_PRODUCTION_DECISIONS_LDAP_SSL_ENABLED = \"false\" CP4BA_DEPLOYMENT_PRODUCTION_DECISIONS_LDAP_SECRET_NAME = \"ldap-ssl-cert\" CP4BA_DEPLOYMENT_PRODUCTION_DECISIONS_LDAP_USER_NAME_ATTRIBUTE = \"*:uid\" CP4BA_DEPLOYMENT_PRODUCTION_DECISIONS_LDAP_USER_DISPLAY_NAME_ATTR = \"uid\" CP4BA_DEPLOYMENT_PRODUCTION_DECISIONS_LDAP_GROUP_BASE_DN = \"ou=Groups,ou=${PROJECT_NAME},ou=odm,dc=ibm,dc=com\" CP4BA_DEPLOYMENT_PRODUCTION_DECISIONS_LDAP_GROUP_NAME_ATTRIBUTE = \"*:cn\" CP4BA_DEPLOYMENT_PRODUCTION_DECISIONS_LDAP_GROUP_DISPLAY_NAME_ATTR = \"cn\" CP4BA_DEPLOYMENT_PRODUCTION_DECISIONS_LDAP_GROUP_MEMBERSHIP_SEARCH_FILTER = \"(|(\\&(objectclass=groupofnames)(member={0}))(\\&amp;(objectclass=groupofuniquenames)(uniquemember={0})))\" CP4BA_DEPLOYMENT_PRODUCTION_DECISIONS_LDAP_GROUP_MEMBER_ID_MAP = \"groupofnames:member\" CP4BA_DEPLOYMENT_PRODUCTION_DECISIONS_LDAP_TDS_LC_USER_FILTER = \"(\\&amp;(cn=%v)(objectclass=person))\" CP4BA_DEPLOYMENT_PRODUCTION_DECISIONS_LDAP_TDS_LC_GROUP_FILTER = \"(\\&amp;(cn=%v)(|(objectclass=groupofnames)(objectclass=groupofuniquenames)(objectclass=groupofurls)))\"","title":"LDAP Overrides"},{"location":"overrides/cloudpaks/cp4d/","text":"document.title = \"Overrides - CP4D\"; CP4D Overrides \u00b6 In progress !!!!!","title":"Data"},{"location":"overrides/cloudpaks/cp4d/#cp4d-overrides","text":"In progress !!!!!","title":"CP4D Overrides"},{"location":"overrides/cloudpaks/cp4i/","text":"document.title = \"Overrides - CP4I\"; CP4I Overrides \u00b6 In progress !!!!!","title":"Integration"},{"location":"overrides/cloudpaks/cp4i/#cp4i-overrides","text":"In progress !!!!!","title":"CP4I Overrides"},{"location":"overrides/cloudpaks/cp4waiops/","text":"document.title = \"Overrides - CP4WAIOps\"; CP4WAIOps Overrides \u00b6 In progress !!!!!","title":"Wations AIOps"},{"location":"overrides/cloudpaks/cp4waiops/#cp4waiops-overrides","text":"In progress !!!!!","title":"CP4WAIOps Overrides"},{"location":"overrides/platforms/","text":"document.title = \"Overrides - Platforms\"; Platform Overrides \u00b6 In progress !!!!!","title":"Index"},{"location":"overrides/platforms/#platform-overrides","text":"In progress !!!!!","title":"Platform Overrides"},{"location":"overrides/platforms/aws/","text":"document.title = \"Overrides - AWS\"; AWS Overrides \u00b6 In progress !!!!!","title":"AWS"},{"location":"overrides/platforms/aws/#aws-overrides","text":"In progress !!!!!","title":"AWS Overrides"},{"location":"overrides/platforms/azure/","text":"document.title = \"Overrides - Azure\"; Azure Overrides \u00b6 In progress !!!!!","title":"Azure"},{"location":"overrides/platforms/azure/#azure-overrides","text":"In progress !!!!!","title":"Azure Overrides"},{"location":"overrides/platforms/gcp/","text":"document.title = \"Overrides - GCP\"; GCP Overrides \u00b6 In progress !!!!!","title":"Google"},{"location":"overrides/platforms/gcp/#gcp-overrides","text":"In progress !!!!!","title":"GCP Overrides"},{"location":"overrides/platforms/ibm/","text":"document.title = \"Overrides - IBM\"; IBM Overrides \u00b6 In progress !!!!!","title":"IBM"},{"location":"overrides/platforms/ibm/#ibm-overrides","text":"In progress !!!!!","title":"IBM Overrides"},{"location":"overrides/platforms/vsphere/","text":"document.title = \"Overrides - VSphere\"; VSphere Overrides \u00b6 In progress !!!!!","title":"VSphere"},{"location":"overrides/platforms/vsphere/#vsphere-overrides","text":"In progress !!!!!","title":"VSphere Overrides"},{"location":"overrides/software/db2/","text":"document.title = \"Overrides - DB2\"; DB2 Overrides \u00b6 In progress !!!!!","title":"DB2"},{"location":"overrides/software/db2/#db2-overrides","text":"In progress !!!!!","title":"DB2 Overrides"},{"location":"overrides/software/ldap/","text":"document.title = \"Overrides - LDAP\"; LDAP Overrides \u00b6 In progress !!!!!","title":"LDAP"},{"location":"overrides/software/ldap/#ldap-overrides","text":"In progress !!!!!","title":"LDAP Overrides"},{"location":"overrides/software/turbonomics/","text":"document.title = \"Overrides - Turbonomics\"; Turbonomics Overrides \u00b6 In progress !!!!!","title":"Turbonomics"},{"location":"overrides/software/turbonomics/#turbonomics-overrides","text":"In progress !!!!!","title":"Turbonomics Overrides"}]}