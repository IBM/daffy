{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"Process New User Experienced User <p>           Daffy is Deployment Automation Framework For You. A tool to do all the heavy lifting of the OpenShift and IBM Cloud Pak installs. The National Market Top Team created Daffy to assist the technical sales teams with the progression of IBM Cloud Pak opportunities. The goal is to provide the technical sales with a set of (easy to use) scripts that will aid in the installation of OpenShift and the IBM Cloud Pak's.         </p> <p>           Current Version v2024-01-31 (Release Notes) </p>          Fit for purpose                 Using Daffy IBMers, business partners and customers are onboarded to IBM Cloud Paks in less than a few hours, removing challenges that previously existed when setting up OpenShift.        <p>Important</p>"},{"location":"#daffy-scripts-were-designed-to-help-pre-sales-ctpbp-with-poc-deployments-if-you-choose-to-use-this-in-a-production-environment-you-may-but-it-will-be-the-installers-responsibility-to-support-that-installation-ibm-can-not-give-support-for-daffy-itself-as-it-relates-to-openshift-and-cloud-pak-deployments-you-can-open-a-ticket-with-ibm-support-the-installerbusiness-partner-would-need-to-verify-that-the-environment-meets-all-ha-best-practices-management-aspects-and-security-requirements-as-this-is-a-scripting-engine-you-have-full-access-to-the-logiccode-and-have-ability-to-make-any-changes-you-feel-fit-if-you-do-make-any-changes-to-the-daffy-engine-outside-of-your-cluster-environment-file-you-are-on-your-own-and-we-will-not-be-able-to-assist-with-that-environment-please-refer-to-the-production-deployment-guides-for-the-recommended-approach-when-advising-customers-on-how-to-deploy-a-production-ready-environment","title":"Daffy scripts were designed to help pre-sales (CTP/BP) with POC deployments. If you choose to use this in a production environment, you may, but it will be the installer's responsibility to support that installation. IBM can not give support for Daffy itself. As it relates to OpenShift and Cloud pak deployments, you can open a ticket with IBM Support. The installer/business partner would need to verify that the environment meets all HA, best practices, management aspects, and security requirements. As this is a scripting engine, you have full access to the logic/code and have ability to make any changes you feel fit. If you do make any changes to the Daffy engine outside of your cluster environment file, you are on your own, and we will not be able to assist with that environment. Please refer to the Production Deployment Guides for the recommended approach when advising customers on how to deploy a Production Ready Environment.","text":""},{"location":"Meet-the-Team/","title":"Meet the Team","text":""},{"location":"Meet-the-Team/#creators","title":"CreatorsKyle DawsonDavid KrierJeff ImholzSunil S. PatelDaffy the Mascot","text":"<p>Principal Automation Technical Specialist, IBM Technology, US National Market</p> <p>Principal Integration Technical Specialist, IBM Technology, US National Market</p> <p>Principal Automation Technical Specialist, IBM Technology, US National Market</p> <p>Principal Technical Sales Manager, Data-AI-Automation Blackbelt team, IBM Technology, US National Market</p> <p>OpenShift Expert, IBM Technology, US National Market Global Sales - Software Sales</p>"},{"location":"hidden/","title":"MKDOCS Install &amp; Configuration","text":"<p>This site uses MKDocs for the publishing of the \"GitHub Pages\" site</p> <p>You MUST install both MKDocs and the Material Theme.</p> <p>Here is how you setup your machine to edit the documentation hosted on the Github Pages Site. Details of these steps are below.</p> <p>Outline of the steps you will take</p> <ol> <li>Install MKdocs</li> <li>Install the MKdocs Theme (Material)</li> <li>Get a Personal Access Token from GitHub</li> <li>Clone this repository</li> <li>Make your changes (Feel free to reach out to Dave Krier or Kyle Dawson for help on how to use MKDocs)</li> <li>Deploy your changes</li> </ol>"},{"location":"hidden/#install-mkdocs","title":"Install MKDocs","text":"<p>Here is the installation documentation for MKDocs</p> <p>Install the MKDocs theme Install guidance is on the Material theme page.</p> <p>Material Theme - Quick Start</p>"},{"location":"hidden/#material-theme-documentation","title":"Material Theme Documentation","text":"<pre><code>pip3 install mkdocs-material\n</code></pre> <p>You can check the version of Material you currently have installed with this command. </p> <pre><code>python3 -m pip list\n</code></pre> <p>If you have any issues with the pip install, you may need to perform an uninstall and reinstall. </p> <pre><code>pip3 uninstall  mkdocs-material\npip3 install  mkdocs-material\n\npip3 uninstall mkdocs\npip3 install mkdocs\n</code></pre> <p>Link to the Material Theme Documentation</p>"},{"location":"hidden/#you-need-a-personal-access-token-from-github-this-is-super-simple","title":"You need a \"Personal Access Token\" from Github. (This is super simple).","text":"<p>Why do I need this? You need this because you need to be able to push changes back into the github repository.</p> <p>Here is a Github doc page you can look at for help if the steps below do not make sense.</p> <ol> <li>Login to Github.</li> <li>Click on your user icon in the upper right hand corner</li> <li>Select Settings --&gt; Developer Settings --&gt; Personal Access Tokens</li> <li>Generate a new token with only these prevliages<ul> <li>Repo:Status</li> <li>Repo-Deployment</li> <li>Public-Repo</li> <li>Take note of the access key (It's the only time you will see it)</li> </ul> </li> </ol> <p>Important</p> <p>You will use this Access Token as your password when using the gh-deploy mkdocs command. </p>"},{"location":"hidden/#clone-this-repo-in-github","title":"Clone this repo in Github.","text":"<p>I assume you will all know how to do that!!</p>"},{"location":"hidden/#mkdocs-user-guide-modifying-the-site","title":"MKDocs User Guide - Modifying the site","text":"<p>MKDocs Documentation</p> <p>Deploying your changes.</p> <ol> <li> <p>Navigate to your cloned repo directory in a terminal window.</p> <p>You must be inside the /daffydoc directory to run the following commands.</p> </li> <li> <p>Build the files using the build command</p> </li> </ol> <pre><code>mkdocs build\n</code></pre> <ol> <li>Deploy the newly built files to the Github Pages site </li> </ol> <p>Note</p> <p>This is where you will use the access token from Github. Use the access     token instead of your password. </p> <pre><code>mkdocs gh-deploy\n</code></pre>"},{"location":"release/","title":"Release","text":""},{"location":"release/#v2024-01-31","title":"v2024-01-31","text":"<pre><code>    Tested OpenShift with Current Daffy Release\n          GCP                4.12.40/4.14.6\n          AWS                4.12.40/4.14.6\n          Azure              4.12.40/4.14.6\n          ROKS               4.12.44/4.14.5\n          VSphere            4.12.40/4.14.6\n          KVM                4.12.40/4.14.6\n          IBM IPI            4.12.40/4.14.6\n          ROSA               4.12.40/4.14.6\n          ARO                4.12.40\n\n    Cloud Pak for Data\n          Added support for 4.7.2, 4.7.3, &amp; 4.7.4\n          Deprecated support for 4.6.*\n    Cloud Pak for Business Automation\n          Added Support for 23.0.1 IFX005\n    Cloud Pak for Integration\n          Added Support for 2023.4.1\n    OpenShift\n          Added Support for 4.14\n          Removed support for 4.10 &amp; 4.11\n    Misc\n          Remove appstore apps\n</code></pre>"},{"location":"release/#v2023-08-04","title":"v2023-08-04","text":"<pre><code>    Tested OpenShift with Current Daffy Release\n          GCP                4.10.56/4.12.21/4.13.2\n          AWS                4.10.56/4.12.21/4.13.2\n          Azure              4.10.56/4.12.21/4.13.2\n          ROKS               4.10.61/4.12.20/4.13.0\n          VSphere            4.10.56/4.12.21/4.13.2\n          KVM                4.10.56/4.12.21/4.13.2\n          IBM IPI            4.10.56/4.12.21/4.13.2\n          ROSA               4.10.56/4.12.21/4.13.4\n          ARO                4.10.63\n          *** Known issue with OCP 4.10.39 and above.  Operators fail to install sporadically on all Cloud Paks\n    Cloud Pak for Data\n          Added support for 4.7.1\n          Added new Watsonx Data Service\n          Deprecated support for 4.5.3\n    Cloud Pak for Business Automation\n          Added Support for 23.0.1 IFX001\n          Added Support for RHEL 9.X\n          Removed Support for 22.0.2\n          Added support to use MSP (ROSA,ARO,ROKS) certificates in cloud pak urls\n    Misc\n          Added Support for RHEL 9.X for Db2 and LDAP\n          Changed default ROSA install to multi availability zone (still support single)\n          Updated ROSA to be able to bring your own VPC\n          ROSA update to support private &amp; private-link clusters\n          Updated build of EFS to support multi availability zones for ROSA  \n          Support for OCP Version in ARO build\n</code></pre>"},{"location":"release/#v2023-07-24","title":"v2023-07-24","text":"<pre><code>    Tested OpenShift with Current Daffy Release\n          GCP                4.10.56/4.12.16/4.13.2\n          AWS                4.10.56/4.12.16/4.13.2\n          Azure              4.10.56/4.12.16/4.13.2\n          ROKS               4.10.61/4.12.20/4.13.0\n          VSphere            4.10.56/4.12.16/4.13.2\n          KVM                4.10.56/4.12.16/4.13.2\n          IBM IPI            4.10.56/4.12.16/4.13.2\n          ROSA               4.10.56/4.12.16/4.13.0\n          ARO                4.10.40\n          *** Known issue with OCP 4.10.39 and above.  Operators fail to install sporadically on all Cloud Paks\n    Instana\n          Added support for 247-0\n    Cloud Pak for Business Automation\n          Added Support for 23.0.1\n    Cloud Pak for Data\n          Added support for 4.7.0\n          Deprecated support for 4.5.3\n    Cloud Pak for Integration\n          Added support for cluster scoped install\n          Updated App Connect to 12.0.8.0-r3\n    Cloud Pak for Watston AIOps\n          Added support for V4.1.0\n          Removed support for 3.5.x\n    Misc\n          Daffy bastion support for RHEL 9.x\n          Fixed variable for GCP Install\n          Fixed bug in AWS where it wouldn't build ODF machine sets if brought own VPC\n</code></pre>"},{"location":"release/#v2023-06-22","title":"v2023-06-22","text":"<pre><code>    Tested OpenShift with Current Daffy Release\n          GCP                4.10.56/4.12.16/4.13.2\n          AWS                4.10.56/4.12.16/4.13.2\n          Azure              4.10.56/4.12.16/4.13.2\n          ROKS               4.10.53/4.12.16\n          VSphere            4.10.56/4.12.16/4.13.2\n          KVM                4.10.56/4.12.16/4.13.2\n          IBM IPI            4.10.56/4.12.16/4.13.2\n          ROSA               4.10.56/4.12.16/4.13.0\n          ARO                4.10.40\n          *** Known issue with OCP 4.10.39 and above.  Operators fail to install sporadically on all Cloud Paks\n    Cloud Pak for Business Automation\n        Added Support for 22.0.2 IF004\n        Added Support for 22.0.2 IF005\n    Cloud Pak for Data\n        Added Support for 4.6.6\n        Update process documented using Daffy (https://ibm.github.io/daffy/Cloud-Paks/Data/#day-2-operations-upgrade)\n    Cloud Pak for Integration\n        Updated to support cloud pak foundational services in a custom namespace (cp4i-fs)\n        Added support for 2023.2.1\n        Removed ability to deploy operations dashboard with 2023 as its deprecated.\n        Added Event Endpoint Management as its own deployable service\n    Cloud Pak for Watson AIOps\n        Added support for V 3.7.2\n    OpenShift\n        Added Support for 4.13\n        Update mirror-registry to 1.3.6\n    Turbonomic\n        Added support for 8.9.2\n        Added support for 8.9.0\n        Removed support for all versions prior to 8.7.5\n    Misc\n        On-line Configurator Updated to support latest versions of cloud pak's\n        Command-line configurator updated to support latest updates\n</code></pre>"},{"location":"release/#v2023-05-22","title":"v2023-05-22","text":"<pre><code>    Tested OpenShift with Current Daffy Release\n          GCP                4.10.56/4.12.12\n          AWS                4.10.56/4.12.12\n          Azure              4.10.56/4.12.12\n          ROKS               4.10.53/4.12.7\n          VSphere            4.10.56/4.12.12\n          KVM                4.10.56/4.12.12\n          IBM IPI            4.10.56/4.12.12\n          ROSA               4.10.56/4.12.12\n          ARO                4.10.40\n          *** Known issue with OCP 4.10.39 and above.  Operators fail to install sporadically on all Cloud Paks\n      Cloud Pak for Business Automation\n          Added Support for 22.0.2 IF003\n      Cloud Pak for Watson AIOps\n          Added Support for 3.7.1\n      Cloud Pak for Data\n          Added Support for 4.6.5\n          Streamlined status messages for only those enabled\n          Added --exportcpdvars to build and services to get the variables for upgrades\n      Cloud Pak for Integration\n          Updated versions with MQ, ACE, APIC\n          Streamlined status messages for only those enabled\n      WebSphere Automation\n          Added support for 1.6\n      Misc\n          Removed support for 4.12.11 and 4.11.35 and below on AWS/ROSA due to known OCP bug with AWS\n</code></pre>"},{"location":"release/#v2023-04-27","title":"v2023-04-27","text":"<pre><code>      Tested OpenShift with Current Daffy Release\n          GCP                4.10.36/4.12.9\n          AWS                4.10.36/4.12.9\n          Azure              4.10.36/4.12.9\n          ROKS               4.10.53/4.12.7\n          VSphere            4.10.36/4.12.9\n          KVM                4.10.36/4.12.9\n          IBM IPI            4.10.36/4.12.9\n          ROSA               4.10.36/4.12.9\n          ARO                4.10.40\n          *** Known issue with OCP 4.10.39 and above.  Operators fail to install sporadically on all Cloud Paks\n      Cloud Pak for Integration\n          Updated to latest versions of subscriptions and services\n      Cloud Pak for Data\n          Updated to 4.6.4 with support on OpenShift 4.12\n          New Watson Knowledge Catalog settings (WKC)\n            Data Quality - CP4D_WKC_ENABLE_DQ=\"true\"\n            Data Graph - CP4D_WKC_ENABLE_GRAPH=\"true\"\n            Data Lineage (Manta) - CP4D_WKC_ENABLE_MANTA=\"true\"\n            CP4D_WKC_MANTA_KEYLOC=\"directory where license key for manta is stored Default is /tmp\"\n            CP4D_WKC_MANTA_LICENSE=\"Name of Manta license file, Default is license.key\"\n          New Data Replication Service\n      Cloud Pak for Watson AIOPS\n          Added support for 3.7.0\n          Remove support for all versions below 3.5.0\n      Instana Server\n          Added Support for 239-246 for backend Instana Server on OpenShift (must have sales &amp; agent keys)\n          INSTANA_SERVER_INSTALL=true\n      Misc\n          Added support for KVM and Ubuntu 22.04 (deprecating 20.04 in next release)\n          Added support for Ubuntu 22.04 as new bastion (deprecating 20.04 in next release)\n</code></pre>"},{"location":"release/#v2023-03-24a","title":"v2023-03-24a","text":"<pre><code>      Cloud Pak for Business Automation\n          SSL certificates allowed as Alt name *.apps.${CLUSTER}.${BASE_DOMAIN}\n      OpenShift\n          SSL certificates allowed as Alt name *.apps.${CLUSTER}.${BASE_DOMAIN} and api.${CLUSTER}.${BASE_DOMAIN}\n</code></pre>"},{"location":"release/#v2023-03-24","title":"v2023-03-24","text":"<pre><code>      Cloud Pak for Security\n          Removed entire cloud pak\n      Cloud Pak for Business Automation\n          Support for OpenShift 4.12\n          Removed Support for 22.0.1\n          Removed Support for OPS\n      CLoud Pak for WAIOPS\n          Removed Support for 3.4.0 and 3.4.1\n          Added Support for 3.6.1 and 3.6.2\n      Cloud Pak for Data\n          Removed Support for 4.5.1 and 4.5.2\n      Cloud Pak for Integration\n          Added Support for OpenShift 4.12\n      OpenShift\n          Removed Support for 4.8 and 4.9\n      Misc\n          Added basic info during the setup of Daffy.  Install location, version, cert folder, samples, etc\n          Removed daffy fixpak feature\n          Removed support for running daffy on mac desktop\n</code></pre>"},{"location":"release/#v2023-03-09","title":"v2023-03-09","text":"<pre><code>      Tested OpenShift with Current Daffy Release\n          GCP                4.10.36/4.12.2(OCP Only)\n          AWS                4.10.36/4.12.2(OCP Only)\n          Azure              4.10.36/4.12.2(OCP Only)\n          ROKS               4.10.47/4.12.3(OCP Only)\n          VSphere            4.10.36/4.12.2(OCP Only)\n          KVM                4.10.36/4.12.2(OCP Only)\n          IBM IPI            4.10.36/4.12.2(OCP Only)\n          ROSA               4.10.36/4.12.1(OCP Only)\n          ARO                4.10.40\n          *** Known issue with OCP 4.10.39 and above.  Operators fail to install sporadically on all Cloud Paks\n      Turbonomic\n          Added Support 8.7.0 &amp; 8.7.5\n          Added Support for KubeTurbo to OCP Build\n             Must have TURBO_KUBE_MONITORING=true in the env file along with the KubeTurbo necessary vars\n      Airgap\n          Added new option to mirror OpenShift Catalog as stand only command line flag\n      Instana\n          Added support for Instana agent to be used in all Cloud Paks (must have an instana download key)\n            INSTANA_MONITORING=\"true\"\n      Cloud Pak for Business Automation\n          Added Support for 22.0.2 IF001 and IF002\n          Added bring your own SSL certificates to CP4BA(cp-console and cpd)\n              Download SSL certificates from IBM Secrets Manager(Optionally)\n          Removed Support for OPS(Open Prediction Service)              \n      Cloud Pak for Integration\n          Added bring your own SSL certificates to CP4I\n          Added support for Event Endpoint Management by default when deploying API Connect\n          Support for cluster logging and monitoring\n      Cloud Pak for Data\n          Support for 4.6.1,4.6.2 and 4.6.3\n          Support for new service AI FactSheets\n          Support for new service Data Replication\n          Added flags for Manta with WKC\n      OpenShift\n          Support for Azure ARO Cluster install\n              Cloud Paks Supported - CP4BA, CP4I and WSA\n          Support for Ingress Certs install *.apps and api URLs. If stored in /data/daffy/certs/${CLUSTER_NAME}\n          Support for OpenShift 4.12\n               IBM,AWS,Azure,GCP,KVM,VSphere,Rosa,ROKS\n          AWS Added support during cleanup to remove S3 Buckets that were part of cluster (Default this is off AWS_S3_CLEANUP_CLUSTER_BUCKETS_ON_DESTROY=false)\n          IBM IPI added improvements to cluster cleanup(Retry logic and improved timing and geting IBM Cloud support to fix their backend)\n      IBM\n          Support to download Certs from IBM Cloud Secrets Manager and stored in /data/daffy/certs/${CLUSTER_NAME}\n      Db2\n          Added new precheck for running as root user\n      Misc\n          Added cleanup logic to remove older log files.  Default is 30 days.  LOG_DIR_RETENTION=30\n          Blink messages that are long, will now show a count down to how much longer it will wait for certain functions plus total time\n</code></pre>"},{"location":"release/#v2023-01-11","title":"v2023-01-11","text":"<pre><code>      Tested OpenShift with Current Daffy Release\n          GCP       4.8.51 / 4.10.36\n          AWS       4.8.51 / 4.10.36\n          Azure     4.8.51 / 4.10.36\n          ROKS      4.8.51 / 4.10.43\n          VSphere   4.8.51 / 4.10.36\n          KVM       4.8.51 / 4.10.36\n          IBM IPI            4.10.36\n          ROSA               4.10.36\n          *** Known issue with OCP 4.10.39 and above.  Operators fail to install sporadically on all Cloud Paks\n      Cloud Pak for Integration\n          Added Support for 2022.4.1\n      WebSphere Automation\n          Added Support for 1.5\n      Cloud Pak for Business Automation\n          Added Support for 22.0.2\n      Cloud Pak for Watson AIOPS\n          Added support for 3.5.1\n          Added support for 3.6.0\n          Removed support for all 3.3.x versions\n      OpenShift\n          Support for AWS Rosa(Install OCP and Cloud Paks)\n          Support for EFS for storage(will setup EFS Storage on AWS/ROSA) -Build Cluster and CP4I and CP4BA support\n            Create EFS Provider and Setup in Cluster\n          Bug fix - Proxy install on RHEL, convert NO_PROXY to lower case no_proxy. Lower case works on both Ubuntu and RHEL\n      VSphere\n          New Flag to skip building vsphere folders. If large VSphere network, the checking of folders can take a long time (10 min)  Default : VSPHERE_CREATE_FOLDERS=\"true\"\n          Support for full path of Network Path VSPHERE_NETWORK1=\"itzna-itz-wdc04-private/itz-550005mqws-arcqrk30-segment\"\n      AirGap\n          Support for OCP 4.9 and 4.10 and ODF(Support from dwakeman@us.ibm.com)\n      OpenShift Data Foundation(ODF)\n          Changed core logic for ODF to build new machine sets for Infra Storage nodes Separate from standard worker machine sets\n               aws-ipi, vsphere-ipi, azure-ipi, ibm-ipi, gcp-ipi\n</code></pre>"},{"location":"release/#v2022-12-02a","title":"v2022-12-02a","text":"<pre><code>      Cloud Pak for Business Automation\n          Added Support for 22.0.1 IF005\n          Added new Service docprocessing(Automation Document Processing - ADP)\n      Cloud Pak for Data\n          Updated default value for CRIO PIDS tunning to 16384\n          Bug fix for SPSS and WS - version flag change\n      Misc\n          New logic to install podman on Ubuntu - Issue with opensuse.org cert (Solution from green@techd.com)\n</code></pre>"},{"location":"release/#v2022-12-02","title":"v2022-12-02","text":"<pre><code>      Tested OpenShift with Current Daffy Release\n          GCP       4.8.51 / 4.10.39\n          AWS       4.8.51 / 4.10.39\n          Azure     4.8.51 / 4.10.39\n          ROKS      4.8.51 / 4.10.39\n          VSphere   4.8.51 / 4.10.39\n          KVM       4.8.51 / 4.10.39\n          IBM IPI            4.10.39\n      Airgap\n          Ability to build local repo on internet facing bastion, Loads openshift catalog into local Repository\n      Cloud Pak for Data\n         Support for 4.6.0\n         Added Watson Pipelines (Only supported 4.6.0)\n      Websphere Automation\n         Added support for NFS Storge defaults\n      AppStore\n         Added CP4D Backup and Restore(DR Usecase)\n      TechZone Tiles\n         Cloud Pak for Data\n               Added support for CP4D 4.6.0\n               Added Watson Pipelines\n               Added Watson OpenScale\n</code></pre>"},{"location":"release/#v2022-11-10b","title":"v2022-11-10b","text":"<pre><code>      Airgap\n          Bug fix for testing internet access when firewall blocks but does not drop connections\n          Replace podman default auth file (/run/user/0/containers/auth.json) with new podman command override REGISTRY_AUTH_FILE\n          Ability to build local repo on internet facing bastion, no need to export and move to airgap\n          mirror-registry precheck - tool requires hostname be in /etc/hosts or mirror-registry crashes\n      OpenShift\n          Updated versions for CoreOS(VSphere Airgap) and logic to download correct versions   \n      WebSpher Automation\n          Bug fix - missing step to prepare input files is service.sh(step 3)  \n      AppStore\n          Added new CP4D Backup and Restore(Use case for DR)\n</code></pre>"},{"location":"release/#v2022-11-10a","title":"v2022-11-10a","text":"<pre><code>      Cloud Pak for Data\n          Added Watson OpenScale capability\n      Misc\n          Cleanup of logs for services\n</code></pre>"},{"location":"release/#v2022-11-10","title":"v2022-11-10","text":"<pre><code>      Tested OpenShift with Current Daffy Release\n          GCP       4.8.51 / 4.10.36\n          AWS       4.8.51 / 4.10.36\n          Azure     4.8.51 / 4.10.36\n          ROKS      4.8.51 / 4.10.36\n          VSphere   4.8.51 / 4.10.36\n          KVM       4.8.51 / 4.10.36\n          IBM IPI            4.10.36\n      Cloud Pak for Business Automation\n          Added Support for 22.0.1 IF004\n          Added ADS to Production Decisions Service and Starter Services decisions and content-decisions\n          Added support for Roks vpn-gen2 for step 2 and step 3(Cloud Pak and Services)\n          RPA\n                Added support for ROKS\n      Cloud Pak for Data\n          Removed Storage Vendor tag (not needed for 4.5.x)\n      Cloud Pak for Integration\n          Fixed bug with APIC that would fail when apply CR if webhook wasn't finalized\n          Updated for latest releases of 2022.4.1 services\n      OpenShift\n          Added new precheck for VSPHERE_USERNAME, it cannot contain \\\n      Misc\n          Added new Override for KVM Build options (KVM_VM_OPTION) and new default is to not enable VNC\n          Added new Override for Airgap (OCP_REGISTRY_IMAGE_EXPORT_FILE) to enable export of registry. Default is true\n          Added better error message for ROKS when logged into wrong IBM Cloud Account\n                Updated refresh.sh to allow menu choice for version\n</code></pre>"},{"location":"release/#v2022-10-18","title":"v2022-10-18","text":"<pre><code>      Tested OpenShift with Current Daffy Release\n          GCP       4.8.49 / 4.10.32\n          AWS       4.8.49 / 4.10.32\n          Azure     4.8.49 / 4.10.32\n          ROKS      4.8.49 / 4.10.32\n          VSphere   4.8.49 / 4.10.32\n          KVM       4.8.49 / 4.10.32\n          IBM IPI            4.10.32\n      Cloud Pak for Business Automation\n          Added Support for 22.0.1 IF003\n          Removed Support for 21.0.3 and all IFIXs\n          Removed Support for 22.0.1 without IFIX\n          Added Logic for Production Decisions to automate the LDAP addition to Zen\n          RPA\n                Added support for 21.0.4 and 21.0.5\n                Removed support for 21.0.2 and 21.0.3\n      Cloud Pak for Data\n          Added NEW CP4DCTL Install to Tools\n          Support for 4.5.3 (ROKS issue fixed with WKC)\n          Removed support for 4.0.x\n      Cloud Pak for WAIOps\n         Added support for V3.5\n      AppStore\n          Created new /data/daffy/appstore.sh to install appstore utilities\n          New Daffy CLI environment configurator\n      Misc\n          Updated all samples to have false value and removed the comment character\n          Added new Deployment Type - PostSale via DAFFY_DEPLOYMENT_TYPE\n          Added install of openshift-install via tools.sh\n          Fixed bug with image registry route for vsphere-ipi\n</code></pre>"},{"location":"release/#v2022-09-29","title":"v2022-09-29","text":"<pre><code>      Tested OpenShift with Current Daffy Release\n          GCP       4.8.49 / 4.10.32\n          AWS       4.8.49 / 4.10.32\n          Azure     4.8.49 / 4.10.32\n          ROKS      4.8.49 / 4.10.32\n          VSphere   4.8.49 / 4.10.32\n          KVM       4.8.49 / 4.10.32\n          IBM IPI   4.10.32\n      Cloud Pak for Business Automation\n          Added Support for 22.0.1 IF002\n          Added Support for Decision Production Service(Step 3 - Build DB and LDAP assets and Deploy Services)\n          Added new starter services all (removed old samples)\n      OpenShift\n          Added support for all IPI installs to enable Masters true (OCP_MASTER_NODES_SCHEDULABLE=true)\n          Added support to auto login to IBM Account for ROKS install (IBMCLOUD_ACCOUNT_ID=\"\")\n          Added ability to increase wait time for VSPhere UPI reboot for  (VSPHERE_IGNITION_FILES_DEPLOYMENT_WAIT_TIME=\"500\")\n          Added Support to override VSphere UPI network device (VSPHERE_NETWORK_ADAPTER=\"\")\n          Added Red Hat pull secret validation precheck\n          Upgraded RHACM to 2.6\n          Upgraded Mirror Registry to 1.2.6\n          Added support for 4.11\n          Added precheck to IBM ipi for CIS instance and domain name\n          Added support for 4.11(OCP only)\n          Added support for Manual credentials mode on GCP &amp; AWS\n          Added prechecks for bring your own VPC for GCP, AWS, &amp; Azure\n      Cloud Pak for Data\n          Added support for 4.5.2\n          Removed support for 4.0.2-4.0.5\n      Cloud Pak for Integration\n          Removed support for 2021.3.1 &amp; 2021.2.1\n      Cloud Pak for WAIOps\n          Added support for 3.4.1 &amp; 3.4.2\n          Added deployment of Infrastructure Automation component (CP4WAIOPS_DEPLOY_IA=true)\n          Reconfigured scripts to align with Daffy deployment pattern.\n              Infrastructure Automation and Event Manager will be installed with the service.sh script. AI Manager gets deployed with the base of the cloud pak.\n      IBM IPI\n          Added precheck CIS and DNS domain\n          Added VPC quota precheck that builds VPC, subnet, and instances to test for quota limits (Can be toggled off IBM_ALLOW_VPC_PRECHECK=false)\n      AppStore\n          New AppStore feature\n            New App - IBM Sterling B2B Install tool\n      Misc\n          Remove support for OCP 4.6 and 4.7 -  https://access.redhat.com/support/policy/updates/openshift\n          Updated IDS LDAP install and --console info to include search filters on output\n          During cleanup for ROKS, logout from ibmcloud\n          Added ROKS and ibm-ipi to security-cleanup tool\n          Added OpenShift Ceph Tools into any installation of OCS/ODF.\n          Added install of dos2unix util for prepare host\n          Removed install of nmon for prepare host\n          Added ROKS and IBM IPI to security-cleanup.sh\n          Added OpenShift Ceph Tools into any installation of OCS/ODF\n          Updated CloudCTL and added support to install stand alone via tools.sh\n</code></pre>"},{"location":"release/#v2022-08-17b","title":"v2022-08-17b","text":"<pre><code>      Cloud Pak for Integration\n          Fixed Daffy code issue with Platform Navigator not finishing operator install\n</code></pre>"},{"location":"release/#v2022-08-17a","title":"v2022-08-17a","text":"<pre><code>      Cloud Pak for Data\n          Fixed issue with operators for services stuck in Upgrade Pending\n          (Known CP4D issue with release of 4.5.2, breaking previous releases of 4.5.0 and 4.5.1. Would occur with Daffy, CP4D-cli, or running manually)\n</code></pre>"},{"location":"release/#v2022-08-17","title":"v2022-08-17","text":"<pre><code>      Tested OpenShift with Current Daffy Release\n          GCP       4.8.46 / 4.10.22\n          AWS       4.8.46 / 4.10.22\n          Azure     4.8.46 / 4.10.22\n          ROKS      4.8.46 / 4.10.22\n          VSphere   4.8.46 / 4.10.22\n          KVM       4.8.46 / 4.10.22\n          IBM IPI   4.10.22\n      Cloud Pak for Business Automation\n          Added Support for 22.0.1 IF001\n          Added Support for Decision Production Service(Step 2 - Namespace and base Operators)\n          Added Support for Content Production Service(Step 2 - Namespace and base Operators)\n          Added Support for Workflow Production Service(Step 2 - Namespace and base Operators)\n          Addd New RPA Server\n              MSSQL Server project and Database configured\n              OpenLDAP for RPA configured\n              Added 21.0.3\n              Added 21.0.2 IF005\n      Cloud Pak for Integration\n          Add support for each service to be in its own namespace\n      Cloud Pak for Data\n          Added Support for 4.5.0 and 4.5.1\n          Added Support to specify different storage class for each service\n          Added new service Match 360\n          Added new service Open OpenPages\n          Added new service Analytics Engine powered by Apache Spark\n          Added new service Db2 Warehouse\n          Added new service Data Privacy\n          Added new service Cognos Analytics\n          Added new service Db2 OLTP              \n      Turbonomics\n          Added Platform Operator Install\n          Added Kubeturbo Operator Metrics Collector\n      Watsion WAIOps\n          Added support for 3.4.0\n      IBM IPI\n          Added support for new platform type of IBM Cloud (ibm-ipi) OCP 4.10+\n      Misc\n          Added ability to install older version of daffy with refresh.sh (--list)\n          Fixed bug for KVM precheck on Host that has Memory &gt; 1 TB\n          Added precheck to validate the number of cluster nodes matches the number of nodes in environment file\n</code></pre>"},{"location":"release/#v2022-07-14","title":"v2022-07-14","text":"<pre><code>      Tested OpenShift with Current Daffy Release\n          GCP       4.8.42 / 4.10.17\n          AWS       4.8.42 / 4.10.17\n          Azure     4.8.42 / 4.10.17\n          ROKS      4.8.42 / 4.10.17\n          VSphere   4.8.42 / 4.10.17\n          KVM       4.8.42 / 4.10.17\n      Cloud Pak for Business Automation\n          Added 21.0.3 IF008 and IF009\n          Added 22.0.1\n          New Samples ocp-starter-ocs-all-22.0.1 and roks-starter-ibm-all-22.0.1\n      Cloud Pak for Integration\n          Updated to install via single operator per product\n          Added support for 2022.2.1\n      Cloud Pak for Data\n           Added 4.0.8 and 4.09\n      WebSphere Automation\n          Added support for v1.4\n      Cloud Pak for Security\n          Added support for v1.10\n      OpenShift\n          Added ability to enable masters as worker nodes (UPI Only) - OCP_MASTER_NODES_SCHEDULABLE=true(Default is false)\n          Added support for mirror-registry to support airgap and building of own registry(Quay mirror-registry)\n          Added support Red Hat Advanced Cluster Management (RHACM)\n          Added support for ODF overview page in OpenShift Console\n      Misc\n          Added new icons to output messages to help users with types of messages being displayed\n          Added new logging to capture all info displayed to console for each process and list log location\n          Added support to create new disk for NFS\n          Added precheck to validate cluster version matches environment file OCP version\n          Added precheck for OCS/ODF to make sure that your cluster has at least 6 workers\n          Added precheck for Cluster Name to make sure that it matches environment file value\n          Fixed bug to catch errors and exit when creating ROKS cluster\n          Fixed bug with OS check, will exit now if not running on support platform              \n          Fixed bug with OCP 4.10 and Local registry mirrors (dkikuchi@us.ibm.com)\n          Fixed bug with CP4BA using wrong variable CP_DEPLOYMENT_PLATFORM should be CP4BA_AUTO_PLATFORM (Toby.Liu@ibm.com)\n          Fixed order of OCP installs as it relates to Airgap and Local Registry Auth Info (gmarcy@us.ibm.com)\n</code></pre>"},{"location":"release/#v2022-05-23","title":"v2022-05-23","text":"<pre><code>      Tested OpenShift with Current Daffy Release\n          GCP       4.8.40\n          AWS       4.8.40\n          Azure     4.8.40\n          ROKS      4.8.36\n          VSphere   4.8.40\n          KVM       4.8.40\n      OpenShift\n          Added support for AWS Zone overrides(1,2 or 3)\n          Added support for AWS KMS keys for disk encryption (via AWS_ENABLE_KMS_KEY=true)\n          Added support for AWS to skip the AdministratorAccess precheck\n          Added AWS sample security policies to use if not allowing AdministratorAccess policy\n      VSphere\n          Added support for VSphere Data Center with more then one VSphere Cluster\n          Added ability to pause VSphere Install prior deploying ignition files (OCP_DEPLOY_ALL_IGNTIION_FILES_PAUSE=true)\n      Cloud Pak for Security\n          Added new cloud pak and Version 1.9\n      Cloud Pak for Business Automation\n          Starter Services\n              workflow - workflow-workstreams,bai,baml,baw_authoring,case,content_integration,pfs,workstreams\n          Added IF008 support\n          Added support for basic sample CP4BA CR's.  List, Status, Console support at basic non dynamic way\n          Added support for Open Prediction Service HUB(OPS)\n          Added support for OpenShift 4.9 and 4.10\n      Cloud Pak for Data\n          Renamed Service Cognos to Cognos Dashboards\n      Cloud Pak for WAIOps\n          Added support for version 3.3\n          Added AI Manager 3.3\n          Added Event Manager 3.3\n      IBM\n          TechZone flag to help with using ROKS Cluster provisioned by TechZone (ROKS_PROVIDER=techzone)\n      Misc\n          Added precheck for DNS PTR reverse lookup for UPI install with (DNSMASQ_BUILD=false)\n          **Tech Preview - Added support for Macbook bastion (Except vsphere-*, kvm-upi, db2 and ldap)\n          Added new tools process to install supporting tools outside of the daffy process. (oc,aws,gcloud,cloudctl,mustgather etc)\n          Added new mustgather process\n          Add new Variables to help track usage of daffy tool\n      Bug\n          Added logic to check for errors during prepareHost step\n          Fixed logic to test exact name for storage class, not partial name check\n          Fixed order logic to approveVCenterCert during precheck\n          Fixed channel name for CP4D Scheduling Operator based on release\n</code></pre>"},{"location":"release/#v2022-04-20c","title":"v2022-04-20c","text":"<pre><code>      OpenShift\n          Added support for AWS to skip the AdministratorAccess precheck\n          Added AWS sample security policies to use if not allowing AdministratorAccess policy\n          Skip DNS check for AWS Publish Internal\n          Support AWS OCP Internal Publish\n</code></pre>"},{"location":"release/#v2022-04-20b","title":"v2022-04-20b","text":"<pre><code>      OpenShift\n          Support Azure OCP Internal Publish\n      Cloud Pak for Data\n          Support for 4.0.8 (cloudctl and Single Catalog)\n</code></pre>"},{"location":"release/#v2022-04-20a","title":"v2022-04-20a","text":"<pre><code>      OpenShift\n          Support ability to Skip HAProxy Build\n          Support ability to Skip DNSMasq Build\n      Misc\n          Support Proxy for bastion host\n</code></pre>"},{"location":"release/#v2022-04-20","title":"v2022-04-20","text":"<pre><code>      Tested OpenShift with Current Daffy Release\n          GCP       4.8.36\n          AWS       4.8.36\n          Azure     4.8.36\n          ROKS      4.8.35\n          VSphere   4.8.36\n          KVM       4.8.36\n      Cloud Pak for Business Automation\n          Added IF007 support\n          Added support to install DB2 on Ubuntu/Redhat Linux Server\n          Added support to install IDS(LDAP) on RedHat Linux Server\n      Cloud Pak for Data\n          Precheck and only support OCP 4.8 for CPD 4.0.X\n          Fixed DV bug on ROKS  - New Db2 Tuning added(With Warning)\n          Support for 4.0.7 (cloudctl not working only Single Catalog)\n          Added new service Cognos Dashboard\n          Added new service DB2 Data Management Console\n      WebSphere Automation\n          Added support for version 1.3\n          Added an environment file for wsa\n      VSphere\n          Updated ISO images to latest version for each base version\n          Added better error message for UPI install and Missing ISO Image\n      OpenShift\n          Updated TShirt Size output to allow copy/paste to simplify overrides\n          Added support to install custom openshift-install program (TechZone request)\n          Added VSphere IPI support to specify resource pool from custom openshift-install program (TechZone request)\n          Added support for 4.10 (Note: Currently only CP4I supports 4.10)\n          Added support for OpenShift Data Foundation(ODF) for OCP 4.10 only\n      Misc\n          Added support for RHEL 8.X bastion (Except UPI)\n          Added new Security Cleanup script\n          Added new version script in the root of daffy\n          Added support for the all-in-one command to skip Cloud Pak install\n          Enhanced Secure MQ demo and documentation\n      Bug\n          Fixed master cleaup.sh if user passed wrong env file, it would then do rebuild not cleanup.\n          VSphere UPI install was broken with worker7-9 logic and haproxy and dnsmasq template files.\n          Fixed daffy-init.sh to check for root user and exit if not root\n</code></pre>"},{"location":"release/#v2022-03-31","title":"v2022-03-31","text":"<pre><code>      Tested OpenShift with Current Daffy Release\n            GCP       4.8.31\n            AWS       4.8.31\n            Azure     4.8.31\n            ROKS      4.8.31\n            VSphere   4.8.31\n            KVM       4.8.31\n      Cloud Pak for Business Automation\n            Added IF005 support\n      Cloud Pak for Data\n            Added support for Cloud Pak for Data 4.0.6\n            Added install for python3 and pip3 for &gt;= 4.0.6\n            Added build/remove of portworx storage classes\n            Added support for WKC, WML and WS on ROKS\n            Added new service Decision Optimization(DODS)\n            Added new service DataStage\n            Updated cloudctl install only supports latest version of CP4D\n      Cloud Pak for Integration\n            Added new single MQ instance\n            Added new App Connect Designer instance\n            Added new API Connect instance\n            Added new App Connect Dashboard instance\n            Added new Operations Dashboard tracing instance\n            Added new Event Streams instance\n            Added new HA MQ instance\n            Added new Asset Repository instance\n      WebSphere Automation (WSA)\n            Added new component that supports install of WebSphere Automation\n      OpenShift\n            Added support for Proxy Install\n            Added new root level cleanup.sh, that calls the ocp cleanup.sh\n            Added confirmation during cleanup.sh of OCP\n            Added logic to not restart ROKS nodes multiple times if doing more than one service at different times\n            Added support for KVM UPI to support up to 9 worker nodes\n      Misc\n            Added logic to support user overrides for cloud Pak components versions(All Supported Cloud Paks)\n            Added support for AWS custom Subnets and AMI ID overrides in OCP install\n            Added new confirmation of warranty during Daffy install\n</code></pre>"},{"location":"release/#v2022-02-15b","title":"v2022-02-15b","text":"<pre><code>      Cloud Pak for Business Automation\n            Switched from Patterns to Services terminology\n            Added logic to support user overrides for cloud Pak components versions\n</code></pre>"},{"location":"release/#v2022-02-15a","title":"v2022-02-15a","text":"<pre><code>      Automated ROKS oc login step\n      validated cluster name to be lower case and alphanumeric\n</code></pre>"},{"location":"release/#v2022-02-15","title":"v2022-02-15","text":"<pre><code>      Tested OpenShift with Current Daffy Release\n              GCP       4.8.26\n              AWS       4.8.26\n              Azure     4.8.26\n              ROKS      4.8.26\n              VSphere   4.8.26\n              KVM       4.8.26\n      Added Cloud Pak\n            Cloud Pak for Business Automation 21.0.3\n                Starter Services\n                    content - filenet,cmis,ier,tm,bai\n                    decisions - odm,bai\n                    content-decisions  - filenet,cmis,ier,tm,odm,bai\n      Added new OCP_INSTALL_TYPE\n              msp = Managed Service Provider(roks-msp)\n      Added ROKS Support\n            Build and Cleanup of ROKS Cluster\n            Install Cloud Paks on existing Cluster(Daffy did not build ROKS Cluster)\n            Supports Classic only\n      Added support for Cloud Pak for Data 4.0.5\n      Added new consoleFooter Function\n      Added logic to support local cert folder outside of Daffy and not to remove on refresh command\n      Updated refresh logic to display current version and new version during execution\n\n      Updated command line options to support multiple case options(--precheck|--Precheck, etc)\n      Updated default Storage Class and Vendor logic to support roks via env.sh files\n      Updated all Cloud Paks and Service install precheck to use prepare host function\n      Updated cp4waiops logic, renamed and removed old preinstall logic\n      Updated sample env files to new testing versions of Openshift 4.8.26\n      Updated imageContentSources.yaml for AirGap install to list more standard mirrors\n\n      Bug Fixes\n          VSphere OCP install-config.yaml - added quotes around cluster and network values\n          Before OCP Install cluster, remove ocp-install dir to remove old trans files\n          Fixed Typos and Spelling errors\n          Hide errors when sourcing .profile if it does not exist\n</code></pre>"},{"location":"release/#v2022-01-18","title":"v2022-01-18","text":"<pre><code>      Added Cloud Pak\n            Cloud Pak for WAIOps\n            Multi-Cloud Pak in single Cluster\n      Added Support for Case Install\n            Cloud Pak for Data\n            Cloud Pak for Data Services\n      Added support for Cloud Pak for Data 4.0.3 and 4.0.4\n      Added support for Cloud Pak for Data services operations tools\n      Added support for Cloud Pak for Integration 2021.4.1\n      Added Cloud Pak for Data services\n            Statistical Package for the Social Sciences(SPSS)\n            Watson Machine Learning(WML)\n      Added Support for Bastion on Windows WLS2(Ubuntu 20.04)\n      Added Daffy version to all status commands\n      Added support for VSphere restricted folder install\n      Added support for Image Registry for UPI install of OCP\n      Added support to allow full install of OpenShift and Cloud Pak from single command(all-in-one)\n\n      Updated CP4D\n            New Namespace was added and all CP4D services/operands will be deployed in separate/new namespace\n\n      Updated CP4D Services\n            Added more info to service status output\n            Added new single status command for all Daffy Supported Services\n            Added ability to install more then once service at a time\n            Added ability to remove service via cleanup function for each service\n\n      Cleaned Up logging\n              Moved more logs from tmp to log folder\n              Displayed more log names and location in output\n\n      Bug Fixes\n            VSphere install now allows for special character passwords(dalmeter@us.ibm.com)\n            Nodes were mislabeled for OpenShift Container Storage\n            Taint added to OpenShift Nodes for OpenShift Container Storage\n            Local Volumes for UPI install now have tolerance for OpenShift Container Storage nodes\n            Added Overwrite Flag for 99-worker-cp4d-crio-conf to allow for node expansions with IPI install(dkikuchi@us.ibm.com)\n            GCP removed unused roles required for precheck\n            Corrected CP4D version(dkikuchi@us.ibm.com)\n            Data Virtualization was looking for hard coded version and would not install post 4.0.2\n            KVM IP precheck only checked that IP given was part of local not not full IP\n\n      Removed Features\n            Removed support for OpenShift 4.7\n</code></pre>"},{"location":"release/#v2021-12-01","title":"v2021-12-01","text":"<pre><code>      Added Cloud Pak\n          (CP4I) Cloud Pak for Integration 2021.3.1 and 2021.2.1\n      Added Platform Support\n          AWS(aws) - IPI\n      Added OpenShift Support for 4.9\n      Added pre check logic to validate base domain and cluster name to verify valid DNS Syntax (FQDN)\n      Added support - Subdomain for base domain on KVM and VSphere Installs\n      Added support for KVM Web Dashboard - vmdashboard\n\n      Updated OpenShift Container Storage for IPI to use provider Storage Class\n          IPI Installs now have easy disk expansion from Console for OCS\n      Updated OCS to label all Storage nodes as Infrastructure(Infra) nodes\n      Update Daffy Stats URL and new arguments\n</code></pre>"},{"location":"release/#v2021-11-12-initial-release","title":"v2021-11-12 - Initial Release","text":"<pre><code>      OpenShift Supported 4.6,4.7 and 4.8\n      Cloud Pak for Data Supported 4.01\n      OpenShift Container Storage Supported\n      Platforms Supported\n               Google Cloud Provider(gcp) - IPI\n               Azure(az) - IPI\n               VSphere(vsphere) - IPI and UPI\n               KVM(kvm) - UPI\n</code></pre>"},{"location":"AppStore/","title":"App Store","text":"IBM Daffy CLI Configurator <p>Installer</p>"},{"location":"AppStore/IBMDaffyCLIConfigurator/","title":"IBM Daffy CLI Configurator","text":""},{"location":"AppStore/IBMDaffyCLIConfigurator/#info","title":"Info","text":"<p>A command line tool to build a Daffy environment file based on sets of questions. This tool was developed to simplify the creation of an environment file, figure out cluster details, copy to Daffy to run the environment file, and be able to store your environment files to your own GitHub repository.</p> <p>The Daffy CLI Environment Configurator will walk you through the things it needs for a successful install of OpenShift, CloudPaks, Services, and a combination of any of them that work with the Daffy steps.</p>"},{"location":"AppStore/IBMDaffyCLIConfigurator/#prerequisites","title":"Prerequisites:","text":"<p>You are using a Linux bastion that is supported by Daffy. You can get a bastion through TechZone or following the steps located at https://ibm.github.io/daffy/Supporting-Software/Bastion/. You have Daffy installed on your bastion. The rest of this document assumes that you have both of these.</p> <p>This tool supports starting at multiple places to meet your end goal of what you want Daffy to build. It can start by building OpenShift or you can bring your credentials for an existing cluster and it will provide details of various things for your environment file to continue to a Cloud Pak or a Service within a cloud pak.  </p> <p>For more advanced features of this tool, an existing GitHub repository is available and you have a token to login to it.</p>"},{"location":"AppStore/IBMDaffyCLIConfigurator/#ownersupport","title":"Owner/Support","text":"<p>Slack Channel #daffy-user-group</p> <p>jimholz@us.ibm.com</p>"},{"location":"AppStore/IBMDaffyCLIConfigurator/#install-command","title":"Install Command","text":"<pre><code>/data/daffy/appstore.sh --DaffyCLIConfigurator\n</code></pre>"},{"location":"AppStore/IBMDaffyCLIConfigurator/#running-the-tool","title":"Running the tool","text":"<p>The tool by default will be installed in /data/appstore/daffy-cli. It only has one command to use it. <pre><code>/data/appstore/daffy-cli/create.sh\n</code></pre></p>"},{"location":"AppStore/IBMDaffyCLIConfigurator/#other-features","title":"Other Features","text":"<p>The Daffy CLI Environment Configurator has a few features outside of just running the tool above. <pre><code>/data/appstore/daffy-cli/create.sh --help\n</code></pre></p> <p>To utilize GitHub to store your environment files, you can use these 2 commands <pre><code>/data/appstore/daffy-cli/create.sh --copytoGitHub\n</code></pre> <pre><code>/data/appstore/daffy-cli/create.sh --pullfromGitHub\n</code></pre></p>"},{"location":"AppStore/indexBeta/","title":"indexBeta","text":"IBM Daffy CLI Configurator <p>Installer</p>"},{"location":"AppStore/info/","title":"Information","text":""},{"location":"AppStore/info/#what-is-the-appstore","title":"What is the AppStore?","text":"<p>The Daffy app store is the official repository for all additional software, services, and platforms built on top of OpenShift and IBM Cloud Paks. Although these apps are maintained by developers outside of the Daffy team, they are seamlessly integrated with Daffy.</p> <p>Installation is simple, most apps can be installed in a single command via the Daffy tools script.</p> <pre><code>/data/daffy/appstore.sh\n</code></pre>"},{"location":"AppStore/info/#support-model","title":"Support Model","text":"<p>Apps are integrated with Daffy, but supported by developers outside of Daffy. For questions and troubleshooting, please contact the maintainers of the apps.</p>"},{"location":"AppStore/info/#add-your-app","title":"Add Your App","text":"<p>Getting your app added to the Daffy app store is a simple process. More info on this coming soon.</p>"},{"location":"Beta/","title":"Index","text":""},{"location":"Beta/#active-beta-features-currently-closed","title":"Active Beta Features - Currently Closed","text":"<p>Warning</p> <p>Early features - Still in testing mode, may not always work.  User be aware!!!</p> <p>To get the beta version of Daffy, run the following command on your bastion: <pre><code>curl http://get.daffy-installer.com/download-scripts/daffy-beta-init.sh | bash\n</code></pre></p> <p>Warning</p> <p>Please do not share above with others</p> <p>We update the beta code nightly. If you have the beta versions, you can run this command to get the daily updates to the beta code of daffy: <pre><code>/data/daffy/refresh-beta.sh\n</code></pre></p> <p>If you want to switch back to the production of the daffy code, you can run this: <pre><code>/data/daffy/refresh.sh\n</code></pre></p>"},{"location":"Cloud-Paks/","title":"Index","text":""},{"location":"Cloud-Paks/#cloud-paks","title":"Cloud Paks","text":"Business Automation Data Integration Watson AIOps WebSphere Automation"},{"location":"Cloud-Paks/#what-is-required-to-deploy-a-cloud-pak","title":"What is required to deploy a Cloud Pak?","text":"<p>Before you can deploy a Cloud Pak, you must have the following:</p>"},{"location":"Cloud-Paks/#you-must-have-a-cluster-running","title":"You must have a cluster running","text":"<ul> <li>An existing cluster in a supported provider</li> <li>Can be built with Daffy or any other process  </li> </ul>"},{"location":"Cloud-Paks/#ibm-entitlement-key","title":"IBM entitlement key","text":"<ul> <li>You can obtain your existing entitlement key here:</li> <li>https://myibm.ibm.com/products-services/containerlibrary</li> </ul> <p>Important: if you're installing on a customer owned platform account or an on-prem customer data center, you <code>MUST</code> instruct your customer to register for a trial account and use their pull secret for the install if they don't own the software. Do not use your own pull secret for customer engagements.</p> <p>Note</p> <p>all IBMer's are entitled to an IBM Entitlement Key. Your key can <code>ONLY</code> be used for training and demo purposes. Do not provide your personal entitlement key to others.</p> <p>If customer or business partner does not have an IBM entitlement key, go to the following link to get one (IBM Internal Link):</p> <p>IBM Trial Software Process </p> <p>Warning</p> <p>For internal IBM use only, Link below will only work while in the IBM Network</p>"},{"location":"Cloud-Paks/Business-Automation/","title":"Business Automation","text":""},{"location":"Cloud-Paks/Business-Automation/#cloud-pak-for-business-automation","title":"Cloud Pak for Business Automation","text":"<p>At this point, you have a working OCP cluster on your platform of choice. Your &lt;ENVIRONMENT_NAME&gt;-env.sh configuration file will contain details of the platform and OCP installation. You will now add the following configurations to this file:</p> <p>1) The Cloud Pak info that you wish to install</p> <p>2) The services that you wish to install on the Cloud Pak</p>"},{"location":"Cloud-Paks/Business-Automation/#step-2-deploy-cloud-pak","title":"Step 2: Deploy Cloud Pak","text":"<p>Deploying the Cloud Pak for Business Automation only requires two entries to your environment file (/data/daffy/env/  &lt;ENVIRONMENT_NAME&gt;-env.sh): You need to pick starter services and/or production services or RPA service.</p> Variable Name Info Install Type Required CP4BA_VERSION The version you want to install Both Yes CP4BA_IFIX The fix version of your version supported Both No CP4BA_DEPLOYMENT_STARTER_SERVICE The name of the service you want to deploy Starter No CP4BA_DEPLOYMENT_STARTER_SERVICE_SAMPLE The name of sample yaml you want to deploy Starter No CP4BA_DEPLOYMENT_PRODUCTION_DECISIONS true if you want to deploy decisions Production No CP4BA_DEPLOYMENT_PRODUCTION_CONTENT true if you want to deploy content Production No CP4BA_DEPLOYMENT_PRODUCTION_WORKFLOW true if you want to deploy workflow Production No <p>Valid Options:</p> Variable Name Valid Options CP4BA_VERSION 23.0.1 CP4BA_IFIX IF001 &amp; IF005 CP4BA_DEPLOYMENT_STARTER_SERVICE content,decisions,docprocessing,content-decisions,workflow,docprocessing,samples,all"},{"location":"Cloud-Paks/Business-Automation/#rpa-server","title":"RPA Server","text":"<p>Warning</p> <p>Currently does not support running RPA and Cp4BA in same cluster.  Known issue with Common Services as they are not compatible versions.</p> Variable Name Info Required CP4BA_ENABLE_SERVICE_RPA_SERVER true if you want to deploy RPA Server No CP4BA_RPA_SERVER_VERSION Version of RPA to deploy No CP4BA_RPA_SERVER_IFIX The fix version of your version supported Yes if RPA True CP4BA_RPA_SERVER_FIRST_TENANT_OWNER_EMAIL Owner Email Address Yes if RPA True CP4BA_RPA_SERVER_FIRST_TENANT_OWNER_NAME Owner Full Name Yes if RPA True CP4BA_RPA_SERVER_SMTP_USER SMTP User that RPA will use to send Email Yes if RPA True CP4BA_RPA_SERVER_SMTP_PORT SMTP Port that RPA will use to send Email Yes if RPA True CP4BA_RPA_SERVER_SMTP_SERVER SMTP Server/IP that RPA will use to send Email Yes if RPA True <p>Valid Options:</p> Variable Name Valid Options CP4BA_RPA_SERVER_VERSION 21.0.4 or 21.0.5 CP4BA_RPA_SERVER_IFIX N/A <p>You can copy the following to your &lt;ENVIRONMENT_NAME&gt;-env.sh: <pre><code>#Core CP4BA Settings\n###################################################\nCP4BA_VERSION=\"23.0.1\"\nCP4BA_IFIX=\"IF001\"\nCP4BA_DEPLOYMENT_STARTER_SERVICE=\"content\"\n\n#Prodution Services\n###################################################\nCP4BA_DEPLOYMENT_PRODUCTION_DECISIONS=\"false\"\n\n#Prodution Services - only step 2 supported today.\n###################################################\nCP4BA_DEPLOYMENT_PRODUCTION_CONTENT=\"false\"\nCP4BA_DEPLOYMENT_PRODUCTION_WORKFLOW=\"false\"\n\n#RPA Server\n############################################\nCP4BA_ENABLE_SERVICE_RPA_SERVER=\"false\"\nCP4BA_RPA_SERVER_VERSION=\"21.0.5\"\n#CP4BA_RPA_SERVER_IFIX=\"\"\n#CP4BA_RPA_SERVER_FIRST_TENANT_OWNER_EMAIL=\"daffy@us.ibm.com\"\n#CP4BA_RPA_SERVER_FIRST_TENANT_OWNER_NAME=\"Daffy Admin\"\n#CP4BA_RPA_SERVER_SMTP_USER=\"GmailID@Gmail.com\"\n#CP4BA_RPA_SERVER_SMTP_PORT=\"587\"\n#CP4BA_RPA_SERVER_SMTP_SERVER=\"gmail.smtp.com\"\n</code></pre></p>"},{"location":"Cloud-Paks/Business-Automation/#starter-service-mapping","title":"Starter Service Mapping","text":"Service Components CP4BA Version decisions odm, ads, bastudio, aae, bai 23.0.1 content filenet, cmis, ier, tm, bai 23.0.1 content-decisions filenet, cmis, ier, tm, odm, ads, bastudio, aae, ai 23.0.1 workflow workflow, workstreams, pfs, baw_authoring, case, bai 23.0.1 docprocessing docprocessing, content, cmis, css, tm 23.0.1 all All Components(except iccsap) 23.0.1 samples Depends on sample 23.0.1 <p>Run the following command to deploy the Cloud Pak for Business Automation:</p> <pre><code>/data/daffy/cp4ba/build.sh &lt;ENVIRONMENT_NAME&gt;\n</code></pre> <p>When this step is complete, approximately after 10 minutes depending on your environment, you will have the Cloud Pak running. These are just the core Cloud Pak operators, no service is running at this point. The cluster is now ready to deploy the service.  At this stage, the cluster consists of IBM Foundation Services and the Cloud Pak for Business Automation operators in the following projects based on selection above:</p> <ul> <li>cp4ba-starter</li> <li>cp4ba-starter-decisions</li> <li>cp4ba-starter-docprocessing</li> <li>cp4ba-starter-content</li> <li>cp4ba-starter-workflow</li> <li>cp4ba-content</li> <li>cp4ba-decisions</li> <li>cp4ba-workflow</li> <li>ibm-licensing (23.0.1 only)</li> <li>ibm-cert-manager (23.0.1 only)</li> </ul> HTML Video embed"},{"location":"Cloud-Paks/Business-Automation/#step-3-deploy-services","title":"Step 3: Deploy Services","text":"<p>Deploying the service does not need any new values to your environment file (&lt;ENVIRONMENT_NAME&gt;-env.sh&gt;). It will use the same values during the Cloud Pak deployment.</p> Variable Name Info Install Type Required CP4BA_VERSION The version you want to install Both Yes CP4BA_IFIX The fix version of your version support it Both No CP4BA_DEPLOYMENT_STARTER_SERVICE The name of the service you want to deploy Starter No CP4BA_DEPLOYMENT_PRODUCTION_DECISIONS true if you want to deploy decisions Production No CP4BA_DEPLOYMENT_STARTER_SERVICE_SAMPLE The name of sample yaml you want to deploy Starter No Valid Options: Variable Name Valid Options CP4BA_VERSION 23.0.1 CP4BA_IFIX IF001 &amp; IF005 CP4BA_DEPLOYMENT_STARTER_SERVICE content,decisions,docprocessing,content-decisions,workflow,all,samples <p>Instead of using the included services, you can also deploy your own sample.</p> Variable Valid Option Required CP4BA_DEPLOYMENT_STARTER_SERVICE samples No CP4BA_DEPLOYMENT_STARTER_SERVICE_SAMPLE see list below No <p>Sample Name</p> <p>The value you use is without the .yaml in the name.</p> <p>cd /data/daffy/cp4ba/templates/services/samples  </p> <p>To use samples, you would have to build your own CR yaml and store in the above directory and you would give the name of the sample.</p>"},{"location":"Cloud-Paks/Business-Automation/#rpa-server_1","title":"RPA Server","text":"<p>Warning</p> <p>Currently does not support a ROKS deployment. There is known issue with RPA Server and ticket is open with IBM Support.</p> Variable Name Info Required CP4BA_ENABLE_SERVICE_RPA_SERVER true if you want to deploy RPA Server No CP4BA_RPA_SERVER_VERSION Version of RPA to deploy Yes if RPA True CP4BA_RPA_SERVER_IFIX The fix version of your version supported Yes if RPA True CP4BA_RPA_SERVER_FIRST_TENANT_OWNER_EMAIL Owner Email Address Yes if RPA True CP4BA_RPA_SERVER_FIRST_TENANT_OWNER_NAME Owner Full Name Yes if RPA True CP4BA_RPA_SERVER_SMTP_USER SMTP User that RPA will use to send Email Yes if RPA True CP4BA_RPA_SERVER_SMTP_PORT SMTP Port that RPA will use to send Email Yes if RPA True CP4BA_RPA_SERVER_SMTP_SERVER SMTP Server/IP that RPA will use to send Email Yes if RPA True <p>You can copy the following to your &lt;ENVIRONMENT_NAME&gt;-env.sh: <pre><code>#Core CP4BA Settings\n###################################################\nCP4BA_VERSION=\"23.0.1\"\n#CP4BA_IFIX=\"\"\nCP4BA_DEPLOYMENT_STARTER_SERVICE=\"content\"\n#CP4BA_DEPLOYMENT_STARTER_SERVICE_SAMPLE=\"&lt;YourSampleHere&gt;\"\n\n#RPA Server\n############################################\nCP4BA_ENABLE_SERVICE_RPA_SERVER=\"false\"\nCP4BA_RPA_SERVER_VERSION=\"21.0.5\"\nCP4BA_RPA_SERVER_IFIX=\"\"\n#CP4BA_RPA_SERVER_FIRST_TENANT_OWNER_EMAIL=\"daffy@us.ibm.com\"\n#CP4BA_RPA_SERVER_FIRST_TENANT_OWNER_NAME=\"Daffy Admin\"\n#CP4BA_RPA_SERVER_SMTP_USER=\"GmailID@Gmail.com\"\n#CP4BA_RPA_SERVER_SMTP_PORT=587\n#CP4BA_RPA_SERVER_SMTP_SERVER=\"gmail.smtp.com\"\n</code></pre></p>"},{"location":"Cloud-Paks/Business-Automation/#starter-service-mapping_1","title":"Starter Service Mapping","text":"Service Components CP4BA Version decisions odm, ads, bastudio, aae, bai 23.0.1 content filenet, cmis, ier, tm, bai 23.0.1 ontent-decisions filenet, cmis, ier, tm, odm, ads, bastudio, aae bai 23.0.1 workflow workflow, workstreams, pfs, baw_authoring, case, bai 23.0.1 docprocessing docprocessing, content, cmis, css, tm 23.0.1 all all (except iccsap) 23.0.1 samples Depends on sample Depends on sample <p>Be aware, this step is async, meaning that the Daffy engine will deploy the service to the cluster and then complete. This only takes a few minutes to complete. When the deployment of the service script is done, the service is not running yet. Depending on your service, it can take from 1 hour to 6 to complete. You can use the status command below to watch its progress.</p>"},{"location":"Cloud-Paks/Business-Automation/#production-services","title":"Production Services","text":"<p>Options for Services</p> Service Components CP4BA Version decisions odm, ads, bastudio, aae, bai 23.0.1"},{"location":"Cloud-Paks/Business-Automation/#decisions-production","title":"Decisions Production","text":"<p>To deploy a Decisions Production Pattern, you have to have a db2 database and an IDS LDAP server. This will also include BAI.  Daffy can either use your existing assets or can build them locally where daffy is installed.  If you just want daffy to build all the needed components on your local bastion, just set the build flags below to true and daffy will build it all.</p> <p>Important</p> <p>To have daffy build your database and LDAP config info, you need to have DB2 and IDS LDAP installed locally. Instructions: DB2 and LDAP</p> Variable Name Info Required Valid Options CP4BA_DEPLOYMENT_PRODUCTION_DECISIONS Do you want to deploy Decisions? No true or false CP4BA_DEPLOYMENT_PRODUCTION_DECISIONS_BUILD_DB Do you want to deploy Decisions DB2 Database locally? No true or false CP4BA_DEPLOYMENT_PRODUCTION_DECISIONS_BUILD_LDAP Do you want to deploy Decisions LDAP locally? No true or false CP4BA_DEPLOYMENT_PRODUCTION_DECISIONS_LDAP_SERVER DNS Name or IP address for your IDS LDAP Server? No DNS or IP address CP4BA_DEPLOYMENT_PRODUCTION_DECISIONS_DC_ODM_DATABASE_SERVERNAME DNS Name or IP address for your DB2 Server No DNS or IP address <pre><code>CP4BA_DEPLOYMENT_PRODUCTION_DECISIONS=\"true\"\nCP4BA_DEPLOYMENT_PRODUCTION_DECISIONS_BUILD_DB=\"true\"\nCP4BA_DEPLOYMENT_PRODUCTION_DECISIONS_BUILD_LDAP=\"true\"\nCP4BA_DEPLOYMENT_PRODUCTION_DECISIONS_LDAP_SERVER=\"XXX.XXX.XXX.XXX\"\nCP4BA_DEPLOYMENT_PRODUCTION_DECISIONS_DC_ODM_DATABASE_SERVERNAME=\"XXX.XXX.XXX.XXX\"\n</code></pre>"},{"location":"Cloud-Paks/Business-Automation/#execute-service","title":"Execute Service","text":"<p>Run the following command to deploy the Cloud Pak for Business Automation services:</p> <pre><code>/data/daffy/cp4ba/service.sh &lt;ENVIRONMENT_NAME&gt;\n</code></pre> HTML Video embed"},{"location":"Cloud-Paks/Business-Automation/#step-3a-status","title":"Step 3a: Status","text":"<p>The service can take a few hours to complete, based on which one you chose to deploy. To help monitor the status of the service deployment, you can run the help flag to see what flags you can use to get information on your service deployment.</p> <p>Run the following commands to check the Cloud Pak for Business Automation to see what command flags you can run:</p> <pre><code>/data/daffy/cp4ba/service.sh &lt;ENVIRONMENT_NAME&gt; --help\n</code></pre> <p>The following command will give you the status of all starter and productin components for the service you deployed:</p> <pre><code>/data/daffy/cp4ba/service.sh &lt;ENVIRONMENT_NAME&gt; --Status\n</code></pre> <p>The following command will give you the status of RPA Server you deployed:</p> <pre><code>/data/daffy/cp4ba/service.sh &lt;ENVIRONMENT_NAME&gt; --RPAStatus\n</code></pre> <p>If you want to have a running job to refresh every few seconds,  you can run the above command via the watch command:</p> <pre><code>watch -c /data/daffy/cp4ba/service.sh &lt;ENVIRONMENT_NAME&gt; --Status\n</code></pre> <p>To find out the connection info to your new starter and productin  services, you can run the console flag to get user names, passwords, and URLs to connect to:</p> <pre><code>/data/daffy/cp4ba/service.sh &lt;ENVIRONMENT_NAME&gt; --Console\n</code></pre> <p>To find out the connection info to your RPA Server, you can run the console flag to get user names, passwords, and URLs to connect to:</p> <pre><code>/data/daffy/cp4ba/service.sh &lt;ENVIRONMENT_NAME&gt; --RPAConsole\n</code></pre> HTML Video embed"},{"location":"Cloud-Paks/Business-Automation/#post-daffy-steps","title":"Post Daffy Steps","text":""},{"location":"Cloud-Paks/Business-Automation/#rpa-server_2","title":"RPA Server","text":""},{"location":"Cloud-Paks/Business-Automation/#openldap-config","title":"OpenLdap Config","text":"<p>Once you have installed RPA server, you will need add the LDAP Server from the Cloud Pak Dashboard.  The following steps will help you manually preform these steps.  </p> <p>The details for the next steps will come when you install Step 3 of Daffy for RPA Server, via your command line console.</p> Screenshot <p></p> <p>1) Login Cloud Pak Dashboard Link via \"IBM provided credentials(admin only)\"</p> Screenshot <p></p> <p>2) From the hamburger menu bar, under Administration, select Access Control</p> Screenshot <p></p> <p>3) Click \"Add users\"</p> Screenshot <p></p> <p>4) Add your RPA user and give Access to all roles.</p> <p>5) Logout of the Cloud Pak dashboard and close your browser.</p> <p>At this point, you are ready to logon to your RPA Server Console.</p>"},{"location":"Cloud-Paks/Business-Automation/#decisions-server","title":"Decisions Server","text":"<p>Once you have installed Production Decisions Server pattern, you will need to do a few steps.</p> <ol> <li>Map Your LDAP Groups to IDP Roles</li> <li>Install and Configure your Rule Designer</li> </ol>"},{"location":"Cloud-Paks/Business-Automation/#map-ldap-groups-to-roles","title":"Map LDAP groups to Roles","text":"<p>Run the following command to Import and Map your LDAP groups to Zen roles</p> <pre><code>/data/daffy/cp4ba/service.sh &lt;env_name&gt; --decisionImportLDAPGroups\n</code></pre> Screenshot <p></p>"},{"location":"Cloud-Paks/Business-Automation/#rule-designer","title":"Rule DesignerHere is how you can connect to your new Decision Center <ul> <li>Right Click your Rule Project</li> <li>Select Decision Center | connect</li> </ul> Fill out from based on daffy output from --console <ol> <li>URL:             Decision Center</li> <li>Authentication:  Zen API Key</li> <li>User ID:         Decisions Admin Username</li> <li>API Key:         Decisions Admin Zen API Key</li> </ol>    Screenshot   <p>d. Click Next and then Finish</p>","text":"<p>After you installed Decisions Services, you need to install and connect Rule Designer to your new instance. For the next steps, any information you need from your environment you should be able to collect from the service.sh --console  command output of Daffy.</p> <p>Important</p> <p>Original Instructions can be found here</p> <p>1) Download and install Eclipse. Download Eclipse</p> Screenshot <p></p> <p>2) Install ODM from Marketplace.</p> <p>a. Start Eclipse. Click Help &gt; Eclipse Marketplace.</p> <p>b. In the Find field, enter the text ODM and click Go.</p> <p>c. Locate the entry IBM Operational Decision Manager for Developers v8.11.0 - Rule Designer that matches the version to install, and then click Install. </p> Screenshot <p></p> <p>3) Download truststore.jks from your cluster</p> Screenshot <p></p> <p>4) Update your Eclipse.ini and add these lines at the end:(update path info based on your setup)</p> <pre><code>-Djavax.net.ssl.trustStore=C:/Users/Administrator/Desktop/MyTrustStores/truststore.jks\n-Djavax.net.ssl.trustStorePassword=changeit\n</code></pre> <p>5) Get the Zen Key API from the CPD console</p> <p>Important</p> <p>Before the Zen API Key can be generated, you must Map LDAP groups to Roles from above and then via Browser, logon to the Cloud Pak Decisions Desktop once.</p> Screenshot <p></p> <p>6) Connect Rule Designer</p>"},{"location":"Cloud-Paks/Data/","title":"Data","text":""},{"location":"Cloud-Paks/Data/#cloud-pak-for-data","title":"Cloud Pak for Data","text":"<p>At this point, you have a working OCP cluster on your platform of choice. Your &lt;ENVIRONMENT_NAME&gt;-env.sh configuration file will contain details of the platform and OCP installation. You will now add the following details to your env file:</p> <p>1) The Cloud Pak info that you wish to install</p> <p>2) The services that you wish to install on the Cloud Pak</p>"},{"location":"Cloud-Paks/Data/#step-2-deploy-cloud-pak","title":"Step 2: Deploy Cloud Pak","text":"<p>Deploying the Cloud Pak for Data requires one entry to your environment file (/data/daffy/env/&lt;ENVIRONMENT_NAME&gt;-env.sh).</p> <p>CP4D_VERSION=</p> <p>You can copy the following to your &lt;ENVIRONMENT_NAME&gt;-env.sh:</p> <pre><code>CP4D_VERSION=\"4.7.4\"\n</code></pre> <p>With these values, the Daffy engine will be able to install the version of Cloud Pak for Data and prepare for the desired services.</p> CP4D Supported Version OCP Versions 4.7.4 4.12 4.7.3 4.12 4.7.2 4.12 4.7.1 4.12 4.7.0 4.12 <p>Run the following command to deploy the Cloud Pak for Data:</p> <p><pre><code>/data/daffy/cp4d/build.sh &lt;ENVIRONMENT_NAME&gt;\n</code></pre> When this step is complete, approximately after 60 minutes depending on your environment, you will have the Cloud Pak running. These are just the core Cloud Pak operators, no service/pattern is running at this point. The cluster is now ready to deploy the services/patterns.  At this stage, the cluster consists of bedrock operators and the Cloud Pak for Data operators in the following projects:</p> <p>cpd-instance cpd-operators ibm-common-services </p>"},{"location":"Cloud-Paks/Data/#step-3-deploy-services","title":"Step 3: Deploy Services","text":"<p>Set the flags in your environment file (&lt;ENVIRONMENT_NAME&gt;-env.sh) for the CP4D services you wish to deploy.</p> Variable Name Value's Info Required CP4D_ENABLE_SERVICE_WKS true / false Watson Knowledge Studio No CP4D_ENABLE_SERVICE_WKC true / false Watson Knowledge Catalog No CP4D_ENABLE_SERVICE_DV true / false Data Virtualization No CP4D_ENABLE_SERVICE_WS true / false Watson Studio No CP4D_ENABLE_SERVICE_SPSS true / false Statistical Package for Social Sciences No CP4D_ENABLE_SERVICE_WML true / false Watson Machine Learning No CP4D_ENABLE_SERVICE_DATASTAGE true / false DataStage No CP4D_ENABLE_SERVICE_DODS true / false Decision Optimization No CP4D_ENABLE_SERVICE_DMC true / false DB2 Management Console No CP4D_ENABLE_SERVICE_COGNOS true / false Cognos No CP4D_ENABLE_SERVICE_MATCH_360 true / false Match 360 No CP4D_ENABLE_SERVICE_OPENPAGES true / false Open Pages No CP4D_ENABLE_SERVICE_ANALYTICS_ENGINE true / false Analytics Engine powered by Apache Spark No CP4D_ENABLE_SERVICE_DB2_WAREHOUSE true / false DB2 Warehouse No CP4D_ENABLE_SERVICE_DATAPRIVACY true / false Data Privacy No CP4D_ENABLE_SERVICE_COGNOS_ANALYTICS true / false Cognos Analytics No CP4D_ENABLE_SERVICE_DB2 true / false DB2 OLTP No CP4D_ENABLE_SERVICE_WATSON_OPENSCALE true / false Watson OpenScale No CP4D_ENABLE_SERVICE_WS_PIPELINES true / false Watson Pipelines No CP4D_ENABLE_SERVICE_FACTSHEETS true / false AI FactSheets No CP4D_ENABLE_SERVICE_REPLICATION true / false Data Replication No CP4D_ENABLE_SERVICE_WATSONX_DATA true / false Data Replication No <p>You can copy the following to your &lt;ENVIRONMENT_NAME&gt;-env.sh:</p> <pre><code>CP4D_ENABLE_SERVICE_WKS=\"false\"\nCP4D_ENABLE_SERVICE_WKC=\"false\"\nCP4D_ENABLE_SERVICE_DV=\"false\"\nCP4D_ENABLE_SERVICE_SPSS=\"false\"\nCP4D_ENABLE_SERVICE_WS=\"false\"\nCP4D_ENABLE_SERVICE_WML=\"false\"\nCP4D_ENABLE_SERVICE_DATASTAGE=\"false\"\nCP4D_ENABLE_SERVICE_DODS=\"false\"\nCP4D_ENABLE_SERVICE_DMC=\"false\"\nCP4D_ENABLE_SERVICE_COGNOS_DASHBOARDS=\"false\"\nCP4D_ENABLE_SERVICE_MATCH_360=\"false\"\nCP4D_ENABLE_SERVICE_OPENPAGES=\"false\"\nCP4D_ENABLE_SERVICE_ANALYTICS_ENGINE=\"false\"\nCP4D_ENABLE_SERVICE_DB2_WAREHOUSE=\"false\"\nCP4D_ENABLE_SERVICE_DATAPRIVACY=\"false\"\nCP4D_ENABLE_SERVICE_COGNOS_ANALYTICS=\"false\"\nCP4D_ENABLE_SERVICE_DB2=\"false\"\nCP4D_ENABLE_SERVICE_WATSON_OPENSCALE=\"false\"\nCP4D_ENABLE_SERVICE_WS_PIPELINES=\"false\"\nCP4D_ENABLE_SERVICE_FACTSHEETS=\"false\"\nCP4D_ENABLE_SERVICE_REPLICATION=\"false\"\nCP4D_ENABLE_SERVICE_WATSONX_DATA=\"false\"\n</code></pre> <p>Run the following command to deploy the Cloud Pak for Data services:</p> <pre><code>/data/daffy/cp4d/service.sh &lt;ENVIRONMENT_NAME&gt;\n</code></pre>"},{"location":"Cloud-Paks/Data/#step-3a-status","title":"Step 3a: Status","text":"<p>The service can take a few hours to complete, based on which one you chose to deploy. To help monitor the status of the service deployment you can run the --help flag to see what flags you can use to get information on your service deployment.</p> <p>Run the following commands to check the Cloud Pak for Data to see what command flags you can run: <pre><code>/data/daffy/cp4d/service.sh &lt;ENVIRONMENT_NAME&gt; --help\n</code></pre> The following command will give you the status of all components for the pattern you deployed:</p> <pre><code>/data/daffy/cp4d/service.sh &lt;ENVIRONMENT_NAME&gt; --AllStatus\n</code></pre> <p>If you want to have a running job to refresh every few seconds,  you can run the above command via the watch command:</p> <p><pre><code>watch -c /data/daffy/cp4d/service.sh &lt;ENVIRONMENT_NAME&gt; --AllStatus\n</code></pre> If you want to want to see more detail status on an individual service, you can run each service status:</p> <p><pre><code>/data/daffy/cp4d/service.sh &lt;ENVIRONMENT_NAME&gt; --WKCStatus\n</code></pre> <pre><code>/data/daffy/cp4d/service.sh &lt;ENVIRONMENT_NAME&gt; --WKSStatus\n</code></pre> <pre><code>/data/daffy/cp4d/service.sh &lt;ENVIRONMENT_NAME&gt; --WSStatus\n</code></pre> <pre><code>/data/daffy/cp4d/service.sh &lt;ENVIRONMENT_NAME&gt; --DVStatus\n</code></pre> <pre><code>/data/daffy/cp4d/service.sh &lt;ENVIRONMENT_NAME&gt; --WMLStatus\n</code></pre> <pre><code>/data/daffy/cp4d/service.sh &lt;ENVIRONMENT_NAME&gt; --SPSSStatus\n</code></pre> <pre><code>/data/daffy/cp4d/service.sh &lt;ENVIRONMENT_NAME&gt; --DataStageStatus\n</code></pre> <pre><code>/data/daffy/cp4d/service.sh &lt;ENVIRONMENT_NAME&gt; --DODSStatus\n</code></pre> <pre><code>/data/daffy/cp4d/service.sh &lt;ENVIRONMENT_NAME&gt; --Match360Status\n</code></pre> <pre><code>/data/daffy/cp4d/service.sh &lt;ENVIRONMENT_NAME&gt; --OpenPagesStatus\n</code></pre> <pre><code>/data/daffy/cp4d/service.sh &lt;ENVIRONMENT_NAME&gt; --AnalyticsEngineStatus\n</code></pre> <pre><code>/data/daffy/cp4d/service.sh &lt;ENVIRONMENT_NAME&gt; --DB2WarehouseStatus\n</code></pre> <pre><code>/data/daffy/cp4d/service.sh &lt;ENVIRONMENT_NAME&gt; --DataPrivacyStatus\n</code></pre> <pre><code>/data/daffy/cp4d/service.sh &lt;ENVIRONMENT_NAME&gt; --CognosAnalyticsStatus\n</code></pre> <pre><code>/data/daffy/cp4d/service.sh &lt;ENVIRONMENT_NAME&gt; --DB2Status\n</code></pre> <pre><code>/data/daffy/cp4d/service.sh &lt;ENVIRONMENT_NAME&gt; --OpenscaleStatus\n</code></pre> <pre><code>/data/daffy/cp4d/service.sh &lt;ENVIRONMENT_NAME&gt; --WSPipelinesStatus\n</code></pre> <pre><code>/data/daffy/cp4d/service.sh &lt;ENVIRONMENT_NAME&gt; --FactsheetStatus\n</code></pre> <pre><code>/data/daffy/cp4d/service.sh &lt;ENVIRONMENT_NAME&gt; --ReplicationStatus\n</code></pre> <pre><code>/data/daffy/cp4d/service.sh &lt;ENVIRONMENT_NAME&gt; --WatsonxDataStatus\n</code></pre> <pre><code>/data/daffy/cp4d/build.sh &lt;ENVIRONMENT_NAME&gt; --Console\n</code></pre></p> <p></p>"},{"location":"Cloud-Paks/Data/#day-2-operations-upgrade","title":"Day 2 Operations: Upgrade","text":"<p>Read the documentation for complete instructions and help - https://www.ibm.com/docs/en/cloud-paks/cp-data/4.6.x?topic=upgrading. This assumes you have an environment file filled out with the correct information about the storage, services, and current cloud pak version.</p> <p>Step 1: Update the cpd-cli</p> <p>Run the following command to download the Cloud Pak for Data command line utility and choose the version you want to upgrade to: <pre><code>/data/daffy/tools.sh --installCP4DCloudCLI\n</code></pre></p> <p>Step 2: Login to the cpd-cli</p> <p>Run the following command to login using the command line utility previously (must already be logged into OpenShift): <pre><code>/data/daffy/tmp/cpdcli/&lt;CPDCLI_VERSION_INFO&gt;/cpd-cli manage login-to-ocp --token=$(oc whoami -t) --server=&lt;OPENSHIFT_API_ADDRESS&gt;\n</code></pre></p> <p>Step 3: Export the CPD variables for base cloud pak</p> <p>Run the following command to get the cp4d variables exported for the base cloud pak platform: <pre><code>/data/daffy/cp4d/build.sh &lt;ENVIRONMENT_NAME&gt; --exportcpdvars\n</code></pre></p> <p>Step 4: Update the version in your variables environment to match the version you want to get to. The file will be located in /data/daffy/tmp//cp4d/cpd_vars.sh. Find the line export VERSION=4.6.x <p>Step 5: Check to make sure the status of the components all say completed.</p> <p>Run the following commands to update the base cloud pak OLM objects <pre><code>source /data/daffy/tmp/&lt;CLUSTER_NAME&gt;/cp4d/cpd_vars.sh\n/data/daffy/tmp/cpdcli/&lt;CPDCLI_VERSION_INFO&gt;/cpd-cli manage get-cr-status --cpd_instance_ns=${PROJECT_CPD_INSTANCE}\n</code></pre></p> <p>Step 6: Update the base cloud pak (OLM Objects)</p> <p>Run the following commands to update the base cloud pak OLM objects <pre><code>/data/daffy/tmp/cpdcli/&lt;CPDCLI_VERSION_INFO&gt;/cpd-cli manage apply-olm --release=${VERSION} --cpd_operator_ns=${PROJECT_CPD_OPS} --upgrade=true\n</code></pre></p> <p>Step 7: Update the base cloud pak services</p> <p>Run the following command to update base cloud pak services: <pre><code>/data/daffy/tmp/cpdcli/&lt;CPDCLI_VERSION_INFO&gt;/cpd-cli manage apply-cr \\\n--components=${COMPONENTS} \\\n--release=${VERSION} \\\n--cpd_instance_ns=${PROJECT_CPD_INSTANCE} \\\n--block_storage_class=${STG_CLASS_BLOCK} \\\n--file_storage_class=${STG_CLASS_FILE} \\\n--license_acceptance=true \\\n--upgrade=true\n</code></pre></p> <p>Step 8: Export the services cp4d variables</p> <p>Warning</p> <p>This will overwrite the cpd_vars file from the platform steps and will only look at the components/services installed. Make sure you update the version again as in step 4.</p> <p>Run the following command to get the cp4d variables exported for the cloud pak services that are installed: <pre><code>/data/daffy/cp4d/service.sh &lt;ENVIRONMENT_NAME&gt; --exportcpdvars\n</code></pre></p> <p>Step 9: Upgrade the components/services</p> <p>Run the following command to update cloud pak services: <pre><code>source /data/daffy/tmp/&lt;CLUSTER_NAME&gt;/cp4d/cpd_vars.sh\n/data/daffy/tmp/cpdcli/&lt;CPDCLI_VERSION_INFO&gt;/cpd-cli manage apply-cr \\\n--components=${COMPONENTS} \\\n--release=${VERSION} \\\n--cpd_instance_ns=${PROJECT_CPD_INSTANCE} \\\n--block_storage_class=${STG_CLASS_BLOCK} \\\n--file_storage_class=${STG_CLASS_FILE} \\\n--license_acceptance=true \\\n--upgrade=true\n</code></pre></p> <p>Warning</p> <p>If Step 9 fails for some reason, run the following command and then step 8 again.</p> <pre><code>source /data/daffy/tmp/&lt;CLUSTER_NAME&gt;/cp4d/cpd_vars.sh\n/data/daffy/tmp/cpdcli/&lt;CPDCLI_VERSION_INFO&gt;/cpd-cli manage restart-container\n</code></pre>"},{"location":"Cloud-Paks/DataBeta/","title":"DataBeta","text":""},{"location":"Cloud-Paks/DataBeta/#cloud-pak-for-data","title":"Cloud Pak for Data","text":"<p>At this point, you have a working OCP cluster on your platform of choice. Your &lt;ENVIRONMENT_NAME&gt;-env.sh configuration file will contain details of the platform and OCP installation. You will now add the following details to your env file:</p> <p>1) The Cloud Pak info that you wish to install</p> <p>2) The services that you wish to install on the Cloud Pak</p>"},{"location":"Cloud-Paks/DataBeta/#step-2-deploy-cloud-pak","title":"Step 2: Deploy Cloud Pak","text":"<p>Deploying the Cloud Pak for Data requires one entry to your environment file (/data/daffy/env/&lt;ENVIRONMENT_NAME&gt;-env.sh).</p> <p>CP4D_VERSION=</p> <p>You can copy the following to your &lt;ENVIRONMENT_NAME&gt;-env.sh:</p> <pre><code>CP4D_VERSION=\"4.5.3\"\n</code></pre> <p>With these values, the Daffy engine will be able to install the version of Cloud Pak for Data and prepare for the desired services.</p> CP4D Supported Version OCP Versions 4.5.3 4.8, 4.10 4.5.2 4.8, 4.10 4.5.1 4.8, 4.10 4.5.0 4.8, 4.10 <p>Run the following command to deploy the Cloud Pak for Data:</p> <p><pre><code>/data/daffy/cp4d/build.sh &lt;ENVIRONMENT_NAME&gt;\n</code></pre> When this step is complete, approximately after 60 minutes depending on your environment, you will have the Cloud Pak running. These are just the core Cloud Pak operators, no service/pattern is running at this point. The cluster is now ready to deploy the services/patterns.  At this stage, the cluster consists of bedrock operators and the Cloud Pak for Data operators in the following projects:</p> <p>cpd-instance cpd-operators ibm-common-services </p>"},{"location":"Cloud-Paks/DataBeta/#step-3-deploy-services","title":"Step 3: Deploy Services","text":"<p>Set the flags in your environment file (&lt;ENVIRONMENT_NAME&gt;-env.sh) for the CP4D services you wish to deploy.</p> Variable Name Value's Info Required CP4D_ENABLE_SERVICE_WKS true / false Watson Knowledge Studio No CP4D_ENABLE_SERVICE_WKC true / false Watson Knowledge Catalog No CP4D_ENABLE_SERVICE_DV true / false Data Virtualization No CP4D_ENABLE_SERVICE_WS true / false Watson Studio No CP4D_ENABLE_SERVICE_SPSS true / false Statistical Package for Social Sciences No CP4D_ENABLE_SERVICE_WML true / false Watson Machine Learning No CP4D_ENABLE_SERVICE_DATASTAGE true / false DataStage No CP4D_ENABLE_SERVICE_DODS true / false Decision Optimization No CP4D_ENABLE_SERVICE_DMC true / false DB2 Management Console No CP4D_ENABLE_SERVICE_COGNOS true / false Cognos No CP4D_ENABLE_SERVICE_MATCH_360 true / false Match 360 No CP4D_ENABLE_SERVICE_OPENPAGES true / false Open Pages No CP4D_ENABLE_SERVICE_ANALYTICS_ENGINE true / false Analytics Engine powered by Apache Spark No CP4D_ENABLE_SERVICE_DB2_WAREHOUSE true / false DB2 Warehouse No CP4D_ENABLE_SERVICE_DATAPRIVACY true / false Data Privacy No CP4D_ENABLE_SERVICE_COGNOS_ANALYTICS true / false Cognos Analytics No CP4D_ENABLE_SERVICE_DB2 true / false DB2 OLTP No <p>You can copy the following to your &lt;ENVIRONMENT_NAME&gt;-env.sh:</p> <pre><code>CP4D_ENABLE_SERVICE_WKS=\"false\"\nCP4D_ENABLE_SERVICE_WKC=\"false\"\nCP4D_ENABLE_SERVICE_DV=\"false\"\nCP4D_ENABLE_SERVICE_SPSS=\"false\"\nCP4D_ENABLE_SERVICE_WS=\"false\"\nCP4D_ENABLE_SERVICE_WML=\"false\"\nCP4D_ENABLE_SERVICE_DATASTAGE=\"false\"\nCP4D_ENABLE_SERVICE_DODS=\"false\"\nCP4D_ENABLE_SERVICE_DMC=\"false\"\nCP4D_ENABLE_SERVICE_COGNOS_DASHBOARDS=\"false\"\nCP4D_ENABLE_SERVICE_MATCH_360=\"false\"\nCP4D_ENABLE_SERVICE_OPENPAGES=\"false\"\nCP4D_ENABLE_SERVICE_ANALYTICS_ENGINE=\"false\"\nCP4D_ENABLE_SERVICE_DB2_WAREHOUSE=\"false\"\nCP4D_ENABLE_SERVICE_DATAPRIVACY=\"false\"\nCP4D_ENABLE_SERVICE_COGNOS_ANALYTICS=\"false\"\nCP4D_ENABLE_SERVICE_DB2=\"false\"\n</code></pre> <p>Run the following command to deploy the Cloud Pak for Data services:</p> <pre><code>/data/daffy/cp4d/service.sh &lt;ENVIRONMENT_NAME&gt;\n</code></pre>"},{"location":"Cloud-Paks/DataBeta/#step-3a-status","title":"Step 3a: Status","text":"<p>The service can take a few hours to complete, based on which one you chose to deploy. To help monitor the status of the service deployment you can run the --help flag to see what flags you can use to get information on your service deployment.</p> <p>Run the following commands to check the Cloud Pak for Data to see what command flags you can run: <pre><code>/data/daffy/cp4d/service.sh &lt;ENVIRONMENT_NAME&gt; --help\n</code></pre> The following command will give you the status of all components for the pattern you deployed:</p> <pre><code>/data/daffy/cp4d/service.sh &lt;ENVIRONMENT_NAME&gt; --AllStatus\n</code></pre> <p>If you want to have a running job to refresh every few seconds,  you can run the above command via the watch command:</p> <p><pre><code>watch -c /data/daffy/cp4d/service.sh &lt;ENVIRONMENT_NAME&gt; --AllStatus\n</code></pre> If you want to want to see more detail status on an individual service, you can run each service status:</p> <p><pre><code>/data/daffy/cp4d/service.sh &lt;ENVIRONMENT_NAME&gt; --WKCStatus\n</code></pre> <pre><code>/data/daffy/cp4d/service.sh &lt;ENVIRONMENT_NAME&gt; --WKSStatus\n</code></pre> <pre><code>/data/daffy/cp4d/service.sh &lt;ENVIRONMENT_NAME&gt; --WSStatus\n</code></pre> <pre><code>/data/daffy/cp4d/service.sh &lt;ENVIRONMENT_NAME&gt; --DVStatus\n</code></pre> <pre><code>/data/daffy/cp4d/service.sh &lt;ENVIRONMENT_NAME&gt; --WMLStatus\n</code></pre> <pre><code>/data/daffy/cp4d/service.sh &lt;ENVIRONMENT_NAME&gt; --SPSSStatus\n</code></pre> <pre><code>/data/daffy/cp4d/service.sh &lt;ENVIRONMENT_NAME&gt; --DataStageStatus\n</code></pre> <pre><code>/data/daffy/cp4d/service.sh &lt;ENVIRONMENT_NAME&gt; --DODSStatus\n</code></pre> <pre><code>/data/daffy/cp4d/service.sh &lt;ENVIRONMENT_NAME&gt; --Match360Status\n</code></pre> <pre><code>/data/daffy/cp4d/service.sh &lt;ENVIRONMENT_NAME&gt; --OpenPagesStatus\n</code></pre> <pre><code>/data/daffy/cp4d/service.sh &lt;ENVIRONMENT_NAME&gt; --AnalyticsEngineStatus\n</code></pre> <pre><code>/data/daffy/cp4d/service.sh &lt;ENVIRONMENT_NAME&gt; --DB2WarehouseStatus\n</code></pre> <pre><code>/data/daffy/cp4d/service.sh &lt;ENVIRONMENT_NAME&gt; --DataPrivacyStatus\n</code></pre> <pre><code>/data/daffy/cp4d/service.sh &lt;ENVIRONMENT_NAME&gt; --CognosAnalyticsStatus\n</code></pre> <pre><code>/data/daffy/cp4d/service.sh &lt;ENVIRONMENT_NAME&gt; --DB2Status\n</code></pre> <pre><code>/data/daffy/cp4d/build.sh &lt;ENVIRONMENT_NAME&gt; --Console\n</code></pre></p> <p></p>"},{"location":"Cloud-Paks/Integration/","title":"Integration","text":""},{"location":"Cloud-Paks/Integration/#cloud-pak-for-integration","title":"Cloud Pak for Integration","text":"<p>At this point, you have a working OCP cluster on your platform of choice. Your &lt;ENVIRONMENT_NAME&gt;-env.sh configuration file will contain details of the platform and OCP installation. You will now add to this file, the details of:</p> <p>1) The Cloud Pak info that you wish to install</p> <p>2) The services that you wish to install on the Cloud Pak</p>"},{"location":"Cloud-Paks/Integration/#step-2-deploy-cloud-pak","title":"Step 2: Deploy Cloud Pak","text":"<p>Deploying the Cloud Pak for Integration only requires one entry to your environment file (/data/daffy/env/&lt;ENVIRONMENT_NAME&gt;-env.sh)</p> <p>CP4I_VERSION=2023.4.1</p> <p>You can copy the following to your &lt;ENVIRONMENT_NAME&gt;-env.sh:</p> <pre><code>CP4I_VERSION=\"2023.4.1\"\n</code></pre> <p>With this one value, the Daffy engine will be able to install the version of Cloud Pak for Integration and the Platform Navigator.</p> <p>The service consist of the following products:</p> <p>platform navigator</p> Integration Supported Version OCP Versions 2023.4.1 4.12 &amp; 4.14 2023.2.1 4.12 2022.4.1 4.12 2022.2.1 4.12 <p>Run the following command to deploy the Cloud Pak for Integration:</p> <pre><code>/data/daffy/cp4i/build.sh &lt;ENVIRONMENT_NAME&gt;\n</code></pre> <p>When this step is complete, up to an hour depending on your environment, you will have the Cloud Pak running. This will install all of the Cloud Pak operators including foundational services and the Platform Navigator. The cluster is now ready to deploy additional services/patterns.  At this stage, the cluster consists of common services and the Cloud Pak for Integration operators and some services in the following projects:</p> <p>cp4i</p> <p>ibm-common-services</p>"},{"location":"Cloud-Paks/Integration/#step-3-deploy-services","title":"Step 3: Deploy Services","text":"<p>Deploying services within the Cloud Pak for Integration requires you to set the flags within the environment file (&lt;ENVIRONMENT_NAME&gt;-env.sh).</p> <p>With these values, the Daffy engine will be able to install the version of Cloud Pak for Integration and prepare for the desired services.</p> Variable Name Value's Info Required CP4I_ENABLE_SERVICE_ACEDESIGN true / false App Connect Designer No CP4I_ENABLE_SERVICE_ACEDASH true / false App Connect Dashboard No CP4I_ENABLE_SERVICE_ASSETREPO true / false Integration Asset Repository No CP4I_ENABLE_SERVICE_TRACING true / false Operations Dashboard Tracing No CP4I_ENABLE_SERVICE_MQSINGLE true / false Single Instance of MQ No CP4I_ENABLE_SERVICE_APIC true / false API Connect No CP4I_ENABLE_SERVICE_MQHA true / false Cloud Native MQ HA No CP4I_ENABLE_SERVICE_EVENTSTREAMS true / false Event Streams No CP4I_ENABLE_SERVICE_ENDPOINT_MGMT true / false Event Endpoint Management No <p>Run the following command to deploy the Cloud Pak for Integration services:</p> <pre><code>/data/daffy/cp4i/service.sh &lt;ENVIRONMENT_NAME&gt;\n</code></pre>"},{"location":"Cloud-Paks/Integration/#step-3a-status","title":"Step 3a: Status","text":"<p>The service can take a few hours to complete, based on which one you chose to deploy. To help monitor the status of the CP4I service deployment you can run the help flag to see what flags you can use to get information on your service deployment:</p> <pre><code>/data/daffy/cp4i/service.sh &lt;ENVIRONMENT_NAME&gt; --help\n</code></pre> <p>Run the following commands to check the Cloud Pak for Integration services installation progress:</p> <pre><code>/data/daffy/cp4i/service.sh &lt;ENVIRONMENT_NAME&gt; --AllStatus\n</code></pre> <p>If you want to want to see more detail status on an individual service, you can run each service status:</p> <p><pre><code>/data/daffy/cp4i/service.sh &lt;ENVIRONMENT_NAME&gt; --AceDashStatus\n</code></pre> <pre><code>/data/daffy/cp4i/service.sh &lt;ENVIRONMENT_NAME&gt; --AceDesignStatus\n</code></pre> <pre><code>/data/daffy/cp4i/service.sh &lt;ENVIRONMENT_NAME&gt; --AssetRepoStatus\n</code></pre> <pre><code>/data/daffy/cp4i/service.sh &lt;ENVIRONMENT_NAME&gt; --TracingStatus\n</code></pre> <pre><code>/data/daffy/cp4i/service.sh &lt;ENVIRONMENT_NAME&gt; --MQSingleStatus\n</code></pre> <pre><code>/data/daffy/cp4i/service.sh &lt;ENVIRONMENT_NAME&gt; --APICStatus\n</code></pre> <pre><code>/data/daffy/cp4i/service.sh &lt;ENVIRONMENT_NAME&gt; --MQHAStatus\n</code></pre> <pre><code>/data/daffy/cp4i/service.sh &lt;ENVIRONMENT_NAME&gt; --EventStreamsStatus\n</code></pre> <pre><code>/data/daffy/cp4i/service.sh &lt;ENVIRONMENT_NAME&gt; --EEMStatus\n</code></pre> To find out the connection info to your Integration Platform Navigator instance, you can run the console flag to get user names, passwords, and URLs to connect to:</p> <pre><code>/data/daffy/cp4i/build.sh &lt;ENVIRONMENT_NAME&gt; --console\n</code></pre>"},{"location":"Cloud-Paks/Watson-AIOPS/","title":"Watson AIOPS","text":""},{"location":"Cloud-Paks/Watson-AIOPS/#cloud-pak-for-watson-ai-ops","title":"Cloud Pak for Watson AI Ops","text":"<p>At this point, you should have a working OCP cluster on your platform of choice. Your &lt;ENVIRONMENT_NAME&gt;-env.sh configuration file will contain details of the platform and OCP installation. To install CP4WAIOPS, you must add some additional information to the env file. Below is a description of the information, you wll need to add.</p> <p>1) Version of the CP4WAIOPS product to install</p> <p>2) The CP4WAIOPS services that you wish to install</p> <p>Daffy automation scripts currently support the deployment of</p> <ul> <li>AI Manager - (This is installed with the cp4waiops/build.sh script)</li> <li>Event Manager - This is an optional component that can be installed with the service.sh scrpt. You must set the install fag to true before you run the service.sh script.</li> </ul> <p>Note</p> <p>The Daffy Event Manager deployment script ONLY installs the operator. You must configure and deploy the custom resource.  </p> <ul> <li>Infrastructure Automation - This is an optional component that can be installed with the service.sh scrpt. You must set the install fag to true before you run the service.sh script.</li> </ul>"},{"location":"Cloud-Paks/Watson-AIOPS/#step-2-deploy-cloud-pak-for-waiops-ai-manager","title":"Step 2: Deploy Cloud Pak for WAIOPS + AI Manager","text":"<p>Deploying the Cloud Pak for Watson AIOps only requires one entry to your environment file (/data/daffy/env/&lt;ENVIRONMENT_NAME&gt;-env.sh)</p> <p>CP4WAIOPS_VERSION= <p>You can copy the following to your &lt;ENVIRONMENT_NAME&gt;-env.sh:</p> <pre><code>CP4WAIOPS_VERSION=\"3.7.2\"\n</code></pre> <p>With this one value, the Daffy engine will be able to install the version of Cloud Pak for Watson AI Ops and the Platform Navigator. Along with the base cloud pak components, the AI Manager will be installed.</p> <p>The service consist of the following products:</p> <p>AI Manager</p> AIOps Supported Version OCP Versions 4.1.0 4.10 3.7.2 4.10 3.7.1 4.10 3.7.0 4.10 3.6.2 4.10 3.6.1 4.10 3.6.0 4.10 <p>Run the following command to deploy the Cloud Pak for Watson AIOps + AI Manger:</p> <pre><code>/data/daffy/cp4waiops/build.sh &lt;ENVIRONMENT_NAME&gt;\n</code></pre> <p>When this step is complete, up to an hour depending on your environment, you have the Cloud Pak running. This will install all of the Cloud Pak operators including foundational services and the Platform Navigator. The cluster is now ready to deploy additional services and or demos.  At this stage, the cluster consists  of common services and the Cloud Pak for Watson AIOps operators and some services in the following projects:</p> <p>cp4waiops</p> <p>ibm-common-services</p> <p>Warning</p> <p>Occasionally you may see the following error message, which is usually not a big concern. We have noticed that in some cases (primarily on ROKS when doing an all in one deployment) the install of the event manager will take longer than normal to deploy. In this case you may see a message like this below. If that happens, please give some additional time (usually no more than 30 minutes) to verify your installation.</p> <p></p> <p>Run the --console command after 30 minutes to show you the login information. Details of the --console command are below.</p>"},{"location":"Cloud-Paks/Watson-AIOPS/#step-3-deploy-cloud-pak-for-waiops-optional-services","title":"Step 3: Deploy Cloud Pak for WAIOPS Optional Services","text":"<p>There are 2 services that can be optionally installed after you have the Cloud Pak for Watson AIOPS installed.</p> <ul> <li>Event Manager (Operator ONLY)</li> <li>Infrastructure Automation</li> </ul> <p>The Event Manager for WatsonAIOps is an optional service deployment that can be added to your WatsonAIOps Cloud Pak deployment. To deploy the Event Manager component of WatsonAIOps, you will need to set the flag within your environment file then run the service.sh script.  </p> <p>Warning</p> <p>As of today, you can ONLY deploy the Event Manager service as an additional component to the Cloud Pak for Watson AIOps. Installing the Watson AIOps Cloud Pak will by default install the AI Manager component. It is not possible today to only install the Event Manager component without the AI Manager.  </p> <p>Here is the flag that will need to be set to enable the deployment of Event Manager Operator:</p> <pre><code>CP4WAIOPS_DEPLOY_EMGR=&lt;true|false&gt;\n</code></pre> <p>Note</p> <p>POST EVENT MANAGER INSTALL STEPS The Daffy scripts for deployment of Watson AIOPS Event Manager will configure the subscription and deploy the event manager operator. You will need to configure the NOI (Event Manager) instance manually. This is because Event Manager can be configured to collect, consolidate, and correlate events and topology data from a multitude of sources, which may require additional parameters specific to your environment.  </p> <p>This is a screen shot of what you will see after Daffy deploys the event manager operator. Please follow the instructions to complete the configuration of the Event Manager (NOI) instance.</p> <p></p> <p>Here is the flag that will need to be set to enable the deployment of Infrastructure Automation:</p> <pre><code>CP4WAIOPS_DEPLOY_IA=&lt;true|false&gt;\n</code></pre> <p>Run the following command to deploy the Cloud Pak for Watson AIOps Optional Services:</p> <pre><code>/data/daffy/cp4waiops/service.sh &lt;ENVIRONMENT_NAME&gt;\n</code></pre> <p>When this step is complete you will have the Cloud Pak and the optional services running.</p>"},{"location":"Cloud-Paks/Watson-AIOPS/#step-4-status-console","title":"Step 4: Status &amp; Console","text":"<p>The service can take a few hours to complete, based on which one you chose to deploy. To help monitor the status of the service/pattern deployment, you can run the help flag to see what flags you can use to get information on your service/pattern deployment:</p> <pre><code>/data/daffy/cp4waiops/build.sh &lt;ENVIRONMENT_NAME&gt; --help\n</code></pre> <p>Run the following commands to check the Cloud Pak for Watson AIOps installation progress:</p> <pre><code>/data/daffy/cp4waiops/build.sh &lt;ENVIRONMENT_NAME&gt; --status\n</code></pre> <p>If you want to have a running job to refresh every few seconds,  you can run the status script using the watch command:</p> <pre><code>watch -c /data/daffy/cp4waiops/build.sh &lt;ENVIRONMENT_NAME&gt; --status\n</code></pre> <p>To find out the connection info to your new service/pattern, you can run the console flag to get user names, passwords and URLs to connect to:</p> <pre><code>/data/daffy/cp4waiops/build.sh &lt;ENVIRONMENT_NAME&gt; --console\n</code></pre>"},{"location":"Cloud-Paks/WebSphere-Automation/","title":"WebSphere Automation","text":""},{"location":"Cloud-Paks/WebSphere-Automation/#cloud-pak-for-websphere-automation","title":"Cloud Pak for WebSphere Automation","text":"<p>At this point, you have a working OCP cluster on your platform of choice. Your &lt;ENVIRONMENT_NAME&gt;-env.sh configuration file will contain details of the platform and OCP installation. You will now add to this file, the details of:</p> <p>1) The Cloud Pak info that you wish to install</p> <p>2) The services that you wish to install on the Cloud Pak</p>"},{"location":"Cloud-Paks/WebSphere-Automation/#step-2-deploy-wsa","title":"Step 2: Deploy WSA","text":"<p>Deploying WebSphere Automation only requires one entry to your environment file (/data/daffy/env/&lt;ENVIRONMENT_NAME&gt;-env.sh)</p> <p>CPWSA_VERSION= <p>You can copy the following to your &lt;ENVIRONMENT_NAME&gt;-env.sh:</p> <pre><code>CPWSA_VERSION=\"1.6\"\n</code></pre> <p>With this one value, the Daffy engine will be able to install the version of WebSphere Automation.</p> <p>The service consists of the following products:</p> <p>WebSphere Automation Small Profile (consists of):</p> <ul> <li> <p>WebSphere Automation</p> </li> <li> <p>WebSphere Health</p> </li> <li> <p>WebSphere Secure</p> </li> </ul> WSA Supported Version OCP Versions 1.6 4.12, 4.10 1.5 4.10 1.4 4.10 <p>Run the following command to deploy WebSphere Automation:</p> <pre><code>/data/daffy/wsa/build.sh &lt;ENVIRONMENT_NAME&gt;\n</code></pre> <p>When this step is complete, up to an hour depending on your environment, you will have the basics of WebSphere Automation running. This will install all of the operators including foundational services. The cluster is now ready to deploy additional services and/or demos.  At this stage, the cluster consists of common services and WebSphere Automation operators and some services in the following projects:</p> <p>websphere-automation</p> <p>ibm-common-services</p>"},{"location":"Cloud-Paks/WebSphere-Automation/#step-3-deploy-services","title":"Step 3: Deploy Services","text":"<p>Currently there is one service/demo for WebSphere Automation. We are adding new features on a regular basis, so please stay tuned.  If you have a feature request for an additional service or demo, please fill out a request.</p>"},{"location":"Cloud-Paks/WebSphere-Automation/#step-3a-status","title":"Step 3a: Status","text":"<p>The service can take a few hours to complete. To help monitor the status of the service/pattern deployment you can run the help flag to see what flags you can use to get information on your service/pattern deployment:</p> <pre><code>/data/daffy/wsa/build.sh &lt;ENVIRONMENT_NAME&gt; --help\n</code></pre> <p>Run the following commands to check the WebSphere Automation installation progress:</p> <pre><code>/data/daffy/wsa/service.sh &lt;ENVIRONMENT_NAME&gt; --status\n</code></pre> <p>If you want to have a running job to refresh every few seconds,  you can run the status script using the watch command:</p> <pre><code>watch -c /data/daffy/wsa/service.sh &lt;ENVIRONMENT_NAME&gt; --status\n</code></pre> <p>To find out the connection info to your new service/pattern, you can run the console flag to get user names, passwords and URLs to connect to:</p> <pre><code>/data/daffy/wsa/build.sh &lt;ENVIRONMENT_NAME&gt; --console\n</code></pre>"},{"location":"Deploying-OCP/","title":"Index","text":""},{"location":"Deploying-OCP/#deploying-ocp","title":"Deploying OCP","text":"IPI Installs AWS Azure GCP IBM VSphere MSP Installs ARO ROKS ROSA Other Installs TechZone TechZoneTiles UPI Installs KVM VSphere"},{"location":"Deploying-OCP/ARO/","title":"ARO","text":""},{"location":"Deploying-OCP/ARO/#azure-install","title":"Azure Install","text":"<p>At this point, you have a bastion machine where you have installed the Daffy tool, and ready to created your core environment-name-env.sh and so you can execute the install of OCP on Azure via ARO.</p>"},{"location":"Deploying-OCP/ARO/#platform-requirements","title":"Platform Requirements","text":"<p>To use Daffy on Azure ARO, there are some platform info and requirements that need to be gathered or met. We have a simple doc that you should refer to that list all providers and what would be needed.  For Azure ARO, it breaks down to the following basic three items:</p> <p>Account Details - The account that you plan to install OpenShift</p> <p>Permissions - The permissions need to perform the install</p> <p>Quota - The ability to add new workload to that platform</p> <p>For detailed list of the above, you can find in the Daffy Provider Requirements. Please review before proceeding.</p> <p>https://ibm.box.com/v/DaffyProviderRequirements</p>"},{"location":"Deploying-OCP/ARO/#finding-provider-details","title":"Finding Provider Details","text":"<p>To install Daffy on Azure ARO, the hardest part can be finding the provider details.</p>"},{"location":"Deploying-OCP/ARO/#subscription-id","title":"Subscription ID","text":"<p>First find subscriptions in your account from the search box</p> Screenshot Locate Subscriptions <p></p> <p> More Info</p> <p>Once you find the subscription you want to use, you can see the Subscription ID</p> Screenshot Locate Subscription ID <p></p>"},{"location":"Deploying-OCP/ARO/#tenant-id","title":"Tenant ID","text":"<p>First you need to find the Active Direcotry for your account</p> Screenshot Active Directory <p></p> <p>From your active directory, you can locate the Tenant ID    </p> Screenshot Tenant ID <p></p> <p> More Info</p>"},{"location":"Deploying-OCP/ARO/#client-id","title":"Client ID","text":"<p>First you need to find the Active Directory for your account</p> Screenshot Active Directory <p></p> <p>Search for your appliation and from here you can find the client ID for the application you plan to use </p> Screenshot Client ID <p></p> <p> More Info</p>"},{"location":"Deploying-OCP/ARO/#region","title":"Region","text":"<p> More Info</p> <p>To find the region name, you can use the above link to list all azure region names, make sure you pick one that has avaiblity zone support</p> Screenshot <p></p>"},{"location":"Deploying-OCP/ARO/#quota","title":"Quota","text":"<p>In your subscription, under settings, you can find Usage + Quotas</p> Screenshot <p></p> <p>In the Qutoa section, you can filter by regtion and type.  Then you can see your used and your max qutoa limits.</p> Screenshot <p> </p> <p> More Info</p>"},{"location":"Deploying-OCP/ARO/#permission","title":"Permission","text":"<p>Within your Azure project, you would need to go to IAM  Section and create/use Service Account.  From the requirements doc, make sure your service account has the correct permissions.  Look at the Azure section, it is same plus a few extra needed for ARO. Specifically Access to Active Directory.</p>"},{"location":"Deploying-OCP/ARO/#environment-file","title":"Environment File","text":"<p>Deploying the OpenShift on Azure only requires three entries to your existing core environment file (&lt;ENVIRONMENT_NAME&gt;-env.sh) plus a local service account file.</p> <p>Note</p> <p>You can look in the samples directory on your bastion for example of ARO install : /data/daffy/env/samples/aro-msp-env.sh</p> <p>You can copy the sample file to build your new environment  file: <pre><code>cp /data/daffy/env/samples/aro-msp-env.sh /data/daffy/env/&lt;ENVIRONMENT_NAME&gt;-env.sh\n</code></pre></p> <p> Valid Options: </p> Variable Name Info Default Value Required OCP_INSTALL_TYPE Install type must be aro-msp Yes AZURE_SUBSCRIPTION_ID The subscription ID for your account in Azure Yes AZURE_CLIENT_ID The client ID for your account in Azure Yes AZURE_TENANT_ID The Tenant ID for your account in Azure Yes AZURE_REGION The Azure region you want to deploy to Yes ARO_RESOURCE_GROUP_NAME The Azure network resource group name ${CLUSTER_NAME}-aro-vnet No ARO_CLUSTER_RESOURCE_GROUP_NAME The Azure clsuter resource group name ${CLUSTER_NAME}-aro-cluster No ARO_VCPU_QUOTA_NAME The Azure Compute name you will use for deployment Standard DSv3 Family vCPUs No OCP_CREATE_OPENSHIFT_CONTAINER_STORAGE Do you want to deploy ODF storage false No <p>OCP_RELEASE</p> <p>With ARO, you can not choose the version of OpenShift. Base version or minor version.  At deployment time, Azure will pick for you. So OCP_RELEASE value is not used or allowed for ARO deployment.  Once deployment is done, daffy will calculate the OCP version and download the correct version of ocp tools to match your new cluster.</p> <pre><code>#ARO Base Settings\n####################\nOCP_INSTALL_TYPE=\"aro-msp\"\nAZURE_SUBSCRIPTION_ID=\"999999-999999-999999-99999\"\nAZURE_CLIENT_ID=\"999999-999999-999999-99999\"\nAZURE_TENANT_ID=\"999999-999999-999999-99999\"\nAZURE_REGION=\"&lt;YOUR_REGION&gt;\"\n\n#ARO Override Settings\n####################\n#ARO_RESOURCE_GROUP_NAME=\"${CLUSTER_NAME}-aro-vnet\"\n#ARO_CLUSTER_RESOURCE_GROUP_NAME=\"${CLUSTER_NAME}-aro-cluster\"\n#ARO_VCPU_QUOTA_NAME=\"Standard DSv3 Family vCPUs\"\n\n#OpenShift Storage\n####################\n#OCP_CREATE_OPENSHIFT_CONTAINER_STORAGE=\"true\"\n</code></pre> <p>Warning</p> <p>If you plan to install a cloud pak and/or need storage, you need to set the flag to setup OCS Storage</p> <p>Info</p> <p>It will prompt you for the Client Secret during the install</p>"},{"location":"Deploying-OCP/ARO/#execution","title":"Execution","text":"<p>To deploy your OCP cluster to Azure ARO, run the build.sh script from the /data/daffy/ocp directory:</p> <pre><code>/data/daffy/ocp/build.sh &lt;ENVIRONMENT_NAME&gt;\n</code></pre> <p>Once your cluster is fully deployed you can access the help menu which has a number of options.</p> <p>Note</p> <p>&lt;ENVIRONMENT_NAME&gt; is the first part of your name that you used for the &lt;ENVIRONMENT_NAME&gt;-env.sh file</p> <p>Installing Cloud Paks</p>"},{"location":"Deploying-OCP/AWS/","title":"AWS","text":""},{"location":"Deploying-OCP/AWS/#aws-install","title":"AWS Install","text":""},{"location":"Deploying-OCP/AWS/#platform-requirements","title":"Platform Requirements","text":"<p>To use Daffy on Amazon Web Services, there are some platform info and requirements that need to be gathered or met. We have a simple doc that you should refer to that list all providers and what would be needed.  For AWS, it breaks down to the following basic three items:</p> <p>Account Details - The account that you plan to install OpenShift</p> <p>Permissions - The permissions need to perform the install</p> <p>Quota - The ability to add new workload to that platform</p> <p>For detailed list of the above, you can find in the Daffy Provider Requirements. Please review before proceeding.</p> <p>https://ibm.box.com/v/DaffyProviderRequirements</p>"},{"location":"Deploying-OCP/AWS/#finding-provider-details","title":"Finding Provider Details","text":"<p>To install Daffy on AWS, the hardest part can be finding the provider details in the portal.</p> <p>To create or use an existing AWS Access Key ID you can refer to this:</p> <p>https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_access-keys.html?icmpid=docs_iam_console#Using_CreateAccessKey</p> <p>Note: Use the Identity and Access Management (IAM) service to manage access keys.</p> <ol> <li>Select Search - find   IAM   service</li> <li>You can create a new access key or use an existing key. The access key must have authority to the account you wan to install OpenShift into.</li> </ol> <p> </p> <p>Secret Access Key: The secret access key is ONLY displayed at the time of creation. When you create the access key, you will then have the opportunity to capture the secret access key</p> <p>Note</p> <p>This is sensitive information, please make sure you store this in a secure location</p> <p>The screen to the right is an example of what you will see when you create a NEW access Key. </p> <p>Region:</p> <p>For you to use Daffy to install on AWS you need to choose a valid region identifier. This will be the target region you are planning to deploy OpenShift into.  </p> <p>To see a complete list of available AWS Regions, you can select the region drop down list in the AWA Portal. This will be in the upper right hand corner next to your account name. (See picture to the right)</p> <p>Note: Take note of the region identifier such as: us-east-2. This is the value you would use to deploy a OCP cluster into the US East (Ohio) region. </p> <p>Permission:</p> <p>Within your AWS project, you would need to go to IAM  Section and make sure the user that is associated with your Access Key is assigned the correct roles.  </p> <p>At minimum, you need to have this role: AdministratorAccess</p> <p>Please see the requirements doc for more information!</p> <p></p> <p>Hosted Zone:</p> <p>For each OpenShift deployment into AWS, you need to create a Route 53 Hosted Zone .</p> <p>Important: You must create a Hosted Zone that exactly matches your Base Domain.</p> <p>Important: Once you create your Hosted Zone, you must point your DNS registry Name Server records to the assigned AWS DNS Name Server records listed in this Hosted Zone. You will see the Name Servers listed once you have created the Hosted Zone.</p> <p> </p>"},{"location":"Deploying-OCP/AWS/#setting-up-dns","title":"Setting up DNS","text":"HTML Video embed <p>Quota:</p> <p>Please refer to the requirements doc for a list of resource quota's that are required for deployment of OpenShift in AWS.</p>"},{"location":"Deploying-OCP/AWS/#environment-file","title":"Environment File","text":"<p>Below are the AWS specific environment variables that must be defined in the /data/daffy/env/&lt;ENVIRONMENT_NAME&gt;-env.sh file:</p> <ul> <li>AWS_REGION</li> <li>AWS_ACCESS_KEY_ID</li> </ul> <p>Note</p> <p>You can look in the samples directory on your bastion for example of AWS install : /data/daffy/env/samples/aws-ipi-env.sh</p> <p>You can run this command to build your new file from the sample. <pre><code>cp /data/daffy/env/samples/aws-ipi-env.sh /data/daffy/env/&lt;ENVIRONMENT_NAME&gt;-env.sh\n</code></pre> Valid Options:</p> <ul> <li>OCP_INSTALL_TYPE=aws-ipi</li> <li>AWS_REGION=AWS-REGION</li> <li>AWS_ACCESS_KEY_ID=AWS-ACCESS_KEY-ID</li> </ul> <pre><code>OCP_INSTALL_TYPE=\"aws-ipi\"\nAWS_REGION=\"&lt;AWS-REGION&gt;\"\nAWS_ACCESS_KEY_ID=\"&lt;AWS-ACCESS_KEY-ID&gt;\"\n#OCP_CREATE_OPENSHIFT_CONTAINER_STORAGE=true\n</code></pre> <p>Optional:</p> <p>OCP_CREATE_OPENSHIFT_CONTAINER_STORAGE=true</p> <p>If you plan to install a cloud pak and/or need storage, you need to set the flag to setup OCS Storage.</p>"},{"location":"Deploying-OCP/AWS/#execution","title":"Execution","text":"<p>To deploy your cluster, run the build.sh script from the /data/daffy/ocp directory:</p> <pre><code>/data/daffy/ocp/build.sh &lt;ENVIRONMENT_NAME&gt;\n</code></pre> <p>Once your cluster is fully deployed you can access the help menu which as a number of options.</p> <p>Note</p> <p>&lt;environment&gt; is the first part of your name that you used for the &lt;environment&gt;-env.sh file</p> <pre><code>/data/daffy/ocp/build.sh &lt;ENVIRONMENT_NAME&gt; --help\n</code></pre> <p> Installing Cloud Paks</p>"},{"location":"Deploying-OCP/Azure/","title":"Azure","text":""},{"location":"Deploying-OCP/Azure/#azure-install","title":"Azure Install","text":"<p>At this point, you have a bastion machine where you have installed the Daffy tool, created your core environment-name-env.sh and can execute the install of OCP on Azure.</p>"},{"location":"Deploying-OCP/Azure/#platform-requirements","title":"Platform Requirements","text":"<p>To use Daffy on Azure, there are some platform info and requirements that need to be gathered or met. We have a simple doc that you should refer to that list all providers and what would be needed.  For Azure, it breaks down to the following basic three items:</p> <p>Account Details - The account that you plan to install OpenShift</p> <p>Permissions - The permissions need to perform the install</p> <p>Quota - The ability to add new workload to that platform</p> <p>For detailed list of the above, you can find in the Daffy Provider Requirements. Please review before proceeding.</p> <p>https://ibm.box.com/v/DaffyProviderRequirements</p>"},{"location":"Deploying-OCP/Azure/#finding-provider-details","title":"Finding Provider Details","text":"<p>To install Daffy on Azure, the hardest part can be finding the provider details.</p>"},{"location":"Deploying-OCP/Azure/#subscription-id","title":"Subscription ID","text":"<p>First find subscriptions in your account from the search box</p> Screenshot Locate Subscriptions <p></p> <p> More Info</p> <p>Once you find the subscription you want to use, you can see the Subscription ID</p> Screenshot Locate Subscription ID <p></p>"},{"location":"Deploying-OCP/Azure/#tenant-id","title":"Tenant ID","text":"<p>First you need to find the Active Direcotry for your account</p> Screenshot Active Directory <p></p> <p>From your active directory, you can locate the Tenant ID    </p> Screenshot Tenant ID <p></p> <p> More Info</p>"},{"location":"Deploying-OCP/Azure/#client-id","title":"Client ID","text":"<p>First you need to find the Active Directory for your account</p> Screenshot Active Directory <p></p> <p>Search for your appliation and from here you can find the client ID for the application you plan to use </p> Screenshot Client ID <p></p> <p> More Info</p>"},{"location":"Deploying-OCP/Azure/#region","title":"Region","text":"<p> More Info</p> <p>To find the region name, you can use the above link to list all azure region names, make sure you pick one that has avaiblity zone support</p> Screenshot <p></p>"},{"location":"Deploying-OCP/Azure/#quota","title":"Quota","text":"<p>In your subscription, under settings, you can find Usage + Quotas</p> Screenshot <p></p> <p>In the Qutoa section, you can filter by regtion and type.  Then you can see your used and your max qutoa limits.</p> Screenshot <p> </p> <p> More Info</p> <p>Permission:</p> <p>Within your Azure project, you would need to go to IAM  Section and create/use Service Account.  From the requirements doc, make sure your service account has the correct permissions.</p> <p>Dedicated public host Zone:</p> <p>You will need to create a DNS Zone within a new/existing resource group.  For the OpenShift install, you need the following:</p> <ol> <li>Registered DNS Name - myexample.com</li> <li>Azure DNS Zone              - myexample-com</li> <li>Transfer the domain to Azure Name services listed in your new Azure DNS Zone</li> </ol>"},{"location":"Deploying-OCP/Azure/#setting-up-dns","title":"Setting up DNS","text":""},{"location":"Deploying-OCP/Azure/#environment-file","title":"Environment File","text":"<p>Deploying the OpenShift on Azure only requires three entries to your existing core environment file (&lt;ENVIRONMENT_NAME&gt;-env.sh) plus a local service account file.</p> <p>Note</p> <p>You can look in the samples directory on your bastion for example of Azure install : /data/daffy/env/samples/azure-ipi-env.sh</p> <p>You can copy the sample file to build your new environment  file: <pre><code>cp /data/daffy/env/samples/azure-ipi-env.sh /data/daffy/env/&lt;ENVIRONMENT_NAME&gt;-env.sh\n</code></pre></p> <p> Valid Options: </p> Variable Name Info Default Value Required OCP_INSTALL_TYPE Install type must be aro-msp Yes AZURE_SUBSCRIPTION_ID The subscription ID for your account in Azure Yes AZURE_CLIENT_ID The client ID for your account in Azure Yes AZURE_TENANT_ID The Tenant ID for your account in Azure Yes AZURE_REGION The Azure region you want to deploy to Yes AZURE_RESOURCE_GROUP_NAME The Azure network resource group name No AZURE_BASE_DOMAIN_RESOURCE_GROUP_NAME The Azure clsuter resource group name No OCP_CREATE_OPENSHIFT_CONTAINER_STORAGE Do you want to deploy ODF storage false No AZURE_RESOURCE_GROUP_NAME_CREATE_MISSING Do you want to deploy ODF storage false No <pre><code>#Azure Base Settings\n####################\nOCP_INSTALL_TYPE=\"azure-ipi\"\nAZURE_SUBSCRIPTION_ID=\"999999-999999-999999-99999\"\nAZURE_CLIENT_ID=\"999999-999999-999999-99999\"\nAZURE_TENANT_ID=\"999999-999999-999999-99999\"\nAZURE_RESOURCE_GROUP_NAME=\"&lt;YOUR_RESOURCE_GROUP_FOR_CLUSTER&gt;\"\nAZURE_BASE_DOMAIN_RESOURCE_GROUP_NAME=\"&lt;YOUR_RESOURCE_GROUP_FOR_DNS&gt;\"\nAZURE_REGION=\"&lt;YOUR_REGION&gt;\"\n\n#OpenShift Storage\n####################\n#OCP_CREATE_OPENSHIFT_CONTAINER_STORAGE=\"true\"\n#AZURE_RESOURCE_GROUP_NAME_CREATE_MISSING=\"true\"\n</code></pre> <p>If you plan to install a cloud pak and/or need storage, you need to set the flag to setup OCS Storage</p> <p>** It will prompt you for the Client Secret during the install.</p>"},{"location":"Deploying-OCP/Azure/#execution","title":"Execution","text":"<p>To deploy your OCP cluster to Azure, run the build.sh script from the /data/daffy/ocp directory:</p> <pre><code>/data/daffy/ocp/build.sh &lt;ENVIRONMENT_NAME&gt;\n</code></pre> <p>Once your cluster is fully deployed you can access the help menu which has a number of options.</p> <p>Note</p> <p>&lt;ENVIRONMENT_NAME&gt; is the first part of your name that you used for the &lt;ENVIRONMENT_NAME&gt;-env.sh file</p> <p>Installing Cloud Paks</p>"},{"location":"Deploying-OCP/Core-steps/","title":"Core Steps","text":""},{"location":"Deploying-OCP/Core-steps/#step-1-bastion-server","title":"Step 1: Bastion Server","text":"<p>Create Bastion Steps</p> <p>*** If  you do not have a bastion, above button/link will walk you through the process to create a Linux bastion server.</p>"},{"location":"Deploying-OCP/Core-steps/#step-2-sizing","title":"Step 2: Sizing","text":"<p>Go to the following site to size your OpenShift cluster to meet your software needs</p> <p>CloudPak Sizing</p>"},{"location":"Deploying-OCP/Core-steps/#step-3-install-daffy","title":"Step 3: Install Daffy","text":"<p>Log into your Bastion Machine (as root) and run the following command to download the latest Daffy Scripts:</p> <p><pre><code>wget http://get.daffy-installer.com/download-scripts/daffy-init.sh; chmod 755 daffy-init.sh;./daffy-init.sh\n</code></pre> Optional: You may choose to use the Daffy Web Configurator or Daffy CLI Configurator! The purpose of these tools are to help you build your environment file</p> <p>Online Configurator CLI Configurator</p>"},{"location":"Deploying-OCP/Core-steps/#step-4-environment-file","title":"Step 4: Environment File","text":"<pre><code>#Daffy Values\n#########################\nDAFFY_UNIQUE_ID=\"&lt;YourID@email.com&gt;\"\n#This is required - Values POC/Demo/Enablement/HCCX/TechZone\nDAFFY_DEPLOYMENT_TYPE=\"PickValueFromLineAbove\"\n#If POC/Demo, these are required.\n#ISC number must be 18 characters\n#DAFFY_ISC_NUMBER=\"0045h00000w1nvKAAG\"\n#DAFFY_CUSTOMER_NAME=\"Acme Customer\"\n\n#Core Values\n#########################\nBASE_DOMAIN=\"&lt;YOUR.BASEDOMAIN.COM&gt;\"\n#Cluster name must only be lowercase/alphanumberic and \"-\", no spaces\nCLUSTER_NAME=\"&lt;ENVIRONMENT_NAME&gt;\"\n#This is required - Values aws-ipi/azure-ipi/gcp-ipi/vsphere-ipi/vsphere-upi/kvm-upi/roks-msp\nOCP_INSTALL_TYPE=\"PickValueFromLineAbove\"\nOCP_RELEASE=\"4.10.36\"\nVM_TSHIRT_SIZE=\"Large\"\n</code></pre> <p>This file is where you store values that will define your environment and Daffy will use to build your environment.</p> <p>Place your file in the following folder with your environment name in the following formatting:</p> <pre><code>/data/daffy/env/&lt;**ENVIRONMENT_NAME**&gt;-env.sh\n</code></pre> <p>Best practice is to set &lt;ENVIRONMENT_NAME&gt; as your cluster name, but that is not required.</p> Name Example Values Description Requirements DAFFY_DEPLOYMENT_TYPE Enablement The type of Depolment this will be for POC/Demo/Enablement/HCCX/TechZone/PostSale DAFFY_ISC_NUMBER 0045h00000w1nvKAAG Required if Demo, POC or PostSale Must be 18 characters, no spaces DAFFY_CUSTOMER_NAME Acme Shoes Required if Demo, POC or PostSale May contain spaces BASE_DOMAIN acme-shoes.com Is your DNS name your cluster will use valid dns domain name CLUSTER_NAME demo01 The name you want to give your OpenShift Cluster Only lowercase/alphanumeric and \"-\", no spaces OCP_INSTALL_TYPE aws-ipi The name of the install type you want aws-ipi/azure-ipi/gcp-ipi/vsphere-ipi/vsphere-upi/kvm-upi/roks-msp OCP_RELEASE 4.10.36 What version of OpenShift you want to Install VM_TSHIRT_SIZE Large How large you want the OpenShift Cluster to be. Min and Large Supported today <p>Info</p> <p>If MSP type install like ROKS, BASE_DOMAIN is not needed.</p> <p>Optionally: As a starting point, you can copy a sample environment file from the samples folder located here:  /data/daffy/env/samples/&lt;platform&gt;-env.sh <pre><code>cd /data/daffy/env/samples\n</code></pre></p> <p>Replace these values for the next command:</p> <ul> <li> <p>&lt;platform&gt; = the sample file name for the platform you are planning to deploy your OCP Cluster.</p> </li> <li> <p>&lt;environment&gt; = the name of your environment file. As a best practice, we recommend you use the name of your cluster.</p> </li> </ul> <p>Example:  cp /data/daffy/env/samples/aws-ipi-env.sh /data/daffy/env/demo01-env.sh</p> <p>This command will copy the sample file and place it in the /data/daffy/env directory, and your new environment name will be demo01</p> <pre><code>cp /data/daffy/env/samples/&lt;platform&gt;-env.sh /data/daffy/env/&lt;environment&gt;-env.sh\n</code></pre> <p>Debug Flag</p> <p>Setting the debug flag to true will stop you at every check point and ask you to hit enter. Setting the debug flag to false will run though the script without any interference.  </p> <pre><code>DEBUG=\"false\"\n</code></pre>"},{"location":"Deploying-OCP/Core-steps/#step-5-dns-requirements","title":"Step 5:  DNS Requirements","text":"<p>For OpenShift to be installed, you will need to setup your own DNS or use existing domain/subdomain. You can not use local host files or local resolver.</p>"},{"location":"Deploying-OCP/Core-steps/#vsphere-and-kvm-upi","title":"vSphere and KVM UPI","text":"<ol> <li>api.${CLUSTER}.${YOUR.DOMAIN.COM}          ---&gt;    ${YOUR.BASTION.IP}  </li> <li>api-int.${CLUSTER}.${YOUR.DOMAIN.COM}      ---&gt;    ${YOUR.BASTION.IP}  </li> <li>*.apps.${CLUSTER}.${YOUR.DOMAIN.COM}      ---&gt;    ${YOUR.BASTION.IP}  </li> </ol>"},{"location":"Deploying-OCP/Core-steps/#vsphere-ipi","title":"vSphere IPI","text":"<ol> <li>api.${CLUSTER}.${YOUR.DOMAIN.COM}          ---&gt;    Unused Static IP #1   </li> <li>*.apps.${CLUSTER}.${YOUR.DOMAIN.COM}       ---&gt;    Unused Static IP #2   </li> </ol> Allow Daffy to crate DNS entries in IBM Cloud <p>If you want the daffy tool to create your above DNS entires in IBM Cloud, add the following to your ~/.profiles <pre><code>DNS_API_KEY=\"YOURDNSAPIKEY\"\nDNS_DOMAIN_ID=\"YOURDNSDOMAINID\"\n</code></pre></p>"},{"location":"Deploying-OCP/Core-steps/#aws-azure-gcp-ipi","title":"AWS, Azure, GCP IPI","text":"<ol> <li>Have/Create DNS domain/subdomain (More Detail for DNS on next steps)</li> <li>Transfer domain/subdomain to your provider (if not created there already)     a. Create hosted zone in provider       b. Use name servers from hosted zone to transfer your domain/subdomain      </li> </ol>"},{"location":"Deploying-OCP/Core-steps/#techzone-hccx-roks","title":"TechZone, HCCX, ROKS","text":"<ol> <li>You will not need DNS as they will provide for you</li> </ol>"},{"location":"Deploying-OCP/Core-steps/#step-6a-install-openshift","title":"Step 6a: Install OpenShift","text":"<p>Info</p> <p>Before you start to install OpenShift on the Provider, please look at the OpenShift Requirements</p> <p>You are NOW ready to begin making the necessary edits to your /data/daffy/env/&lt;ENVIRONMENT_NAME&gt;-env.sh file for a deployment of OCP to a specific platform.</p> HCCX Gym TechZone ROKS VSphere Azure AWS GCP"},{"location":"Deploying-OCP/Core-steps/#step-6b-existing-cluster","title":"Step 6b: Existing Cluster","text":"<p>If you already have an existing cluster that was not built with Daffy, you can still use daffy for Step 2 and/or Step 3. The only extra step you need to do is via the command line on your bastion. You will need to login to your cluster via the oc login command. You can get this command from your OpenShift console. You would then move on to the Cloud Pak steps and skip the OpenShift Install.</p> <p>For Step 6b only, If you do not have the oc command installed on your bastion, you can use the daffy tools command to installing it for you.</p> <p>https://ibm.github.io/daffy/Tips-and-Tricks/Common-Commands/#daffy-tools</p> <pre><code>  /data/daffy/tools.sh --installOC\n</code></pre> <p>Cloud Paks</p>"},{"location":"Deploying-OCP/GCP/","title":"GCP","text":""},{"location":"Deploying-OCP/GCP/#gcp-install","title":"GCP Install","text":"<p>At this point, you have a bastion machine where you have installed the Daffy tool, created your core -env.sh and can execute the install of OCP on GCP."},{"location":"Deploying-OCP/GCP/#platform-requirements","title":"Platform Requirements","text":"<p>To use Daffy on Google Cloud Platform, there are some platform info and requirements that need to be gathered or met. We have a simple doc that you should refer to that list all providers and what would be needed.  For GCP, it breaks down to the following basic three items:</p> <p>Account Details - the account that you plan to install OpenShift</p> <p>Permissions - the permissions need to perform the install</p> <p>Quota - the ability to add new workload to that platform</p> <p>For detailed list of the above, you can find in the Daffy Provider Requirements. Please review before proceeding.</p> <p>https://ibm.box.com/v/DaffyProviderRequirements</p>"},{"location":"Deploying-OCP/GCP/#finding-provider-details","title":"Finding Provider Details","text":"<p>To install Daffy on Google Cloud Platform, the hardest part can be finding the provider details.</p> <p>Project ID</p> <p>To find your GCP project you can refer to this:</p> <p>https://cloud.google.com/resource-manager/docs/creating-managing-projects</p> <ol> <li>You can list your Project ID from the drop down</li> <li>You can see your Project ID from the dashboard</li> </ol> <p></p> <p>Identifying a region or zone</p> <p>Each region in Compute Engine contains a number of zones. Each zone name contains two parts that describe each zone in detail. The first part of the zone name is the region and the second part of the name describes the zone in the region.</p> <p>Region</p> <p>Regions are collections of zones. Zones have high-bandwidth, low-latency network connections to other zones in the same region. In order to deploy fault-tolerant applications that have high availability, Google recommends deploying applications across multiple zones and multiple regions. This helps protect against unexpected failures of components, up to and including a single zone or region.</p> <p>Choose regions that makes sense for your scenario. For example, if you only have customers in the US, or if you have specific needs that require your data to live in the US, it makes sense to store your resources in zones in the us-central1 region or zones in the us-east1 region.</p> <p>Region: to find a list of regions, you can refer to this:</p> <p>https://cloud.google.com/compute/docs/regions-zones</p> <p>What are service accounts?</p> <p>A service account is a special kind of account used by an application or compute workload, such as a Compute Engine virtual machine (VM) instance, rather than a person. Applications use service accounts to make authorized API calls, authorized as either the service account itself, or as Google Workspace or Cloud Identity users through domain-wide delegation.</p> <p>Service Account:</p> <p>In order to install on GCP with Daffy, you need to create a service account that has the correct permission to install.</p> <p>https://cloud.google.com/iam/docs/creating-managing-service-account-keys#creating</p> <p>Permission:</p> <p>Within your GCP project, you would need to go to IAM  Section and create/use Service Account.  From the requirements doc, make sure your service account has the correct permissions.</p> <p></p> <p>API Services Enabled:</p> <p>For each GCP project, you need to enable API access. Within your GCP project, you would need to enable each API needed for the OpenShift install. From the requirements doc, you can search for each API and confirm/enable each API Service.</p> <p></p> <p>Quota:</p> <p>For each GCP project, you need to enable correct amount of quota to run openshift. Within your GCP project, you would need to Verify your current quota. From the requirements doc, you can search for each quota to find current limit.</p> <ol> <li>Search for \"Quotas\" within your GCP project</li> <li>Select the \"All Quotas\"</li> <li>Search for the quota you are looking for</li> <li>Verify Limit/Used Percentage</li> </ol> <p></p> <p>Once you open the \"All Quotas\" page, you can search for each quota to see its limits.</p> <p></p> <p>Dedicated public host Zone:</p> <p>You will need to create a DNS hosted Zone project.  For the OpenShift install, you need the following:</p> <ol> <li>Registered DNS Name - myexample.com</li> <li>GCP DNS Zone        - myexample-com</li> <li>Transfer the domain to GCP Name services listed in your new GCP DNS Zone</li> </ol>"},{"location":"Deploying-OCP/GCP/#setting-up-dns","title":"Setting up DNS","text":""},{"location":"Deploying-OCP/GCP/#environment-file","title":"Environment File","text":"<p>Deploying the OpenShift on GCP only requires three entries to your existing core environment file (&lt;ENVIRONMENT_NAME&gt;-env.sh) plus a local service account file.</p> <p>Note</p> <p>You can look in the samples directory on your bastion for example of GCP install : /data/daffy/env/samples/gcp-ipi-env.sh</p> <p>You can copy the sample file to build your new environment  file.</p> <p>cp /data/daffy/env/samples/gcp-ipi-env.sh /data/daffy/env/&lt;ENVIRONMENT_NAME&gt;-env.sh</p> <p>Valid Options:</p> <p>OCP_INSTALL_TYPE= gcp-ipi</p> <p>GCP_PROJECT_ID= <p>GCP_REGION=  <p>Optional: OCP_CREATE_OPENSHIFT_CONTAINER_STORAGE=true</p> <pre><code>OCP_INSTALL_TYPE=\"gcp-ipi\"\nGCP_PROJECT_ID=\"&lt;YourGCPProjectID&gt;\"       GCP_REGION=\"&lt;AnyValidGCPRegion&gt;\"\n#OCP_CREATE_OPENSHIFT_CONTAINER_STORAGE=true\n</code></pre> <p>If you plan to install a cloud pak and/or need storage, you need to set the flag to setup OCS Storage.</p> <p>For GCP, you need to download your Service Key as well.  Save this to your home path:  ~/.gcp/osServiceAccount.json</p> <ol> <li>In your GCP project, go to IAM and Select Service accounts</li> <li>Select or create new Service Account</li> <li>From the Service Account, select the Keys tab to create new key.</li> </ol> <p>*** FYI  after you create the key, you can not view/download it.  You can only get the details at the time of creation</p> <p></p>"},{"location":"Deploying-OCP/GCP/#execution","title":"Execution","text":"<p>To deploy your OCP cluster to GCP, run the build.sh script from the /data/daffy/ocp directory:</p> <pre><code>/data/daffy/ocp/build.sh &lt;ENVIRONMENT_NAME&gt;\n</code></pre> <p>Once your cluster is fully deployed, you can access the help menu which has a number of options.</p> <p>Note</p> <p>&lt;ENVIRONMENT_NAME&gt; is the first part of your name that you used for the -env.sh file <p><pre><code>/data/daffy/ocp/build.sh &lt;ENVIRONMENT_NAME&gt; --help\n</code></pre>  Installing Cloud Paks</p>"},{"location":"Deploying-OCP/HCCX-gym/","title":"HCCX Gym","text":""},{"location":"Deploying-OCP/HCCX-gym/#hccx-gym","title":"HCCX Gym","text":""},{"location":"Deploying-OCP/HCCX-gym/#overview","title":"Overview","text":"<p>The OpenShift Gym is a learning environment offering individuals the ability to deploy virtual machines to support installation of IBM technologies such as OpenShift and Cloud Paks.</p> <p>The HCCX Gym has documentation on using Daffy, below are some shortcuts for their instructions. It's the same basic instructions based on VSphere, but they have some pre-built steps for Gym Members.</p> <ol> <li> <p>Main Gym Page</p> </li> <li> <p>VMware IPI deployment - using DAFFY</p> </li> </ol>"},{"location":"Deploying-OCP/HCCX-gym/#prerequisites","title":"Prerequisites","text":"<p>Warning</p> <p>For internal IBM use only, Links may only work while in the IBM Network</p> <ol> <li>A Gym Membership request can be made by filling out the form</li> <li>An active\u00a0TECNet VPN ID is required to access and use the Gym</li> </ol> <p>An email containing details on accessing the features of the OpenShift Gym is sent to the email address supplied once provisioning is completed. General information about the environment supplied is listed below. Refer to the provisioning email for detailed information.</p>"},{"location":"Deploying-OCP/HCCX-gym/#connection","title":"Connection","text":"<ol> <li> <p>Make sure you are connected to the Technet VPN. During initial setup, it may require you to reset your password.</p> </li> <li> <p>Launch your preferred Terminal</p> <ol> <li>On the left tab, click on hosts</li> <li>Click on + New Host</li> <li>Add the IP address that was given in the provisioning email</li> <li>Add password that was given in the email</li> <li>Change port to 32222</li> </ol> </li> </ol>"},{"location":"Deploying-OCP/HCCX-gym/#termius","title":"Termius","text":""},{"location":"Deploying-OCP/HCCX-gym/#standard-terminal","title":"Standard Terminal","text":"<pre><code>ssh admin@{Server IP address} -p 32222\n</code></pre>"},{"location":"Deploying-OCP/HCCX-gym/#set-up","title":"Set Up","text":""},{"location":"Deploying-OCP/HCCX-gym/#login-as-root","title":"Login as root","text":"<p>After logging in as admin, switch to root user.</p> <pre><code>  sudo su -\n</code></pre> <p>Warning</p> <p>Before you can start with daffy, you must registry your RedHat Enterprise Linux(RHEL)( Here)   <pre><code>subscription-manager register --username &lt;username&gt; --password &lt;password&gt; --auto-attach\n</code></pre></p>"},{"location":"Deploying-OCP/HCCX-gym/#install-latest-daffy","title":"Install latest daffy","text":"<pre><code>curl  http://get.daffy-installer.com/download-scripts/daffy-init.sh | bash\n</code></pre>"},{"location":"Deploying-OCP/HCCX-gym/#copy-environment-file","title":"Copy environment file","text":"<p>Next you will copy the pre-populated env file in your home directory to your Daffy env directory <pre><code>cp ~/vmware-ipi-env.sh /data/daffy/env/{env-name}-env.sh\n</code></pre> You may make any changes needed in this file, add cloud paks, change sizing, etc.</p>"},{"location":"Deploying-OCP/HCCX-gym/#deploying","title":"Deploying","text":"<p>You can now run the daffy process.</p> <pre><code>/data/daffy/build.sh  {env-name}\n</code></pre> <p>Installing Cloud Paks</p>"},{"location":"Deploying-OCP/IBM/","title":"IBM","text":""},{"location":"Deploying-OCP/IBM/#ibm-install","title":"IBM Install","text":""},{"location":"Deploying-OCP/IBM/#platform-requirements","title":"Platform Requirements","text":"<p>To use Daffy on IBM Cloud, there are some platform info and requirements that need to be gathered or met. We have a simple doc that you should refer to that list all providers and what would be needed.  For IBM, it breaks down to the following basic three items:</p> <p>Account Details - The account that you plan to install OpenShift</p> <p>Permissions - The permissions need to perform the install</p> <p>Cloud Internet Services - The ability to add DNS</p> <p>For detailed list of the above, you can find in the Daffy Provider Requirements. Please review before proceeding.</p> <p>https://ibm.box.com/v/DaffyProviderRequirements</p>"},{"location":"Deploying-OCP/IBM/#finding-provider-details","title":"Finding Provider Details","text":"<p>To install Daffy on IBM, the hardest part can be finding the provider details in the portal.</p> <p>To create or use an existing IBM API Key you can refer to this:</p> <p>https://cloud.ibm.com/docs/account?topic=account-userapikey&amp;interface=ui</p> <p>Note: Use the Identity and Access Management (IAM) service to manage access keys.</p> <ol> <li>Select Manage - Access (IAM) from drop down menu, then select API keys on the left menu</li> <li>You can create a new access key or use an existing key. The access key must have authority to the account you wan to install OpenShift into.</li> </ol> <p> </p> <p>IBM API Key: The IBM API key is ONLY displayed at the time of creation. When you create the access key, you will then have the opportunity to capture or download the key</p> <p>Note</p> <p>This is sensitive information, please make sure you store this in a secure location</p> <p>The screen below is an example of what you will see when you create a NEW access Key. </p> <p>Region:</p> <p>For you to use Daffy to install on IBM you need to choose a valid region identifier. This will be the target region you are planning to deploy OpenShift into.  </p> <p>To see a complete list of available IBM Regions, go to the following website.</p> <p>https://cloud.ibm.com/docs/overview?topic=overview-locations</p> <p>By default, Daffy sets this to us-south, but others are fully supported.</p> <p>Note: Take note of the region identifier such as: us-south. This is the value you would use to deploy a OCP cluster into the US South (Dallas) region. This is the default if not in your environment file  </p> <p>Permission:</p> <p>Within your IBM account, you would need to go to IAM  Section and make sure the user that is associated with your account is assigned the correct roles.  </p> <p>At minimum, you need to have this role: All Account Management services = All</p> <p>Please see the requirements doc for more information!</p> <p></p> <p>Hosted Zone (CIS)</p> <p>For each OpenShift deployment into IBM, you need to have your own Domain and a Cloud Internet Services. (CIS) service.</p> <p></p> <p>Important: The domain you add in the CIS Service must EXACTlY match the domain you configured in the Domain Name Registration service.  </p> <p>Important: Once you create your domain in your CIS instance, you must point your DNS registry Name Server records to the assigned IBM DNS Name Server records assigned by the CIS Service. You will see the Name Servers listed once you have added the domain to the CIS Service.</p> <p></p>"},{"location":"Deploying-OCP/IBM/#setting-up-dns","title":"Setting up DNS","text":"HTML Video embed"},{"location":"Deploying-OCP/IBM/#environment-file","title":"Environment File","text":"<p>Below is the IBM specific environment variable that must be defined in the /data/daffy/env/&lt;ENVIRONMENT_NAME&gt;-env.sh file:</p> <ul> <li>CIS_INSTANCE_NAME</li> </ul> <p>Note</p> <p>You can look in the samples directory on your bastion for example of IBM install : /data/daffy/env/samples/ibm-ipi-env.sh</p> <p>You can run this command to build your new file from the sample. <pre><code>cp /data/daffy/env/samples/ibm-ipi-env.sh /data/daffy/env/&lt;ENVIRONMENT_NAME&gt;-env.sh\n</code></pre> Valid Options:</p> <ul> <li>OCP_INSTALL_TYPE=ibm-ipi</li> <li>CIS_INSTANCE_NAME=\"YOUR IBM CIS Instance\"</li> </ul> <pre><code>OCP_INSTALL_TYPE=\"ibm-ipi\"\nCIS_INSTANCE_NAME=\"YOUR IBM CIS Instance\"\n#OCP_CREATE_OPENSHIFT_CONTAINER_STORAGE=true\n</code></pre> <p>Optional:</p> <p>OCP_CREATE_OPENSHIFT_CONTAINER_STORAGE=true</p> <p>If you plan to install a cloud pak and/or need storage, you need to set the flag to setup OCS Storage.</p>"},{"location":"Deploying-OCP/IBM/#execution","title":"Execution","text":"<p>To deploy your cluster, run the build.sh script from the /data/daffy/ocp directory:</p> <pre><code>/data/daffy/ocp/build.sh &lt;ENVIRONMENT_NAME&gt;\n</code></pre> <p>Once your cluster is fully deployed you can access the help menu which as a number of options.</p> <p>Note</p> <p>&lt;environment&gt; is the first part of your name that you used for the &lt;environment&gt;-env.sh file</p> <pre><code>/data/daffy/ocp/build.sh &lt;ENVIRONMENT_NAME&gt; --help\n</code></pre> <p> Installing Cloud Paks</p>"},{"location":"Deploying-OCP/Pre-Req/","title":"Pre-Req","text":""},{"location":"Deploying-OCP/Pre-Req/#daffy-pre-requirements","title":"Daffy Pre-Requirements","text":""},{"location":"Deploying-OCP/Pre-Req/#what-is-required-to-use-daffy","title":"What is required to use Daffy?","text":"<p>Before you can use the Daffy scripts, you must have the following:</p>"},{"location":"Deploying-OCP/Pre-Req/#ssh-client-on-your-local-workstation","title":"SSH client on your local workstation","text":"<ul> <li>We highly recommend installing Termius as your SSH client</li> <li>The Termius installer can be found here:  Windows or Mac  (only the free version is needed)</li> </ul>"},{"location":"Deploying-OCP/Pre-Req/#a-bastion-machine","title":"A Bastion Machine","text":"<p>Attention</p> <p>Ubuntu 20.04 has been depricated for use with the Daffy Scripts! While it may still work, it is no-longer being tested. Please upgrade to v22.04. </p> <ul> <li>Create Bastion Instructions</li> <li>Ubuntu 22.04 (Minimum Requirements: 2   CPU, 2GB    Memory) with full root access  (VSphere-UPI, -IPI and -MSP)</li> <li>Ubuntu 22.04 (Minimum Requirements: 60+ CPU, 128GB+ Memory) with full root access  (KVM-UPI)</li> <li>Ubuntu 22.04 (Minimum Requirements: 4+  CPU, 32GB+  Memory, 100 GB Disk) with full root access  (AirGap Bastion/Jump Box Mirror Registry)</li> <li>RHEL 8.X     (Minimum Requirements: 2   CPU, 2GB    Memory) with full root access  (VSphere-UPI, -IPI and -MSP)</li> <li>RHEL 8.X     (Minimum Requirements: 4+  CPU, 32GB+  Memory, 100 GB Disk) with full root access  (AirGap Bastion/Jump Box Mirror Registry)</li> </ul>"},{"location":"Deploying-OCP/Pre-Req/#red-hat-pull-secret","title":"Red Hat pull secret","text":"<ol> <li> <p>If you or your customer does not have a Red Hat pull secret:</p> <ol> <li>Sign up for 60 day trail for OpenShift: RedHat Pull secret Site <ul> <li>Important: If you're installing on a customer owned platform account or an on-prem customer datacenter, you MUST instruct your customer to register for a trial account and use their pull secret for the install. Do not use your own pull secret for customer engagements.</li> </ul> </li> <li>\u200b\u200b\u200b\u200bSign up for IBM/Red Hat partner program<ul> <li>NOTE: All IBMers are entitled to the Red Hat partner program. Your Red Hat pull secret can ONLY be used for training and demo purposes. Do not provide your personal pull secret to customers.</li> </ul> </li> </ol> </li> <li> <p>If you have a Red Hat account, you can find your existing pull secret here:  </p> <ol> <li>Login to Red Hat</li> <li>Scroll down the page until you see \"Tokens\" and download the pull secret</li> </ol> </li> <li> <p>Accessing Red Hat entitlements from your IBM Cloud Paks:</p> <ul> <li>Accessing-red-hat-entitlements-from-your-cloud-paks</li> </ul> </li> </ol>"},{"location":"Deploying-OCP/Pre-Req/#ibm-entitlement-key","title":"IBM Entitlement Key","text":"<ol> <li>If you need to get your own IBM entitlement key, you can get it here<ul> <li>Copy to clipboard and save to a local file</li> </ul> </li> <li>If you need create one for a customer, you can submit a request here</li> <li>Customers can use these links to request their own trial keys here</li> </ol> Core Steps"},{"location":"Deploying-OCP/ROKS/","title":"ROKS","text":""},{"location":"Deploying-OCP/ROKS/#roks-install","title":"ROKS Install","text":"<p>Warning</p> <p>Only ROKS type of classic or satellite supported today.  VPC Type not Supported</p> <p>At this point, you have a bastion machine where you have installed the Daffy tool, created your core environment-name-env.sh and can execute the install of OCP on ROKS.</p>"},{"location":"Deploying-OCP/ROKS/#platform-requirements","title":"Platform Requirements","text":"<p>To use Daffy to provision Red Hat OpenShift Kubernetes Services on IBM Cloud (ROKS), there are some platform info and requirements that need to be gathered or met. We have a simple doc that you should refer to that lists all providers and what would be needed. For ROKS, it breaks down to the following basic two items:</p> <p>Account Details - The account that you plan to install ROKS on</p> <p>Account Type - The account type needed to perform the install</p> <p>For a detailed list of the above, you can read the Daffy Provider Requirements. Please review before proceeding.</p> <p>https://ibm.box.com/v/DaffyProviderRequirements</p>"},{"location":"Deploying-OCP/ROKS/#finding-provider-details","title":"Finding Provider Details","text":"<p>To use Daffy to install ROKS, you must find the provider details. Luckily, Daffy automatically walks you through this process using IBM Cloud CLI. Below are the steps you can use to make sure you use the right information.</p> <p>Account: To find more details on managing your IBM Cloud account, you can refer to this:</p> <p>https://cloud.ibm.com/docs/account?topic=account-account-getting-started</p> <p>You must have an IBMid before logging in. The link above can help create one. If you are an IBM employee, your IBMid is most likely some numbers followed by your name.</p> <p>You can list your Account ID from the drop down: </p> <p> Location/Zone: To find a list of available data center locations/zones, you can refer to this:</p> <p>https://cloud.ibm.com/docs/overview?topic=overview-locations#mzr-table</p> <p>Note</p> <p>Daffy currently only supports single datacenter location installs with classic infrastructure</p>"},{"location":"Deploying-OCP/ROKS/#zones","title":"Zones","text":"<p>Regions are collections of zones. Zones have high-bandwidth, low-latency network connections to other zones in the same region. In order to deploy fault-tolerant applications that have high availability, IBM recommends deploying applications across multiple zones and multiple regions. This helps protect against unexpected failures of components, up to and including a single zone or region.</p> <p>Choose regions that makes sense for your scenario. For example, if you only have customers in the US, or if you have specific needs that require your data to live in the US, it makes sense to store your resources in zones in the dal13 zone or in the wdc07 zone. Daffy currently defaults to dal13 when deploying a ROKS cluster.</p> <p>https://cloud.ibm.com/docs/containers?topic=containers-regions-and-zones#locations</p>"},{"location":"Deploying-OCP/ROKS/#account-types","title":"Account types","text":"<p>Your IBM Cloud account includes many interacting components and systems for resource, user, and access management. Concepts like how certain components are connected or how access works help you in understanding how to set up your account type. Many features are free to use regardless of account type.</p> <p>Account Type: For you to use Daffy to install on ROKS, you need to have a Pay-As-You-Go or subscription IBM Cloud account:</p> <p>https://cloud.ibm.com/docs/account?topic=account-accounts</p>"},{"location":"Deploying-OCP/ROKS/#environment-file","title":"Environment File","text":"<p>Deploying OpenShift on ROKS only requires one entry to your existing core environment file (&lt;ENVIRONMENT_NAME&gt;-env.sh).</p> <p>Note</p> <p>You can look in the samples directory on your bastion for example of ROKS install : /data/daffy/env/samples/roks-msp-env.sh</p> <p>You can copy the sample file to build your new environment  file. <pre><code>cp /data/daffy/env/samples/roks-msp-env.sh /data/daffy/env/&lt;ENVIRONMENT_NAME&gt;-env.sh\n</code></pre> Valid Options:</p> <p>OCP_INSTALL_TYPE= roks-msp</p> <p>Optional:</p> <p>ROKS_ZONE=dal13</p> <pre><code>OCP_INSTALL_TYPE=\"roks-msp\"\n#ROKS_ZONE=\"dal13\"\n</code></pre>"},{"location":"Deploying-OCP/ROKS/#execution","title":"Execution","text":"<p>To deploy your OCP cluster to ROKS, run the build.sh script from the /data/daffy/ocp directory. The installer will ask you a number of questions to login to IBM Cloud via the CLI. When prompted with a region, select any but stay within your geography. For instance, us-south. This is used to talk with IBM Cloud via the right API endpoint. <pre><code>/data/daffy/ocp/build.sh &lt;ENVIRONMENT_NAME&gt;\n</code></pre> Once your cluster is fully deployed, you can access the help menu which has a number of options.</p> <p>Note</p> <p>&lt;ENVIRONMENT_NAME&gt; is the first part of your name that you used for the -env.sh file <pre><code>/data/daffy/ocp/build.sh &lt;ENVIRONMENT_NAME&gt; --help\n</code></pre> <p>Here is a full example for deploying OpenShift on ROKS with the Daffy process.</p> <p>Installing Cloud Paks</p>"},{"location":"Deploying-OCP/ROSA/","title":"ROSA","text":""},{"location":"Deploying-OCP/ROSA/#rosa-install","title":"ROSA Install","text":"<p>At this point, you have a bastion machine where you have installed the Daffy tool, and ready to created your core environment-name-env.sh and so you can execute the install of OCP on AWS via ROSA.</p>"},{"location":"Deploying-OCP/ROSA/#platform-requirements","title":"Platform Requirements","text":"<p>To use Daffy on AWS, there are some platform info and requirements that need to be gathered or met. We have a simple doc that you should refer to that list all providers and what would be needed.  For AWS ROSA, it breaks down to the following basic three items:</p> <p>Account Details - The account that you plan to install OpenShift</p> <p>Permissions - The permissions need to perform the install</p> <p>Quota - The ability to add new workload to that platform</p> <p>For detailed list of ROSA instructions, refer to the ROSA documentation.</p> <p>https://docs.openshift.com/rosa/welcome/index.html</p> <p>For detailed list of the above, you can find in the Daffy Provider Requirements. Please review before proceeding.</p> <p>https://ibm.box.com/v/DaffyProviderRequirements</p>"},{"location":"Deploying-OCP/ROSA/#one-time-rosa-setup","title":"One time ROSA setup","text":"<p>First login to the AWS Console, and search for ROSA service.</p> <p></p> <p>Click Enable OpenShift button in the ROSA Services page.</p> <p></p> <p>Next, go to the Red Hat token page to get a ROSA token for login and generate or load an OpenShift Cluster Manager API Token. Daffy scripts will prompt you for this token at install time.</p> <p>Note</p> <p>You must have a valid Red Hat login id. You can signup with any email address</p> <p>https://console.redhat.com/openshift/token/rosa</p> <p></p>"},{"location":"Deploying-OCP/ROSA/#finding-provider-details","title":"Finding Provider Details","text":"<p>To install OpenShift on AWS ROSA using Daffy, the hardest part can be finding the provider details.</p> <p>To create or use an existing AWS Access Key ID you can refer to this:</p> <p>https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_access-keys.html?icmpid=docs_iam_console#Using_CreateAccessKey</p> <p>Note: Use the Identity and Access Management (IAM) service to manage access keys.</p> <ol> <li>Select Search - find   IAM   service</li> <li>You can create a new access key or use an existing key. The access key must have authority to the account you wan to install OpenShift into.</li> </ol> <p> </p> <p>Secret Access Key: The secret access key is ONLY displayed at the time of creation. When you create the access key, you will then have the opportunity to capture the secret access key</p> <p>Note</p> <p>This is sensitive information, please make sure you store this in a secure location</p> <p>The screen to the right is an example of what you will see when you create a NEW access Key. </p> <p>Region:</p> <p>For you to use Daffy to install on AWS you need to choose a valid region identifier. This will be the target region you are planning to deploy OpenShift into.  </p> <p>To see a complete list of available AWS Regions, you can select the region drop down list in the AWA Portal. This will be in the upper right hand corner next to your account name. (See picture to the right)</p> <p>Note: Take note of the region identifier such as: us-east-2. This is the value you would use to deploy a OCP cluster into the US East (Ohio) region. </p> <p>Permission:</p> <p>Within your AWS project, you would need to go to IAM  Section and make sure the user that is associated with your Access Key is assigned the correct roles.  </p> <p>At minimum, you need to have this role: AdministratorAccess</p> <p>Please see the requirements doc for more information!</p> <p></p>"},{"location":"Deploying-OCP/ROSA/#environment-file","title":"Environment File","text":"<p>Deploying the OpenShift on AWS only requires three entries to your existing core environment file (&lt;ENVIRONMENT_NAME&gt;-env.sh).</p> <p>Note</p> <p>You can look in the samples directory on your bastion for example of ROSA install : /data/daffy/env/samples/rosa-msp-env.sh</p> <p>You can copy the sample file to build your new environment  file: <pre><code>cp /data/daffy/env/samples/rosa-msp-env.sh /data/daffy/env/&lt;ENVIRONMENT_NAME&gt;-env.sh\n</code></pre></p> <p> Valid Options: </p> Variable Name Info Default Value Required OCP_INSTALL_TYPE Install type must be rosa-msp Yes AWS_REGION AWS region you want to deploy to Yes AWS_ACCESS_KEY_ID Your AWS Access Key ID Yes AWS_CREATE_EFS_STORAGE Do you want to create EFS storage false No <pre><code>#ROSA Base Settings\n####################\nOCP_INSTALL_TYPE=\"rosa-msp\"\nAWS_REGION=\"us-east-2\"\nAWS_ACCESS_KEY_ID=\"123YOURACCESSKEYID12\"\n\n#Enable Features\n#############################\n#AWS_CREATE_EFS_STORAGE=\"true\"\n\n#ROSA Override Settings\n####################\n</code></pre> <p>Storage</p> <p>If you plan to install a cloud pak and/or need storage,  ODF/OCS is not currently supportd on ROSA. If you want daffy to setup the EFS Storage for you, you need to set the AWS_CREATE_EFS_STORAGE to true. Or you can build our use some other Storage provider.  </p> <p>Client Secret</p> <p>It will prompt you for the Client Secret during the install</p>"},{"location":"Deploying-OCP/ROSA/#execution","title":"Execution","text":"<p>To deploy your OCP cluster to AWS ROSA, run the build.sh script from the /data/daffy/ocp directory:</p> <pre><code>/data/daffy/ocp/build.sh &lt;ENVIRONMENT_NAME&gt;\n</code></pre> <p>Once your cluster is fully deployed you can access the help menu which has a number of options.</p> <p>Note</p> <p>&lt;ENVIRONMENT_NAME&gt; is the first part of your name that you used for the &lt;ENVIRONMENT_NAME&gt;-env.sh file</p> <p>Installing Cloud Paks</p>"},{"location":"Deploying-OCP/Restricted-Network/","title":"Air Gap","text":""},{"location":"Deploying-OCP/Restricted-Network/#slide-show","title":"Slide Show","text":"1 / 3 Intro 1 / 3 Caption Text 1 / 3 Caption Text 1 / 3 Caption Text 1 / 3 Caption Text 1 / 3 Caption Text 1 / 3 Caption Text 1 / 3 Caption Text 1 / 3 Caption Text \u276e \u276f <p>Download Slides</p>"},{"location":"Deploying-OCP/Restricted-Network/#overview","title":"Overview","text":"<p>Deploying OpenShift in a restricted network (air gap), can be done with the Daffy Restricted Network scripts. This page will primarily focus on the steps to perform an air gap install. However, please note the Proxy Install approach can be a viable option and is a easier approach. Regardless of which one you choose, both install options will result in an OpenShift cluster with restricted network access. Also, note that the air gap install is more complicated and will require a JUMP BOX that has internet access to the Quay Repository.</p> <p>There are two basic options for deployment</p> <ul> <li>Option 1. Air gap install</li> <li>Option 2. Proxy install </li> </ul>"},{"location":"Deploying-OCP/Restricted-Network/#proxy-install","title":"Proxy Install","text":"<p>OpenShift environments can deny direct access to the internet and instead have an HTTP or HTTPS proxy available. This can be done during an cluster installation. This would allow the cluster to be in a restricted network but gain access to public registries via proxy server.  The bastion and cluster you build must also have direct access to the proxy server. This proxy server can then control what access is granted via whitelist.</p> <p>If you are doing proxy install, you only need one bastion and no jump box.  This bastion must have access to the restricted network and the proxy server as well.</p> <pre><code>#Airgap - Proxy Install#\n######################################################################################\nOCP_PROXY_HTTP_PROXY=\nOCP_PROXY_HTTPS_PROXY=\nOCP_PROXY_NO_PROXY=\n</code></pre> <p>Info</p> <p>At this point for a proxy install, you can skip the following steps and move to the last step and install your cluster.</p>"},{"location":"Deploying-OCP/Restricted-Network/#airgap-install","title":"Airgap Install","text":"<p>OpenShift environments can deny direct access to the internet completely. With this method you must replicate or mirror the public registries local in your restricted network. To do this we will use the following terms:</p> <ol> <li>Jump box - machine that will have direct access to the internet to mirror (download) public registry software.</li> <li>Bastion  - machine that will install the cluster and have direct access to the restricted network where the cluster will run.</li> </ol>"},{"location":"Deploying-OCP/Restricted-Network/#requirements","title":"Requirements","text":"<ol> <li>Jump Box  - 4X32 300GB Disk (disk depending on what you plan to mirror)<ol> <li>Default will need storage /data/export(100GB) and /mirror/registry(100GB)</li> </ol> </li> <li>Bastion   - 4X32 300GB Disk (disk depending on what you plan to mirror)<ol> <li>Default will need storage /data/import(100GB) and /mirror/registry(100GB)</li> </ol> </li> </ol>"},{"location":"Deploying-OCP/Restricted-Network/#environment-file","title":"Environment File","text":"<p>You should build your environment file based on the final install you want to build and should include those values in this file as well.  </p> <p>The following values are required specifically for airgap install.(Repo Build and Airgap)</p> Variable Name Info Install Type Required default LOCAL_REGISTRY_ENABLED Do you want this to be an air gap install Both Yes false LOCAL_REGISTRY_DNS_NAME This is your jump box DNS name(can Be IP value) Both Yes LOCAL_REGISTRY_IP This is your jump box IP address Both Yes LOCAL_AIRGAP_REGISTRY_DNS_NAME This is your restricted network bastion DNS name(can Be IP value) airgap Yes(if Airgap) LOCAL_AIRGAP_REGISTRY_IP This is your restricted network bastion box IP address airgap Yes(if Airgap) <pre><code>#Local Registry Info\n###############################\nLOCAL_REGISTRY_ENABLED=\"true\"\nLOCAL_REGISTRY_DNS_NAME=\"&lt;LOCAL IP&gt;\"\nLOCAL_REGISTRY_IP=\"&lt;LOCAL IP&gt;\"\n#LOCAL_AIRGAP_REGISTRY_DNS_NAME=\"&lt;Optional DNS Name&gt;\"\n#LOCAL_AIRGAP_REGISTRY_IP=\"&lt;Optional DNS Name&gt;\"\n</code></pre>"},{"location":"Deploying-OCP/Restricted-Network/#repo-only","title":"Repo Only","text":"<p>If you want to just build a repo on your internet facing bastion, here is only settings you need to mirror. <pre><code>#Local Registry Info\n###############################\nLOCAL_REGISTRY_ENABLED=\"true\"\nLOCAL_REGISTRY_IP=\"&lt;LOCAL IP&gt;\"\n#LOCAL_REGISTRY_DNS_NAME=\"&lt;Optional DNS Name&gt;\"\n</code></pre></p> Click to expand! <pre><code>#Catalogs to mirror\n####################\nOCP_CATALOG_MIRRORS=\"compliance-operator,container-security-operator,file-integrity-operator,local-storage-operator,ocs-operator\"\n\n#Directory Info\n####################\nOCP_REGISTRY_ROOT=\"/mirror/registry\"\nOCP_AIRGAP_EXPORT=\"${DATA_DIR}/export/airgap\"\nOCP_AIRGAP_IMPORT=\"${DATA_DIR}/import/airgap\"\nOCP_AIRGAP_EXPORT_FREE_DISK_SIZE_NEEDED=\"100\"\nOCP_AIRGAP_IMPORT_FREE_DISK_SIZE_NEEDED=\"100\"\n\n#Cert Info\n####################\nCA_CERT_OU=\"ca.${CLUSTER_NAME}.${BASE_DOMAIN}\"\nLOCAL_REGISTRY_CERTS_FOLDER=\"${DATA_DIR}/${PROJECT_NAME}/certs\"\n\n#Registry Info\n####################\nLOCAL_REGISTRY_PORT=\"8443\"\nLOCAL_OCP_REPOSITORY_NAME=\"ocp4/openshift4\"\nLOCAL_OLM_MIRROR_REPOSITORY_NAME=\"olm-mirror\"\n</code></pre>"},{"location":"Deploying-OCP/Restricted-Network/#overrides","title":"Overrides","text":"<p>Current values that you can add to your own environment file to override if needed but not required:</p>"},{"location":"Deploying-OCP/Restricted-Network/#mirror-locally","title":"Mirror locally","text":"<p>From your jump box run the following command: <pre><code>/data/daffy/ocp/registry/build.sh &lt;ENVIRONMENT_NAME&gt;\n</code></pre></p>"},{"location":"Deploying-OCP/Restricted-Network/#export-files","title":"Export Files","text":"<p>After the first command runs, it will display all the files that it created and you will now need to move them to your bastion box in the restricted network. You can do this via portable USB disk, scp, etc.  You just need to move these files any way you can to your restricted network bastion.</p>"},{"location":"Deploying-OCP/Restricted-Network/#move-files","title":"Move Files","text":"<p>In our example we will move via scp because our jump box has access to the bastion.  This could be via firewall or it has dual NIC cards (Public Nic/Private Nic).</p> <pre><code>ssh &lt;BASTION-IP&gt; mkdir -p /data/import/airgap\nscp /data/export/airgap/* &lt;BASTION-IP&gt;:/data/import/airgap\n</code></pre>"},{"location":"Deploying-OCP/Restricted-Network/#prepare-bastion","title":"Prepare Bastion","text":"<p>Once all the files are on the bastion in the restricted network, you can run the script that was built from the previous step and copied over.  This will untar all files, install all command line tools, and also install daffy locally.</p> <p>Info</p> <p>It does not mirror the registry or build the local registry, but gets the bastion ready for that next step.</p> <pre><code>/data/import/airgap/airgap-prep.sh\n</code></pre>"},{"location":"Deploying-OCP/Restricted-Network/#build-local-mirror","title":"Build Local Mirror","text":"<p>From your bastion box run the following command: <pre><code>/data/daffy/ocp/registry/build.sh &lt;ENVIRONMENT_NAME&gt;\n</code></pre></p>"},{"location":"Deploying-OCP/Restricted-Network/#install-cluster","title":"Install Cluster","text":"<p>Now you would just follow the normal steps to build your OpenShift registry.  </p> <pre><code>/data/daffy/ocp/build.sh &lt;ENVIRONMENT_NAME&gt;\n</code></pre>"},{"location":"Deploying-OCP/Restricted-Network/#demo-video","title":"Demo Video","text":"HTML Video embed"},{"location":"Deploying-OCP/TechZone/","title":"TechZone","text":""},{"location":"Deploying-OCP/TechZone/#techzone-install","title":"TechZone Install","text":"<p>At this point, you have a bastion machine where you have installed the Daffy tool, created your core &lt;ENVIRONMENT_NAME&gt;-env.sh.  Depending on if you use TechZone to build your cluster, you may skip Daffy Step 1, which builds your cluster. You will not use the bastion to build your cluster, but follow the steps to have TechZone build your cluster.  Once that is done, you would move on with the Daffy process and install your Cloud Paks from your new bastion.</p> <p>Warning</p> <p>For IBM and Business Partners use only</p>"},{"location":"Deploying-OCP/TechZone/#tech-zone-awsazure","title":"Tech Zone AWS/Azure","text":"<p>(prebuilt cluster):   https://techzone.ibm.com/collection/third-party-cloud-openshift-clusters</p> <ul> <li>Does NOT include bastion with request</li> <li>With this option, you will skip Daffy step 1 (build cluster) as TechZone will build for you (this would fail as you do not have access to create cluster with a TechZone setup)</li> <li>You still need to have a bastion and core values in your &lt;ENVIRONMENT_NAME&gt;-env.sh</li> <li>BASE_DOMAIN is not needed in your environment file</li> <li>OCP_INSTALL_TYPE is needed based on provider you pick(aws-ip or azure-ipi).  All other provider info is not needed in your environment file</li> </ul>"},{"location":"Deploying-OCP/TechZone/#tech-zone-vsphere-gym","title":"Tech Zone VSphere Gym","text":"<p>(Daffy builds cluster): https://techzone.ibm.com/collection/ocp-gymnasium</p> <ul> <li>Includes bastion with request</li> <li>Once you create the request and the VSPhere environment has been provisioned, it will create your own bastion and give you the instructions on how to use Daffy in that environment with the prebuilt /data/daffy/env/vmware-ipi-env.sh</li> <li>To run daffy, you must be full root, do not just sudo the script (sudo /data/daffy/ocp/build.sh vmware-ipi).   Run next command first:<pre><code>  sudo su -\n</code></pre> </li> </ul>"},{"location":"Deploying-OCP/TechZone/#techzone-vmwareroks","title":"TechZone VMWare/Roks","text":"<p>(prebuilt cluster): https://techzone.ibm.com/collection/5fb3200cec8dd00017c57f20</p> <ul> <li>Does NOT include bastion with request</li> <li>No need for VPN, public direct access to cluster</li> <li>Comes with IBM Storage for ROKS and ODF with VSphere</li> <li>With this option, you will skip Daffy step 1 (build cluster) as TechZone will build for you (this will fail as you do not have access to create cluster with a TechZone setup)</li> <li>Once you create the request, you would follow the same steps as ROKS with Daffy</li> <li>You still need to have a bastion and core values in your &lt;ENVIRONMENT_NAME&gt;-env.sh  </li> <li>Extra settings to change in your environment file:<ol> <li>ROKS_PROVIDER=techzone #(ROKS only)</li> <li>DAFFY_DEPLOYMENT_TYPE=TechZone</li> </ol> </li> <li>BASE_DOMAIN is not needed in your environment file</li> <li>OCP_INSTALL_TYPE is needed based on provider you pick(roks-msp or vsphere-ipi).</li> </ul> <p>Installing Cloud Paks</p>"},{"location":"Deploying-OCP/TechZoneTiles/","title":"TechZoneTiles","text":""},{"location":"Deploying-OCP/TechZoneTiles/#techzone-tile-info","title":"TechZone Tile Info","text":"<p>The Purpose of the following TechZone Tiles are to install the full OpenShift/Cloud Pak stack for you. This would enable you to start using your Cloud Pak of choice, without deep skills in OpenShift or Cloud Pak install process. From a Reservetaion to Cloud Pak use, in a few hours of runtime, but 5 min of your time.  Sit back, let Daffy/PakInstaller Build the full stack for you. </p> <ol> <li>This collection of tiles will build a public facing cluster.  No VPN is require or provided</li> <li>More then one user is allowed to access your Cluster and Cloud Pak.  The only limitation is your cloud pak and the size that was built.</li> <li>All Access Points via trusted SSL Certs. No Browser Popup</li> </ol> <p>You can access the Tech Zone Environment Tiles here:</p> <ol> <li> <p>Cloud Pak for Business Automation</p> </li> <li> <p>Cloud Pak for Integration</p> </li> </ol> <p>Warning</p> <p>For IBM and Business Partners use only</p>"},{"location":"Deploying-OCP/TechZoneTiles/#over-all-process","title":"Over all Process","text":"<p>Overall Process can take 4 - 8 hours. </p> <p>Of that, it will take 2 to 3 hours to install OpenShift, Cloud Pak base and install/start the services for the cloud pak. You will get email at each major step.  Then you will have to wait the 1 -5 hours until your serivces are fully running.</p>"},{"location":"Deploying-OCP/TechZoneTiles/#status-email-messages","title":"Status Email Messages","text":"<p>1)  After Cluster is installed - OpenShift is up and running,  you can logon to the OpenShift Cluster console</p> Email Sample <p></p> <p>2)  After the Cloud Pak Base is installed - Cloud Pak Name Spaces is create and All Cloud Pak Operators are installed</p> Email Sample <p></p> <p>3) After the Cloud Pak Services have be installed - This is that the starting process, servics are not ready yet. This is final email as the Pak Installer automation process is over. Now the cloud pak operators take over and can take another 1 - 5 hours</p> Email Sample <p></p> <p></p> TechZone Emails <p>TechZone will also send you emails, at the beginning of the reserveration and and the end once the PakInstaller automation is done. You will get a total of 5 emails for this process. </p>"},{"location":"Deploying-OCP/TechZoneTiles/#pak-installer-portal","title":"Pak Installer Portal","text":"<p>To make the process simple, the end result for your TechZone Reservation will be a single link to the Pak Installer Portal. This portal is your one stop for status and connection info for you cluster and the cloud pak. The Portal requires userid/password to connect to as this cluster is a public facing cluster. </p> <p>These pages update every few minutes with any updated information.  This portal is where you would go to monitor the status of your servies and to find out when they are ready.  You will not get email when they are ready as this can take from 1 - 5 hours to finish. Please use the \"Cloud Pak Status\" tab to monitor the status.</p>"},{"location":"Deploying-OCP/TechZoneTiles/#tabs","title":"Tabs","text":"<p>The Pak Installer portal has 5 tabs at the top of the page. Each tab is designed to give you more info for that topic.</p> Tabs Preview <p></p>"},{"location":"Deploying-OCP/TechZoneTiles/#instructions","title":"Instructions","text":"<p>This page to explain the tabs within the Portal. Points to the Daffy Public documentation site from within your local cluster Portal page </p>"},{"location":"Deploying-OCP/TechZoneTiles/#bastion","title":"Bastion","text":"<p>Connection info to the bastion(RHEL server).  It will gave you the host name, port, userID and password you can use if you want to connect to the bastion server for this new environment. </p> Info <p>This info is not required to be used to run/access your Cloud Pak, but here if you needed it. </p>"},{"location":"Deploying-OCP/TechZoneTiles/#openshift-console","title":"OpenShift Console","text":"<p>Connection info to your new OpenShift Cluster. You can see both ids you can use to connect to the cluster via command line or Web URl. It also shows overall cluster info like version, bastion OS, daffy version, etc. </p> Info <p>This info is not required to be used to run/access your Cloud Pak, but here if you needed it. </p>"},{"location":"Deploying-OCP/TechZoneTiles/#cloud-pak-status","title":"Cloud Pak Status","text":"<p>This page is where can see overall status of the cloud pak and all the sevcies you requeted. The Cloud Pak Status page will auto refresh every few minutes and where you can watch as your services come online. As stated before, this can take 1 -5 hours to get to \"Ready\" state for all services componets. Once services are ready, it will then update the console tab with your console connection info.</p>"},{"location":"Deploying-OCP/TechZoneTiles/#cloud-pak-console","title":"Cloud Pak Console","text":"<p>This page is where can see the connection info for your cloud Pak. This is the page where you will get your URL's, usernames and passwords once the services are up and running. The page will refresh every few minutes. You will not see any values until the operators have completed there task. Once operators are done, it will populate this page with all of your connection details.</p>"},{"location":"Deploying-OCP/TechZoneTilesBeta/","title":"TechZoneTilesBeta","text":""},{"location":"Deploying-OCP/TechZoneTilesBeta/#techzone-tile-info-beta","title":"TechZone Tile Info - Beta","text":"<p>The Purpose of the following TechZone Tiles are to install the full OpenShift/Cloud Pak stack for you. This would enable you to start using your Cloud Pak of choice, without deep skills in OpenShift or Cloud Pak install process. From a Reservetaion to Cloud Pak use, in a few hours of runtime, but 5 min of your time.  Sit back, let Daffy/PakInstaller Build the full stack for you. </p> <ol> <li>This collection of tiles will build a public facing cluster.  No VPN is require or provided</li> <li>More then one user is allowed to access your Cluster and Cloud Pak.  The only limitation is your cloud pak and the size that was built.</li> <li>All Access Points via trusted SSL Certs. No Browser Popup</li> </ol> <p>You can access the Tech Zone Environment Tiles here:</p> <ol> <li> <p>Cloud Pak for Business Automation</p> </li> <li> <p>Cloud Pak for Integration</p> </li> </ol>"},{"location":"Deploying-OCP/TechZoneTilesBeta/#over-all-process","title":"Over all Process","text":"<p>Overall Process can take 4 - 8 hours. </p> <p>Of that, it will take 2 to 3 hours to install OpenShift, Cloud Pak base and install/start the services for the cloud pak. You will get email at each major step.  Then you will have to wait the 1 -5 hours until your serivces are fully running.</p>"},{"location":"Deploying-OCP/TechZoneTilesBeta/#status-email-messages","title":"Status Email Messages","text":"<p>1)  After Cluster is installed - OpenShift is up and running,  you can logon to the OpenShift Cluster console</p> Email Sample <p></p> <p>2)  After the Cloud Pak Base is installed - Cloud Pak Name Spaces is create and All Cloud Pak Operators are installed</p> Email Sample <p></p> <p>3) After the Cloud Pak Services have be installed - This is that the starting process, servics are not ready yet. This is final email as the Pak Installer automation process is over. Now the cloud pak operators take over and can take another 1 - 5 hours</p> Email Sample <p></p> <p></p> TechZone Emails <p>TechZone will also send you emails, at the begging of the reserveration and and the end once the PakInstaller automation is done. You will get a total of 5 emails for this process. </p>"},{"location":"Deploying-OCP/TechZoneTilesBeta/#pak-installer-portal","title":"Pak Installer Portal","text":"<p>To make the process simple, the end result for your TechZone Reservation will be a single link to the Pak Installer Portal. This portal is your one stop for status and connection info for you cluster and the cloud pak. The Portal requires userid/password to connect to as this cluster is a public facing cluster. </p> <p>These pages update every few minutes with any updated information.  This portal is where you would go to monitor the status of your servies and to find out when they are ready.  You will not get email when they are ready as this can take from 1 - 5 hours to finish. Please use the \"Cloud Pak Status\" tab to monitor the status.</p>"},{"location":"Deploying-OCP/TechZoneTilesBeta/#tabs","title":"Tabs","text":"<p>The Pak Installer portal has 5 tabs at the top of the page. Each tab is designed to give you more info for that topic.</p> Tabs Preview <p></p>"},{"location":"Deploying-OCP/TechZoneTilesBeta/#instructions","title":"Instructions","text":"<p>This page to explain the tabs within the Portal. Points to the Daffy Public documentation site from within your local cluster Portal page </p>"},{"location":"Deploying-OCP/TechZoneTilesBeta/#bastion","title":"Bastion","text":"<p>Connection info to the bastion(RHEL server).  It will gave you the host name, port, userID and password you can use if you want to connect to the bastion server for this new environment. </p> Info <p>This info is not required to be used to run/access your Cloud Pak, but here if you needed it. </p>"},{"location":"Deploying-OCP/TechZoneTilesBeta/#openshift-console","title":"OpenShift Console","text":"<p>Connection info to your new OpenShift Cluster. You can see both ids you can use to connect to the cluster via command line or Web URl. It also shows overall cluster info like version, bastion OS, daffy version, etc. </p> Info <p>This info is not required to be used to run/access your Cloud Pak, but here if you needed it. </p>"},{"location":"Deploying-OCP/TechZoneTilesBeta/#cloud-pak-status","title":"Cloud Pak Status","text":"<p>This page is where can see overall status of the cloud pak and all the sevcies you requeted. The Cloud Pak Status page will auto refresh every few minutes and where you can watch as your services come online. As stated before, this can take 1 -5 hours to get to \"Ready\" state for all services componets. Once services are ready, it will then update the console tab with your console connection info.</p>"},{"location":"Deploying-OCP/TechZoneTilesBeta/#cloud-pak-console","title":"Cloud Pak Console","text":"<p>This page is where can see the connection info for your cloud Pak. This is the page where you will get your URL's, usernames and passwords once the services are up and running. The page will refresh every few minutes. You will not see any values until the CP4B operators have completed there task. Once operators are done, it will populate this page with all of your connection details.</p>"},{"location":"Deploying-OCP/VSphere/","title":"VSphere","text":""},{"location":"Deploying-OCP/VSphere/#platform-requirements","title":"Platform Requirements","text":"<p>To install Daffy on VSphere, there are some platform info and requirements that need to be gathered or met. We have a simple doc that you should refer to that list all providers and what would be needed.</p> <p>There is a number of permissions you MUST have as a user on VCenter for deployment of OpenShift on VSphere.</p> <p>Please refer to the requirements document for the specific requirements for IPI or UPI installs on VSphere: https://ibm.box.com/v/DaffyProviderRequirements </p>"},{"location":"Deploying-OCP/VSphere/#finding-provider-details","title":"Finding Provider Details","text":"<p>To install Daffy on VSphere, the hardest part can be finding the provider details in the VCenter Console.</p> <p>Some of the variables are easily understood, but a few can be a bit tricky to find.</p> Variable Name Info Install Type Required VSPHERE_DATASTORE This is the name of the VSphere Datastore IPI/UPI Yes VSPHERE_CLUSTER The VSphere cluster is NOT the same as your OpenShift Cluster name. This is variable is referring to the VSphere Cluster name. IPI/UPI Yes VSPHERE_NETWORK1 This is the VSphere VLAN name IPI/UPI Yes VSPHERE_DATACENTER This is the name of the VSphere Datacenter IPI/UPI Yes VSPHERE_FOLDER This is the location of where you will store the NEW VM's IPI/UPI Yes VSPHERE_API_VIP This is an UNUSED IP address that will be utilized by the OpenShift IPI installer to provision the API Virtual IP Address IPI Yes VSPHERE_INGRESS_VIP This is an UNUSED IP address that will be utilized by the OpenShift IPI installer to provision the Ingress Virtual IP Address IPI Yes VSphere UPI Only VSPHERE_ISO_DATASTORE This is the name of the datastore where the the coreos iso is located UPI Yes VSPHERE_ISO_IMAGE_BASE This is the directory within the datastore where the iso image is located UPI Yes BASTION_HOST This is the name of the bastion host, IP or  DNS value UPI No BASTION_USER This is non admin id on the bastion host that has authorzation to logon via SSH to bastion UPI Yes OCP_INSTALL_GATEWAY This is gatway to assign new new VMs UPI Yes OCP_FORWARD_DNS This is IP for DNS Server to forward request to UPI Yes OCP_INSTALL_DNS This is IP for the DNS Server that will have OpenShift Node entries UPI Yes OCP_INSTALLBOOTSTRAP_IP This is Bootstrap IP you want to assign UPI Yes OCP_INSTALL_MASTER1_IP This is Master 1 IP you want to assign UPI Yes OCP_INSTALL_MASTER2_IP This is Master 2 IP you want to assign UPI Yes OCP_INSTALL_MASTER3_IP This is Master 3 IP you want to assign UPI Yes OCP_INSTALL_WORKER1_IP This is Worker 1 IP you want to assign UPI Yes OCP_INSTALL_WORKER2_IP This is Worker 2 IP you want to assign UPI Yes OCP_INSTALL_WORKER3_IP This is Worker 3 IP you want to assign UPI Yes OCP_INSTALL_WORKER4_IP This is Worker 4 IP you want to assign UPI Yes OCP_INSTALL_WORKER5_IP This is Worker 5 IP you want to assign UPI Yes OCP_INSTALL_WORKER6_IP This is Worker 6 IP you want to assign UPI Yes"},{"location":"Deploying-OCP/VSphere/#setting-up-dns","title":"Setting up DNS","text":"HTML Video embed"},{"location":"Deploying-OCP/VSphere/#environment-file","title":"Environment File","text":"<p>Below are the VSphere IPI specific environment variables that must be defined in the /data/daffy/env/&lt;ENVIRONMENT_NAME&gt;-env.sh file.</p> <p>Note</p> <p>You can look in the samples directory on your bastion for example of VSphere install : /data/daffy/env/samples/vsphere-ipi-env.sh</p> <p>Valid Options:</p> <pre><code>#VSphere Platform Info\n########################\nVSPHERE_USERNAME=\"userid\"     VSPHERE_HOSTNAME=\"vsphere-host-name\"\nVSPHERE_DATASTORE=\"datastore\"     VSPHERE_CLUSTER=\"cluster-name\".   VSPHERE_NETWORK1=\"vlan-name\"      VSPHERE_DATACENTER=\"vsphere-datacenter\"     VSPHERE_FOLDER=\"/${VSPHERE_DATACENTER}/vm/${CLUSTER_NAME}\"\n\n#IPI Only\n########################  \nOCP_INSTALL_TYPE=\"vsphere-ipi\"\nVSPHERE_API_VIP=\"xx.xxx.xxx.xxx\"\nVSPHERE_INGRESS_VIP=\"xx.xxx.xxx\"\n\n#UPI Only\n########################\n#OCP_INSTALL_TYPE=\"vsphere-upi\"\n#VSPHERE_ISO_DATASTORE=\"iso-datastore\"     \n#VSPHERE_ISO_IMAGE_BASE=\"datastore-directory\"\n#BASTION_HOST=\"xx.xxx.xxx\"    \n#BASTION_USER=\"bastion\"\n#OCP_INSTALL_GATEWAY=\"xx.xxx.xxx\"\n#OCP_FORWARD_DNS=\"xx.xxx.xxx\"\n#OCP_INSTALL_DNS=${BASTION_HOST}\n#OCP_NODE_SUBNET_MASK=\"24\"\n#OCP_INSTALLBOOTSTRAP_IP=\"xx.xxx.xxx\"\n#OCP_INSTALL_MASTER1_IP=\"xx.xxx.xxx\"\n#OCP_INSTALL_MASTER2_IP=\"xx.xxx.xxx\"\n#OCP_INSTALL_MASTER3_IP=\"xx.xxx.xxx\"\n#OCP_INSTALL_WORKER1_IP=\"xx.xxx.xxx\"\n#OCP_INSTALL_WORKER2_IP=\"xx.xxx.xxx\"\n#OCP_INSTALL_WORKER3_IP=\"xx.xxx.xxx\"\n#OCP_INSTALL_WORKER4_IP=\"xx.xxx.xxx\"\n#OCP_INSTALL_WORKER5_IP=\"xx.xxx.xxx\"\n#OCP_INSTALL_WORKER6_IP=\"xx.xxx.xxx\"\n\n\n#Storage Option for OpenShift\n########################\n#OCP_CREATE_OPENSHIFT_CONTAINER_STORAGE=true\n</code></pre> <p>Optional:</p> <p>OCP_CREATE_OPENSHIFT_CONTAINER_STORAGE=true</p> <p>If you plan to install a cloud pak and/or need storage, you need to set the flag to setup OCS Storage.</p>"},{"location":"Deploying-OCP/VSphere/#execution","title":"Execution","text":"<p>To deploy your cluster, run the build.sh script from the /data/daffy/ocp directory:</p> <pre><code>/data/daffy/ocp/build.sh &lt;ENVIRONMENT_NAME&gt;\n</code></pre> <p>Once your cluster is fully deployed, you can access the help menu which has a number of options.</p> <p>Note</p> <p>&lt;ENVIRONMENT_NAME&gt; is the first part of your name that you used for the &lt;ENVIRONMENT_NAME&gt;-env.sh file</p> <p>Deploying an OpenShift cluster on VSphere using the Daffy scripts (using VSPhere-IPI install type):</p> <pre><code>/data/daffy/ocp/build.sh &lt;ENVIRONMENT_NAME&gt; --help\n</code></pre> <p>Installing Cloud Paks</p>"},{"location":"Deploying-OCP/extra_features/","title":"Extra features","text":""},{"location":"Deploying-OCP/extra_features/#build-flags","title":"Build Flags","text":"<p>To get a list of the build flags, run the following command and replace the ENVIRONMENT_NAME with the name you gave in your environment file.</p> <pre><code>/data/daffy/ocp/build.sh ENVIRONMENT_NAME --help\n</code></pre>"},{"location":"Deploying-OCP/kvm/","title":"KVM","text":""},{"location":"Deploying-OCP/kvm/#kvm-install","title":"KVM Install","text":"<p>At this point, you have a bastion machine where you have installed the Daffy tool, created your core environment-name-env.sh, and can execute the install of OCP on KVM.</p>"},{"location":"Deploying-OCP/kvm/#platform-requirements","title":"Platform Requirements","text":"<p>To use Daffy on Kernel-based Virtual Machine, there are some platform info and requirements that need to be gathered or met. We have a simple doc that you should refer to that list all providers and what would be needed.  For KVM, it breaks down to the following three basic items:</p> <p>Hardware - enough to run the OCP Cluster based on T-Shirt Sizing</p> <p>OS Version - Ubuntu 20.0.4 (only supported by Daffy)</p> <p>Permission - full root authority</p> <p>For detailed list of the above, you can find in the Daffy Provider Requirements. Please review before proceeding.</p> <p>https://ibm.box.com/v/DaffyProviderRequirements</p> <p>Public DNS Setup:</p> <p>You will need to create a DNS entries and domain. For the OpenShift install, you need the following:</p> <ol> <li>Registered DNS Name - myexample.com</li> <li>DNS Entries - myexample-com<ol> <li>api.${CLUSTER_NAME}.myexample.com        ---&gt;    ${YOUR.BASTION.IP}</li> <li>api-int.${CLUSTER_NAME}.myexample.com    ---&gt;    ${YOUR.BASTION.IP}</li> <li>*.apps.${CLUSTER_NAME}.myexample.com         ---&gt;    ${YOUR.BASTION.IP}</li> </ol> </li> </ol> <p>Setting up DNS for KVM Deployment with OpenShift:  INSERT VIDEO Here</p>"},{"location":"Deploying-OCP/kvm/#environment-file","title":"Environment File","text":"<p>Deploying the OpenShift on Kernel-based Virtual Machine only requires three entries to your existing core environment file (&lt;ENVIRONMENT_NAME&gt;-env.sh) plus a local service account file.</p> <p>Note: you can look in the samples directory on your bastion for example of Kernel-based Virtual Machine install : /data/daffy/env/samples/kvm-upi-env.sh</p> <p>You can copy the sample file to build your new environment  file.</p> <p>cp /data/daffy/env/samples/kvm-upi-env.sh /data/daffy/env/-env.sh <p>Valid Options:</p> <p>OCP_INSTALL_TYPE= kvm-upi</p> <p>Optional:</p> <p>BASTION_HOST=\"xxx.xxx.xxx.xxx\"</p> <p>If your host does not have its own public IP address, you need to specify the bastion IP address that the OCP cluster would use to reach your bastion host, i.e. its local IP address you used to connect to the bastion.</p> <p>If you plan to install a cloud pak and/or need storage, you need to set the flag to setup OCS Storage:</p> <p>OCP_CREATE_OPENSHIFT_CONTAINER_STORAGE=true</p>"},{"location":"Deploying-OCP/kvm/#execution","title":"Execution","text":"<p>To deploy your OCP cluster to Kernel-based Virtual Machine, run the build.sh script from the /data/daffy/ocp directory</p> <pre><code>/data/daffy/ocp/build.sh &lt;ENVIRONMENT_NAME&gt;\n</code></pre> <p>Once your cluster is fully deployed you can access the help menu which has a number of options.</p> <p>Note: &lt;ENVIRONMENT_NAME&gt; is the first part of your name that you used for the &lt;ENVIRONMENT_NAME&gt;-env.sh file <pre><code>/data/daffy/ocp/build.sh &lt;ENVIRONMENT_NAME&gt; --help\n</code></pre></p> <p>Here is a full example for deploying OpenShift on Kernel-based Virtual Machine with the Daffy process.</p> <p> Installing Cloud Paks</p>"},{"location":"More/CommonErrors/","title":"Common Errors","text":""},{"location":"More/CommonErrors/#failed-to-watch-v1clusterversion","title":"Failed to watch *v1.ClusterVersion?","text":"<p>The following error can happen on any platform.  Its not major error, more of an FYI.  Just means that during the cluster setup, the install program was disconnected. Most of the time the install is fine, just a few hiccups. You can ignore this error. <pre><code>E0908 07:05:20.266253 2142422 reflector.go:138] k8s.io/client-go/tools/watch/informerwatcher.go:146: Failed to watch *v1.ClusterVersion: failed to list *v1.ClusterVersion: Get \"https://api.lambda02.daffy-installer.com:6443/apis/config.openshift.io/v1/clusterversions?fieldSelector=metadata.name%3Dversion&amp;limit=500&amp;resourceVersion=0\": EOF\nW0908 07:05:31.158887 2142422 reflector.go:324] k8s.io/client-go/tools/watch/informerwatcher.go:146: failed to list *v1.ClusterVersion: Get \"https://api.lambda02.daffy-installer.com:6443/apis/config.openshift.io/v1/clusterversions?fieldSelector=metadata.name%3Dversion&amp;limit=500&amp;resourceVersion=0\": EOF\nI0908 07:05:31.159042 2142422 trace.go:205] Trace[1375718625]: \"Reflector ListAndWatch\" name:k8s.io/client-go/tools/watch/informerwatcher.go:146 (08-Sep-2022 07:05:21.098) (total time: 10060ms):\nTrace[1375718625]: ---\"Objects listed\" error:Get \"https://api.lambda02.daffy-installer.com:6443/apis/config.openshift.io/v1/clusterversions?fieldSelector=metadata.name%3Dversion&amp;limit=500&amp;resourceVersion=0\": EOF 10060ms (07:05:31.158)\nTrace[1375718625]: [10.060656161s] [10.060656161s] END\n</code></pre></p>"},{"location":"More/CommonErrors/#failed-vcpus-needed","title":"FAILED vCPUs needed","text":"<p>What does this mean? <pre><code>Precheck IBM Cloud VPC quota (LOG /data/daffy/log/cpdata/ocp/ibmcloud-vpc-quota.log )\n################################################################\nibmcloud resource group-create daffy-quota-test\nibmcloud is target --gen 2\nibmcloud is vpc-create daffy-quota-test-vpc \u2014resource-group-name daffy-quota-test\nibmcloud is subnet-create daffy-quota-test-subnet daffy-quota-test-vpc us-south-3 --ipv4-cidr-block 10.240.128.0/18 --resource-group-name daffy-quota-test\n\u274c  FAILED vCPUs needed 176\n</code></pre> The IBM VPC zone that you're trying to deploy to does not have enough quota of VPC to have a successful deployment. VPC quota is based on the region so you have two options:</p> <ol> <li>Deployed to a new zone</li> <li>Request via IBM cloud ticket to increase your quota to 500</li> </ol> <p>https://cloud.ibm.com/docs/vpc?topic=vpc-quotas</p>"},{"location":"More/Glossary/","title":"Glossary","text":""},{"location":"More/Glossary/#glossary","title":"Glossary","text":""},{"location":"More/Glossary/#bastion","title":"Bastion","text":"<p>A bastion host is a server whose purpose is to provide access to a private/public network from an external/internal network, such as the Internet or private lan. We use this primarily for the install and tempory management of the OpenShift Cluster. The server will have install tools, access to destired openshift cluster network and may be removed post install. </p>"},{"location":"More/Glossary/#operator-catalogs","title":"Operator catalogs","text":"<p>An Operator catalog is a repository of metadata that Operator Lifecycle Manager (OLM) can query to discover and install Operators and their dependencies on a cluster. OLM always installs Operators from the latest version of a catalog. As of OpenShift Container Platform 4.6, Red Hat-provided catalogs are distributed using index images.</p> <p>An index image, based on the Operator bundle format, is a containerized snapshot of a catalog. It is an immutable artifact that contains the database of pointers to a set of Operator manifest content. A catalog can reference an index image to source its content for OLM on the cluster.</p> <p>As catalogs are updated, the latest versions of Operators change, and older versions may be removed or altered. In addition, when OLM runs on an OpenShift Container Platform cluster in a restricted network environment, it is unable to access the catalogs directly from the internet to pull the latest content.</p> <p>As a cluster administrator, you can create your own custom index image, either based on a Red Hat-provided catalog or from scratch, which can be used to source the catalog content on the cluster. Creating and updating your own index image provides a method for customizing the set of Operators available on the cluster, while also avoiding the aforementioned restricted network environment issues.</p>"},{"location":"More/Glossary/#cloud-pak","title":"Cloud Pak","text":"<p>IBM Cloud Paks are packaged based on solution domains and harness the combined power of container technology and IBM enterprise expertise to help organizations solve their most pressing challenges:</p> <ul> <li>IBM Cloud Pak\u00ae for Data: Unify cloud storage and simplify the collection, organization and analysis of data.</li> <li>IBM Cloud Pak\u00ae for Business Automation: Automate business operations to achieve better performance.</li> <li>IBM Cloud Pak\u00ae for Watson AIOps: Place AI at the core of your IT operations tool chain. Automate operations management decisions while resolving real-world operations management scenarios to deliver actionable insights.</li> <li>IBM Cloud Pak\u00ae for Integration: Automate application and data flows to improve client experiences. Connect your applications and data wherever they live. Get new tools for automated integrations based on APIs that extend capability and modernize flexibility for ongoing adaption.</li> <li>IBM Cloud Pak\u00ae for Security: Generate deeper insights into threats, and orchestrate actions for scalability and automated responses.</li> <li>IBM Cloud Pak\u00ae for Network Automation: Automate networks to deliver zero-touch operations.</li> </ul>"},{"location":"More/Glossary/#cloud-provider","title":"Cloud Provider","text":"<p>A cloud service provider is a third-party company offering a cloud-based platform, infrastructure, application, or storage services. Much like a homeowner would pay for a utility such as electricity or gas, companies typically have to pay only for the amount of cloud services they use, as business demands require.</p> <p>Besides the pay-per-use model, cloud service providers also give companies a wide range of benefits. Businesses can take advantage of scalability and flexibility by not being limited to physical constraints of on-premises servers, the reliability of multiple data centers with multiple redundancies, customization by configuring servers to your preferences, and responsive load balancing that can easily respond to changing demands. Though businesses should also evaluate security considerations of storing information in the cloud to ensure industry-recommended access and compliance management configurations and practices are enacted and met.</p>"},{"location":"More/Glossary/#cluster","title":"Cluster","text":"<p>Multiple computing nodes or hosts that work together to support an application or middleware such as a database. A cluster is a group of inter-connected computers or hosts that work together to support applications and middleware (e.g. databases).  In a cluster, each computer is referred to as a \u201cnode\u201d. Unlike grid computers, where each node performs a different task, computer clusters assign the same task to each node. Nodes in a cluster are usually connected to each other through high-speed local area networks. Each node runs its own instance of an operating system. A computer cluster may range from a simple two-node system connecting two personal computers to a supercomputer with a cluster architecture. Computer clusters are often used for cost-effective high-performance computing (HPC) and high availability (HA). If a single component fails in a computer cluster, the other nodes continue to provide uninterrupted processing.  A computer cluster can provide faster processing speed, larger storage capacity, better data integrity, greater reliability and wider availability of resources. Computer clusters are usually dedicated to specific functions, such as load balancing, high availability, high performance or large-scale processing.</p>"},{"location":"More/Glossary/#control-plane","title":"Control Plane","text":"<p>In network routing, the control plane is the part of the router architecture that is concerned with drawing the network topology, or the information in a routing table that defines what to do with incoming packets. Control plane functions, such as participating in routing protocols, run in the architectural control element.[1] In most cases, the routing table contains a list of destination addresses and the outgoing interface(s) associated with each. Control plane logic also can identify certain packets to be discarded, as well as preferential treatment of certain packets for which a high quality of service is defined by such mechanisms as differentiated services.</p> <p>Depending on the specific router implementation, there may be a separate forwarding information base that is populated by the control plane, but used by the high-speed forwarding plane to look up packets and decide how to handle them.</p> <p>In computing, the control plane is the part of the software that configures and shuts down the data plane.[2] By contrast, the data plane is the part of the software that processes the data requests.[3] The data plane is also sometimes referred to as the forwarding plane.</p> <p>The distinction has proven useful in the networking field where it originated, as it separates the concerns: the data plane is optimized for speed of processing, and for simplicity and regularity. The control plane is optimized for customizability, handling policies, handling exceptional situations, and in general facilitating and simplifying the data plane processing.[4] [5]</p> <p>The conceptual separation of the data plane from the control plane has been done for years.[6] An early example is Unix, where the basic file operations are open, close for the control plane and read write for the data plane.[7]</p> <p>Control Plane</p>"},{"location":"More/Glossary/#domain-name-system-dns","title":"Domain Name System (DNS)","text":"<p>The Domain Name System (DNS) is the hierarchical and decentralized naming system used to identify computers, servr ices, and other resources reachable through the Internet or other Internet Protocol (IP) networks. The resource records contained in the DNS associate domain names with other forms of information. These are most commonly used to map human-friendly domain names to the numerical IP addresses computers need to locate services and devices using the underlying network protocols, but have been extended over time to perform many other functions as well.</p> <ul> <li>In a nutshell, its a system where your local computer can call other computers on the internet to translate a website name to a computer IP address.  Think of it as the internet phone book.</li> </ul> <p>Domain Name System</p>"},{"location":"More/Glossary/#dns-registrarregistry","title":"DNS Registrar/Registry","text":"<p>A domain name registrar is a company that manages the reservation of Internet domain names. A domain name registrar must be accredited by a generic top-level domain (gTLD) registry or a country code top-level domain (ccTLD) registry. A registrar operates in accordance with the guidelines of the designated domain name registries.</p> <ul> <li>In a nutshell, its a company where you can buy and register a website name.   Like \"mywebsite.com\" You would now own that name and can have that name redirected to any computer IP address.</li> </ul> <p>A domain name registry is a database of all domain names and the associated registrant information in the top level domains of the Domain Name System (DNS) of the Internet that enables third party entities to request administrative control of a domain name. Most registries operate on the top-level and second-level of the DNS.</p> <ul> <li>In a nutshell, its where website names are stored and has a mapping from name to IP Address.   Like \"www.mywebsite.com\" points to 169.45.45.55</li> </ul> <p>Domain Name Registrar</p>"},{"location":"More/Glossary/#enviornment-namefile","title":"Enviornment Name/File","text":"<p>You may see reference to ,  this is what you name your environment and the base name of the file to store your details for that environment. <p>Example  gamm01-env.sh is your file where gamma01 is your  <p>Best practice, but not required, is to name your environment the same as your cluster as this is the core of your environment.</p>"},{"location":"More/Glossary/#ingress-operator","title":"Ingress Operator","text":"<p>The Ingress Operator makes it possible for external clients to access your service by deploying and managing one or more HAProxy-based Ingress Controllers to handle routing. You can use the Ingress Operator to route traffic by specifying OpenShift Container Platform Route and Kubernetes Ingress resources. Configurations within the Ingress Controller, such as the ability to define endpointPublishingStrategy type and internal load balancing, provide ways to publish Ingress Controller endpoints.</p> <ul> <li>In a nutshell, it allows external access to your cluster and all the running pods/services/applications. Its basically an inbound router for your OpenShift Cluster.</li> </ul> <p>Ingress Operator</p>"},{"location":"More/Glossary/#install-type-upi","title":"Install Type UPI","text":"<p>User-provisioned Infrastructure (UPI) provides the ablith to deploy the Openshift container platform (OCP) on existing infrastuure that you created. The user, will create the VMs, networks, loadbalancer, etc.  You must create all require infrastrue before you can start the openshift install process.  Most users use terraform to build these resources.  </p> <p>With Daffy, we call it a UPI, but daffy is the user in the perspective.  Daffy will build all the infrastrure components for you.  From the user point of view, with daffy, this performs like an IPI install. </p>"},{"location":"More/Glossary/#install-type-ipi","title":"Install Type IPI","text":"<p>Installer-provisioned Infrastructure (IPI) provides a full-stack installation and setup of the Openshift container platform (OCP). It creates Bootstrapping node which will take care deploying the cluster. It will create the VMs, networks, loadbalancer, etc.  It creates all require infrastrue during the install for you. With he corrent promission on the provider, this the easiest install.</p>"},{"location":"More/Glossary/#instal-type-msp","title":"Instal Type MSP","text":"<p>Daffy coined this acronym. Managed Service Provider(MSP).  This is when a provider does the heavy lifting of install of Openshift container platform (OCP) and also the management of the running cluster. This includes ROSA(AWS), ROKS(IBM) and ARO(Azure). Daffy will call the exposed API from the provider to provision the cluster for you. </p>"},{"location":"More/Glossary/#master-node","title":"Master Node","text":"<p>The master node is responsible for running several Kubernetes processes that are absolutely necessary to run and manage the cluster properly:</p> <ul> <li> <p>API Server: This is essentially the entry-point to the Kubernates cluster, which itself is a container. This is the process that allows communication between different Kubernetes clients and the cluster. The clients include the UI, if we are using the Kubernetes Dashboard, the API if we are running scripts, or the command-line tool. All these clients talk to the API Server to interact with the cluster.</p> </li> <li> <p>Controller Manager: This keeps track of the state of the cluster. It keeps an eye on the cluster and checks whether a node needs to be repaired or restarted.</p> </li> <li> <p>Scheduler: Scheduler ensures proper pod placement on the worker nodes based on several factors such as the available resources and the current load on the cluster.</p> </li> <li> <p>etcd: This is the key-value storage responsible for holding the state of the cluster at any given time. etcd has the configuration information and status data of each node in the cluster. etcd snapshots allow us to recover the whole cluster state, hence it is used in backing up and restoring a cluster.</p> </li> </ul>"},{"location":"More/Glossary/#namespace","title":"Namespace","text":"<p>What is a namespace in OpenShift? Namespaces. A Kubernetes namespace provides a mechanism to scope resources in a cluster. In OpenShift Online, a project is a Kubernetes namespace with additional annotations. Namespaces provide a unique scope for: Named resources to avoid basic naming collisions.</p> <p>Referance</p>"},{"location":"More/Glossary/#node","title":"Node","text":"<p>A node is a virtual or bare-metal machine in a Kubernetes cluster. Worker nodes host your application containers, grouped as pods. The control plane nodes run services that are required to control the Kubernetes cluster. In OpenShift Container Platform, the control plane nodes contain more than just the Kubernetes services for managing the OpenShift Container Platform cluster.</p> <p>Referance</p>"},{"location":"More/Glossary/#operator","title":"Operator","text":"<p>Red Hat\u00ae OpenShift\u00ae Operators automate the creation, configuration, and management of instances of Kubernetes-native applications. Operators provide automation at every level of the stack\u2014from managing the parts that make up the platform all the way to applications that are provided as a managed service.</p> <p>Referance</p>"},{"location":"More/Glossary/#pod","title":"Pod","text":"<p>Pods are the smallest deployable units of computing that you can create and manage in Kubernetes.</p> <p>A Pod (as in a pod of whales or pea pod) is a group of one or more containers, with shared storage and network resources, and a specification for how to run the containers. A Pod's contents are always co-located and co-scheduled, and run in a shared context. A Pod models an application-specific \"logical host\" it contains one or more application containers which are relatively tightly coupled. In non-cloud contexts, applications executed on the same physical or virtual machine are analogous to cloud applications executed on the same logical host.</p> <p>As well as application containers, a Pod can contain init containers that run during Pod startup. You can also inject ephemeral containers for debugging if your cluster offers this.</p>"},{"location":"More/Glossary/#service","title":"Service","text":"<p>A Kubernetes service serves as an internal load balancer. It identifies a set of replicated pods in order to proxy the connections it receives to them. Backing pods can be added to or removed from a service arbitrarily while the service remains consistently available, enabling anything that depends on the service to refer to it at a consistent address. The default service clusterIP addresses are from the OpenShift Online internal network and they are used to permit pods to access each other.</p> <p>Referance</p>"},{"location":"More/Glossary/#subscription","title":"Subscription","text":"<p>With OpenShift, you need to have a subscription to deploy and run the cluster. This subscription is tied to a billing account for Yearly bililng and payment.  You cluster is tied to the subscription via the pull secret that is stored in the cluster.  At installed time you supplied this but can change this post install at anytime.</p>"},{"location":"More/Glossary/#storage-class","title":"Storage Class","text":"<p>A StorageClass provides a way for administrators to describe the \"classes\" of storage they offer. Different classes might map to quality-of-service levels, or to backup policies, or to arbitrary policies determined by the cluster administrators. Kubernetes itself is unopinionated about what classes represent. This concept is sometimes called \"profiles\" in other storage systems.</p>"},{"location":"More/Glossary/#worker-node","title":"Worker Node","text":"<p>The worker nodes are the part of the Kubernetes clusters which actually execute the containers and applications on them. They have two main components, the Kubelet Service and the Kube-proxy Service.</p> <ul> <li> <p>Kubelet Service: Each worker node has a Kubelet process running on it that allows the cluster to talk to each other and execute some tasks on the worker nodes, such as running application processes. It listens for instructions from the Api Server and manages containers running on the node.</p> </li> <li> <p>Kube-proxy Service: The Kube-Proxy Service is responsible for enabling communication between services within the cluster.</p> </li> </ul> <p>These worker nodes have docker containers for each application running on them. There may be a different number of containers running on each node depending on the distribution of the workload.</p> <p>Worker nodes are generally more powerful than master nodes because they have to run hundreds of clusters on them. However, master nodes hold more significance because they manage the distribution of workload and the state of the cluster.</p>"},{"location":"More/secure-server-ssh-only-login/","title":"Enable SSH Key ONLY Login - Disable Password","text":"<p>It is highly recommended that you disable password login for root access and ONLY allow login with ssh key. To do that follow these steps. </p> <p>Warning</p> <p>As a recommmendation, it is advised that you have a second session open while you are making these chagnes. Once you enable the above changes you will no longer be able to log into the server using passwords. If your ssh key configuration is not corrrect, you will not be able to log in. Leave the second session open until you verify that you can in fact log in with your ssh key.  </p>"},{"location":"More/secure-server-ssh-only-login/#setup-ssh","title":"Setup SSH","text":"<p>create new authorized_keys file </p> <pre><code> mkdir -p /root/.ssh\n touch authorized_keys\n</code></pre> <p>verify you can log in with your ssh key</p>"},{"location":"More/secure-server-ssh-only-login/#edit-the-ssh-configuration","title":"Edit the ssh configuration","text":"<p>Edit the ssh configuratioh file</p> <pre><code> vim /etc/ssh/sshd_config\n</code></pre> <p>Modify these to values.</p> <pre><code> ChallengeResponseAuthentication no\n PasswordAuthentication no\n</code></pre>"},{"location":"More/secure-server-ssh-only-login/#reload-the-ssh-config","title":"Reload the SSH Config","text":"<p>Execute the reload command to enable the changes(as root user). </p> <pre><code> Ubuntu\n systemctl restart ssh.service\n\n RHEL\n systemctl restart sshd.service\n</code></pre>"},{"location":"More/FAQ/","title":"FAQ","text":""},{"location":"More/FAQ/#why-the-name-daffy","title":"Why the name Daffy?","text":"<p>Its a great name and mascot.</p>"},{"location":"More/FAQ/#what-is-your-favorite-cloud-pak","title":"What is your favorite Cloud Pak?","text":"<p>Cloud Pak for Business Automation</p>"},{"location":"More/FAQ/#can-this-be-run-on-the-xbox-platform","title":"Can this be run on the XBOX platform?","text":"<p>No, sorry.</p>"},{"location":"More/FAQ/#daffy-slack-channels","title":"Daffy Slack Channels:","text":"<pre><code>#daffy-user-group\n</code></pre>"},{"location":"More/FAQ/#i-put-invalid-or-wrong-red-hat-pull-secret-during-the-ocp-install-process-how-do-you-fix","title":"I put invalid or wrong Red Hat Pull Secret during the OCP install process, how do you fix?","text":"<p>You can run the security-cleanup.sh script to remove existing pull secret.</p> <p><pre><code>/data/daffy/security-cleanup.sh  --pullSecret\n</code></pre> Follow all the instructions from the above script, then you can run daffy install process again</p>"},{"location":"More/FAQ/#i-put-invalid-or-wrong-ibm-entitlement-during-the-cp-install-process-how-do-you-fix","title":"I put invalid or wrong IBM Entitlement during the CP install process, how do you fix?","text":"<p><pre><code>/data/daffy/security-cleanup.sh  --ibm\n</code></pre> Follow all the instructions from the above script, then you can run daffy install process again</p>"},{"location":"More/FAQ/#how-can-i-create-my-own-bastion","title":"How can I create my own bastion?","text":"<p>We have a document outlining how to create your own bastion in two ways</p> <ol> <li>A paid IBM Cloud Account</li> <li>IBM Technology Zone</li> </ol> <p>Create Bastion Steps</p>"},{"location":"More/FAQ/#roks-unbound-immediate-persistentvolumeclaims","title":"ROKS unbound immediate PersistentVolumeClaims","text":"<p>This means the cluster has requested storage from MSP provider(IBM ROKS) and it is still provisioning it.  It can take a few minutes or hours at times or sometimes never get provisioned. If it is stuck for more then 60 minutes, best option is to delete the stuck pod requesting storage or you can run the daffy rebuild process to get it working in a few hours.</p>"},{"location":"More/FAQ/#what-does-rebuildsh-do","title":"What does rebuild.sh do?","text":"<p>Rebuild starts over with your cluster using your environment file.</p> <ol> <li>Delete Cluster and all resources it created</li> <li>Install new Cluster</li> <li>Install new Cloud Pak</li> <li>Install all services for your Cloud Pak</li> </ol> <p>rebuild.sh requires your env file, that drives all the info it would need.</p> <p>As you can see, it will destroy everything and give you new env, nothing will be saved in cluster.</p>"},{"location":"More/FAQ/Deployment-Estimates/","title":"Deployment Estimates","text":"<p>Note</p> <p>Below are some estimates for Daffy T-Shirt sizes running on each cloud platform. These are rough examples, customer accounts may have more/less discounts then the estimates we obtained.  These numbers are used to help prepare some high level cost estimates for POC's for OpenShift and IBM Cloud Paks.</p>"},{"location":"More/FAQ/Deployment-Estimates/#google-cloud-platform","title":"Google Cloud Platform","text":"<p>Calculator</p> T-Shirt Size 1 Hour 1 Day 1 Week 1 Month Large $6.69 $160.56 $1,123.92 $4,816.80 <p>Estimate on 03-06-2022</p>"},{"location":"More/FAQ/Deployment-Estimates/#amazon-web-services","title":"Amazon Web Services","text":"<p>Calculator</p> T-Shirt Size 1 Hour 1 Day 1 Week 1 Month Large Coming Coming Coming Coming"},{"location":"More/FAQ/Deployment-Estimates/#azure","title":"Azure","text":"<p>Calculator</p> T-Shirt Size 1 Hour 1 Day 1 Week 1 Month Large Coming Coming Coming Coming"},{"location":"More/FAQ/Deployment-Estimates/#red-hat-openshift-kubernetes-services-on-ibm-cloud","title":"Red Hat OpenShift Kubernetes Services on IBM Cloud","text":"<p>Calculator</p> T-Shirt Size 1 Hour 1 Day 1 Week 1 Month Large $5.70 $136.80 $957.60 $4,104.00 <p>Estimate on 03-07-2022</p>"},{"location":"More/FAQ/IBM-Entitlement-Keys/","title":"IBM Entitlement Keys","text":"<p>To run software from the IBM Entitled Registry, you must supply your entitlement key as a Kubernetes pull secret. Below are links for customers to obtain trial keys.</p> Components Length Link Cloud Pak for Data 60 days https://www.ibm.com/account/reg/us-en/signup?formid=urx-42212 Cloud Pak for Business Automation 60 days https://www.ibm.com/account/reg/us-en/signup?formid=urx-44505 Cloud Pak for Integration 60 days https://www.ibm.com/account/reg/us-en/signup?formid=urx-46640 Cloud Pak for AIOps 60 days https://www.ibm.com/account/reg/us-en/signup?formid=urx-51074 Cloud Pak for Security 60 days PENDING"},{"location":"Overview/","title":"Index","text":"<p> Demo Video OpenShift install on Azure with Cloud Pak for Data and Watson Knowledge Catalog <p></p> HTML Video embed"},{"location":"Overview/Process/","title":"Process","text":""},{"location":"Overview/Process/#where-do-i-start","title":"Where Do I Start","text":"<p>The daffy tool allows you to start at any step.  Before you start, you need to see what your goal is.  First step is to get the Prereq and Core steps done.</p> <p>Prereq Core Steps</p> <p>Then you need to decide your end goal and what you want daffy to assist with.</p> <ol> <li>I need to build full stack (OpenShift, Cloud Pak Operators and Cloud Pak Services)  Run Step 1, 2 and 3</li> <li>Do you need to build just the OpenShift Cluster?  Run Step 1 </li> <li>Do you need to deploy just the Cloud Pak Operators?  Run Step 2</li> <li>Do you need to deploy just cloud Pak Services?  Run Step 3</li> </ol>"},{"location":"Overview/Process/#step-1","title":"Step 1","text":"<p>With Step one you will need information for your provider.  This includes provider ids, secrets, regions, etc.  The items that daffy will need to create your cluster in the given provider. In the end you are asking daffy to do a full install of OpenShift in the provider, so it needs all the info necessary to access the provider and build out infrastructure.</p> <p>Deploying OCP</p>"},{"location":"Overview/Process/#step-2","title":"Step 2","text":"<p>If you are starting at step 2, you would NOT need provider info(provider ids, secrets, regions, etc.).  The only thing step 2 needs is OpenShift cluster info.  Access to cluster and cluster and Cloud Pak details. If you used step 1, you already have access to cluster.  But if you are staring at step 2, you first need to logon on to your cluster via the oc login command from your bastion host where you plan to run daffy.</p> <p>Deploying Cloud-Paks</p>"},{"location":"Overview/Process/#step-3","title":"Step 3","text":"<p>If you are starting at step 3, you would NOT need provider info(provider ids, secrets, regions, etc.).  The only thing step 3 needs is OpenShift cluster info.  Access to cluster and cluster and Cloud Pak details. But if you are staring at step 3, you first need to logon on to your cluster via the oc login command from your bastion host where you plan to run daffy.</p> <p>Deploying Cloud-Paks</p>"},{"location":"Overview/Tools-Installed/","title":"Tools Installed","text":"<p> <p></p> <p>Throughout the Daffy process, it will install all support tools it would need. Depending on the step of Daffy you are running and feature it is using, the tools will be different. If the tool is present already and the correct version, it will not be installed.</p> Core      Tool When oc always openshfit-install always kubectl always nmon always curl always nano always vim always tree always wget always jq always dnsutils always openssh-client always grepcidr always acl always(RHEL) dos2unix always nfs-kernel-server NFS Option only  nfs-common NFS Option only  openssl always  Cloud CLI      Tool When awscli AWS Install only azure-cli  Azure Install only  gcloud GCP Install only  cloudctl CP4D/CP4BA python2 cloudctl Flag python3 cloudctl Flag pip2 cloudctl Flag pip3 cloudctl Flag ibmcloud ROKS/IBM-IPI/IBM DNS/Secrets Manager ibm-cp-automation  CP4BA Install Only podman All Cloud Paks VSphere      Tool When rhcos-4.6.XX-x86_64-vmware.x86_64.ova VSphere IPI only rhcos-4.7.XX-x86_64-vmware.x86_64.ova  VSphere IPI only rhcos-4.9.XX-x86_64-vmware.x86_64.ova VSphere IPI only  rhcos-4.10.XX-x86_64-vmware.x86_64.ova VSphere IPI only  govc VSphere Install only Airgap      Tool When mirror-registry build local registry only rhcos-4.6.XX-x86_64-live.x86_64.iso  build local registry only rhcos-4.7.XX-x86_64-live.x86_64.iso  build local registry only rhcos-4.9.XX-x86_64-live.x86_64.iso  build local registry only rhcos-4.10.XX-x86_64-live.x86_64.iso  build local registry only UPI      Tool When haproxy KVM or VSphere install Only dnsmasq KVM or VSphere install Only KVM      Tool When lvm2 KVM install Only  bridge-utils KVM install Only  qemu-kvm KVM install Only  virtinst KVM install Only   libvirt-daemon  KVM install Only   virt-manager  KVM install Only   cifs-utils   KVM install Only  libosinfo-bin   KVM install Only  uvtool KVM install Only  matchbox KVM install Only   rhcos-live-initramfs.x86_64.img  KVM install Only  rhcos-live-kernel-x86_64  KVM install Only  rhcos-live-rootfs.x86_64.img  KVM install Only  KVM Dashboard      Tool When apache2 VMDashboard Only php VMDashboard Only  libapache2-mod-php  VMDashboard Only  php-mysql  VMDashboard Only  php-xml  VMDashboard Only  php-libvirt-php  VMDashboard Only  python  VMDashboard Only  RPA Server      Tool When mssql2019 RPA Server openldap RPA Server CP4D      Tool When cpd-cli CP4D 4.5.X"},{"location":"Security/","title":"Index","text":""},{"location":"Security/#security","title":"Security","text":"IBM Secrets Manager SSL Certificates"},{"location":"Security/#overview","title":"Overview","text":"<p>Trusted certificates can be used to create secure connections to a server via the Internet. A certificate is essential in order to circumvent a malicious party which happens to be on the route to a target server which acts as if it were the target. Such a scenario is commonly referred to as a man-in-the-middle attack. The client uses the CA certificate to authenticate the CA signature on the server certificate, as part of the authorizations before launching a secure connection. Usually, client software\u2014for example, browsers\u2014include a set of trusted CA certificates. This makes sense, as many users need to trust their client software. A malicious or compromised client can skip any security check and still fool its users into believing otherwise.</p>"},{"location":"Security/#useful-links","title":"Useful Links","text":"<ul> <li>SSL Certificates</li> <li>IBM Secrets Manager</li> </ul>"},{"location":"Security/IBMSecretsManager/","title":"IBM Cloud Secrets Manager","text":""},{"location":"Security/IBMSecretsManager/#ibm-cloud-secrets-manager","title":"IBM Cloud Secrets Manager","text":""},{"location":"Security/IBMSecretsManager/#assumptions","title":"Assumptions","text":"<p>With IBM Cloud Secrets Manager, you can create, lease, and centrally manage secrets that are used in IBM Cloud services or your custom-built applications. Secrets are stored in a dedicated instance of Secrets Manager, built on open source HashiCorp Vault.</p> <p>For daffy to be able to download your certs from IBM Secrets Manager, we just need a few variables.  Below are the variables you will need and how to obtain them from the IBM Cloud Web Portal IBM .</p> <p>Info</p> <p>We will not show you how to setup IBM Secrets Manager, Cert Authority or request your certs. Please follow the standard IBM Doc listed in the useful links below.</p>"},{"location":"Security/IBMSecretsManager/#environment-variables","title":"Environment variables","text":"Variable Name Info Required IBM_SECRET_CERT_API_KEY The API key that can access your Secrets Yes(Will Prompt at runtime if the other three are present) IBM_SECRET_SERVICE_API_URL The host name for your Secret Service Yes IBM_SECRET_CERT_ID_INGRESS_API The ID for your API Cert Yes IBM_SECRET_CERT_ID_INGRESS_APPS The ID for your APPS Cert Yes"},{"location":"Security/IBMSecretsManager/#building-certs","title":"Building Certs","text":"<p>When you build your certs, you have two options.</p> <ol> <li> <p>You can build a cert for each URL (*.apps &amp; api ) </p> <ol> <li>Apps Cert <ol> <li>Common Name - *.apps.${CLUSTER}.${BASE_DOMAIN}</li> </ol> </li> <li>API Cert <ol> <li>Common Name - api.${CLUSTER}.${BASE_DOMAIN}</li> </ol> </li> </ol> </li> <li> <p>You can build a single cert with both URLs as alt names of the cert</p> <ol> <li>Common Name - ${CLUSTER}.${BASE_DOMAIN}</li> <li>Alt Names   - ${CLUSTER}.${BASE_DOMAIN}, *.apps.${CLUSTER}.${BASE_DOMAIN}, api.${CLUSTER}.${BASE_DOMAIN}</li> </ol> </li> </ol> <p>Info</p> <p>If you build one cert, you would just use the same cert ID for both apps and api values in env file</p>"},{"location":"Security/IBMSecretsManager/#find-your-values","title":"Find Your Values","text":""},{"location":"Security/IBMSecretsManager/#cert-api-key","title":"Cert API Key","text":"<p>IBM_SECRET_CERT_API_KEY</p> <p>In your IBM Cloud account, you need to create/use an API key that has authority to access your secrets.</p> <ul> <li>IBM Cloud API Keys</li> </ul> Create Key <p></p>"},{"location":"Security/IBMSecretsManager/#service-api-url","title":"Service API Url","text":"<p>IBM_SECRET_SERVICE_API_URL</p> <p>This is the Service API Url for your Secretes Manger Instance</p> Get Service URL <p></p>"},{"location":"Security/IBMSecretsManager/#cert-id-ingress-api","title":"Cert ID Ingress API","text":"<p>IBM_SECRET_CERT_ID_INGRESS_API</p> <p>Each Cert you create has a unique ID, we will use that ID to download the cert via the command line.</p> Secret (Cert) Show Details <p></p> Secret (Cert) ID <p></p>"},{"location":"Security/IBMSecretsManager/#cert-id-ingress-apps","title":"Cert ID Ingress APPS","text":"<p>IBM_SECRET_CERT_ID_INGRESS_APPS</p> <p>Each Cert you create has a unique ID, we will use that ID to download the cert via the command line.</p> Secret (Cert) Show Details <p></p> Secret (Cert) ID <p></p> <p>There is one important fact about the apps cert.  It needs to be a wild card certs </p> <p>*.apps.${CLUSTER_NAME}.${BASE_DOMAIN}</p> Apps Cert Common Name <p></p>"},{"location":"Security/IBMSecretsManager/#execution","title":"Execution","text":""},{"location":"Security/IBMSecretsManager/#cluster-install-time","title":"Cluster Install Time","text":"<p>If you have your required Environment variables in your file, during the cluster install, it will pick them up and download the certs to the correct location.</p> <p>/data/daffy/certs/${CLUSTER_NAME}</p> <p>Info</p> <p>If you are missing any of the variables, the cluster install process will continue and not download your certs, no errors or warnings. You can run as post cluster install step.</p>"},{"location":"Security/IBMSecretsManager/#post-cluster-install","title":"Post Cluster Install","text":"<p>If you already have a cluster up and running, you can run the build.sh post flag to install download your certs.  Because this is explicit call to download certs, it will precheck to see all of your variables are there.  If not it will give you error and not continue.</p> <pre><code>/data/daffy/ocp/build.sh &lt;ENVIRONMENT_NAME&gt; --downloadIBMCloudIngressCerts\n</code></pre>"},{"location":"Security/IBMSecretsManager/#useful-links","title":"Useful Links","text":"<ul> <li>Getting started with Secrets Manager</li> <li>IBM Cloud Secrets Manger cli</li> </ul>"},{"location":"Security/SSLCertificates/","title":"SSL Certificates","text":""},{"location":"Security/SSLCertificates/#ssl-certificates","title":"SSL Certificates","text":""},{"location":"Security/SSLCertificates/#cluster","title":"Cluster","text":"<p>The cluster Ingress Operator makes it possible for external clients to access your service by deploying and managing one or more HAProxy-based Ingress Controllers to handle routing. You can use the Ingress Operator to route traffic by specifying OpenShift Container Platform Route and Kubernetes Ingress resources. Configurations within the Ingress Controller, such as the ability to define endpointPublishingStrategy type and internal load balancing, provide ways to publish Ingress Controller endpoints.</p> <p>By default Openshift will create self-signed, untrusted certs in the cluster.  But if you want to install a trusted cert from a Trusted Certificate Authority(CA), this will allow you to do that.  Daffy supports any valid cert from any Certificate Authority(CA), public or private.  Just needs to be in the PEM format.</p> <p>For ingress there are two major urls.  </p> <ul> <li>*.apps.${CLUSTER_NAME}.${BASE_DOMAIN}</li> <li>api.${CLUSTER_NAME}.${BASE_DOMAIN}</li> </ul>"},{"location":"Security/SSLCertificates/#assumptions","title":"Assumptions","text":"<ol> <li>Must have valid Certificate in PEM format(apps and api)</li> <li>Must have private key from the cert(apps and api)</li> <li>Must have trust chain cert form the CA that issued the cert( apps only)</li> <li>The apps Ingress cert must have wild card Common Name  *.apps.yourcluster.acme.com</li> </ol> <p>When you build your certs, you have two options.</p> <ol> <li> <p>You can build a cert for each URL (*.apps &amp; api ) </p> <ol> <li>Apps Cert <ol> <li>Common Name - *.apps.${CLUSTER}.${BASE_DOMAIN}</li> </ol> </li> <li>API Cert <ol> <li>Common Name - api.${CLUSTER}.${BASE_DOMAIN}</li> </ol> </li> </ol> </li> <li> <p>You can build a single cert with both URLs as alt names of the cert</p> <ol> <li>Common Name - ${CLUSTER}.${BASE_DOMAIN}</li> <li>Alt Names   - ${CLUSTER}.${BASE_DOMAIN}, *.apps.${CLUSTER}.${BASE_DOMAIN}, api.${CLUSTER}.${BASE_DOMAIN}</li> </ol> </li> </ol> <p>Info</p> <p>If you build one cert, you would just duplicate the cert/key files with each name for apps and api</p>"},{"location":"Security/SSLCertificates/#location-of-certs","title":"Location of certs","text":"<ol> <li> <p>Apps ingress</p> <p>When the daffy process runs, it will look in the certs folder for your certs.  If it finds all three certs, it will apply them to the cluster.</p> <p>/data/daffy/certs/${CLUSTER_NAME}/apps.${CLUSTER_NAME}.${BASE_DOMAIN}.crt</p> <p>/data/daffy/certs/${CLUSTER_NAME}/apps.${CLUSTER_NAME}.${BASE_DOMAIN}.key</p> <p>/data/daffy/certs/${CLUSTER_NAME}/${OCP_TRUSTE_CA_FILE}.pem</p> </li> <li> <p>API ingress</p> <p>When the daffy process runs, it will look in the certs folder for your certs.  If it finds both certs, it will apply them to the cluster.</p> <p>/data/daffy/certs/${CLUSTER_NAME}/api.${CLUSTER_NAME}.${BASE_DOMAIN}.crt</p> <p>/data/daffy/certs/${CLUSTER_NAME}/api.${CLUSTER_NAME}.${BASE_DOMAIN}.key</p> </li> </ol>"},{"location":"Security/SSLCertificates/#environment-variables","title":"Environment variables","text":"Variable Name Info Default value Required OCP_TRUSTE_CA_NAME The display name of your CA Cert in OpenShift lets-encrypt No OCP_TRUSTE_CA_FILE The file name of your CA Cert lets-encrypt.pem No"},{"location":"Security/SSLCertificates/#execution","title":"Execution","text":""},{"location":"Security/SSLCertificates/#cluster-install-time","title":"Cluster Install Time","text":"<p>If you have your required certs in the location, during the cluster install, it will pick them up and install them after the cluster has been setup and running.</p> <p>Info</p> <p>If only some of your certs are there or none, the cluster install process will continue, no errors or warnings.</p>"},{"location":"Security/SSLCertificates/#post-cluster-install","title":"Post Cluster Install","text":"<p>If you already have a cluster up and running, you can run the build.sh post flag to install your certs.  Because this is explicit call to install certs, it will precheck to see all of your certs are there and correct location.  If not it will give you error and not continue. You can run as post cluster install step.</p> <pre><code>/data/daffy/ocp/build.sh &lt;ENVIRONMENT_NAME&gt; --updateIngressCerts\n</code></pre>"},{"location":"Security/SSLCertificates/#cp4ba","title":"CP4BA","text":"<p>There are two main ingress access points for CP4BA.</p> <ul> <li>cp-console(icp-management-ingress)</li> <li>cpd(ibm-nginx-svc).</li> </ul> <p>By default, these are setup with self-signed certs. If you want to replace them with trusted certs from any major CA, this process will allow that.</p> <p>For ingress there is a base urls.  </p> <ul> <li>*.apps.${CLUSTER_NAME}.${BASE_DOMAIN}</li> </ul>"},{"location":"Security/SSLCertificates/#assumptions_1","title":"Assumptions","text":"<ol> <li>Must have valid Certificate in PEM format</li> <li>Must have private key from the cert</li> <li>Must have trust chain cert form the CA that issued the cert</li> <li>The apps Ingress cert must have wild card Common Name  *.apps.${CLUSTER_NAME}.${BASE_DOMAIN}</li> </ol>"},{"location":"Security/SSLCertificates/#location-of-certs_1","title":"Location of certs","text":"<ol> <li> <p>CP4BA ingress</p> <p>When the daffy process runs, it will look in the certs folder for your certs.  If it finds all three certs, it will apply them to the cluster.</p> <p>/data/daffy/certs/${CLUSTER_NAME}/apps.${CLUSTER_NAME}.${BASE_DOMAIN}.crt</p> <p>/data/daffy/certs/${CLUSTER_NAME}/apps.${CLUSTER_NAME}.${BASE_DOMAIN}.key</p> <p>/data/daffy/certs/${CLUSTER_NAME}/${OCP_TRUSTE_CA_FILE}.pem</p> </li> </ol>"},{"location":"Security/SSLCertificates/#environment-variables_1","title":"Environment variables","text":"Variable Name Info Default value Required OCP_TRUSTE_CA_NAME The display name of your CA Cert in OpenShift lets-encrypt No OCP_TRUSTE_CA_FILE The file name of your CA Cert lets-encrypt.pem No"},{"location":"Security/SSLCertificates/#execution_1","title":"Execution","text":""},{"location":"Security/SSLCertificates/#post-cp4ba-install-time","title":"Post CP4BA Install Time","text":"<p>For you to run this, the cloud pak must be up and running.  This process can only updated existing pods and secrets.</p> <pre><code>/data/daffy/cp4ba/build.sh &lt;ENVIRONMENT_NAME&gt; --updateIngressCerts\n</code></pre>"},{"location":"Security/SSLCertificates/#useful-links","title":"Useful Links","text":"<ul> <li>PEM Certificate</li> <li>OpenShift Ingress Api</li> <li>OpenShift Ingress Apps</li> <li>Common Services Ingress</li> <li>CP4BA Services Ingress</li> </ul>"},{"location":"Supporting-Software/","title":"Index","text":"Supporting software  Instructions on other software you can install Bastion IBM DB2 IBM LDAP Turbonomics Instana"},{"location":"Supporting-Software/Bastion/","title":"Bastion","text":""},{"location":"Supporting-Software/Bastion/#what-is-a-bastion-server","title":"What is a bastion server?","text":"<p>A bastion host is a server whose purpose is to provide access to a private network from an external network, such as the Internet. Because of its exposure to potential attack, a bastion host must minimize the chances of penetration. Openshift uses a bastion to help create a running cluster. A bastion can be reused for multiple clusters. In some scenarios for POC purposes such as User Provisioned Infrastructure (UPI), the bastion can be used as the proxy to the cluster.</p> <p>Bastion servers can be installed anywhere. This guide assumes the bastion server is Ubuntu 22.04 Minimal and will be in the IBM Cloud.</p> <p>Requirements for completing this task is to have an IBMid, an IBM cloud account, and a local SSH key. For more information, go to Daffy Prerequisites.</p> <p>Important</p> <p>Regardless of where your bastion is hosted, if it is accessable on the public internet it is HIGHLY recommended that you disable root user password authentication and ONLY allow SSH Key login. Please refer to the instructions here for more information on how to do that.</p> <p>Detailed below are the instructions to build your own bastion to do an IPI or MSP install.</p>"},{"location":"Supporting-Software/Bastion/#ibm-cloud","title":"IBM Cloud","text":"<ol> <li> <p>First, open a web browser and go to http://cloud.ibm.com</p> </li> <li> <p>Enter your id and click 'Continue'</p> </li> </ol> <p></p> <ol> <li>Once logged in, select 'Catalog' in the top menu bar</li> </ol> <p></p> <ol> <li>Once the Catalog is loaded, select 'Compute' or search the catalog for Virtual Servers. Select 'Virtual Server for Classic'</li> </ol> <p></p> <p>Alternative: Skip step 3 and search for \"virtual server\", choosing \"virtual server for classic\". Both options achieve the same thing.  </p> <p></p> <ol> <li>Fill out the details. (Public, hostname can be anything, and so can domain \u2013 feel free to leave what is there originally for your domain). Choose your billing method based on needs to be either Hourly or Monthly (~$40/mo.) and choose a Location.</li> </ol> <p></p> <ol> <li>Scroll down and fill out the remainder of the information. Choose a server type and select your SSH key so you can login. Finally, make sure you have the Ubuntu 20.04 Operating System selected.</li> </ol> <p>Note: You can use any other available tool to create a key if needed</p> <p></p> <ol> <li>Click the agreement on the right-hand pane and select 'Create'</li> </ol> <p></p> <ol> <li>This will take you to a device page. You can search for your bastion you have created. Once your server is done provisioning and has a start date, you can login to it via Termius using the Public IP address.</li> </ol> <p>**If connecting to a VPN to connect to the network, you will use the Private IP address, but Public will be used more frequently.</p> <p></p> <ol> <li>Create a new host in Termius: use your SSH key as the password, use root as the username, input the Public IP Address from the device list as your host's address, and create a label.</li> </ol> <p></p> <p>**If you don't use a SSH Key, you can go into the details of the bastion you created by double clicking on it and going to the passwords section. This password will not show until provisioning is complete.</p> <ol> <li>Once you have connected your bastion to Termius, install Daffy to the terminal of your newly created host.</li> </ol>"},{"location":"Supporting-Software/Bastion/#ibm-technology-zone","title":"IBM Technology Zone","text":"<p>An alternative to creating a bastion using a paid IBM Cloud account is to use IBM Technology Zone. There are three options with TechZone</p> <p>Warning</p> <p>For internal IBM or Business partner use only</p> <p>1.Find one on IBM Technology Zone: https://techzone.ibm.com/</p> <p></p> <ol> <li>From there, complete your reservation. Make sure to fill out items 1 \u2013 4, leaving the other fields blank.</li> </ol> <p></p>"},{"location":"Supporting-Software/DB2/","title":"DB2","text":"<p>With the Daffy system you can now install DB2 on a Linux server - RHEL 8.x. You will need to size the server based on your needs for the Database.   Below are the steps you would need to get the software, install it and check on the status of the install.</p>"},{"location":"Supporting-Software/DB2/#obtain-software","title":"Obtain Software","text":"<p>Before you install DB2, you will need to download the DB2 Software and place the binary's on your Linux server.</p> <p>1)   Customer can download the software via the Passport Advantage site.</p> <p>2)  Tech Sellers can download it from IBM Internal DSW Downloads.</p> <p>You will need to search for the part number CC1U0ML, this is the DB2 Software that will be used during the install.</p> <p>Once you get the software, you will need to upload it to the Linux server where you plan to install it.</p> <p>By default, it will need to be in the following location:</p> <p>/data/software/db2</p> <p>You can override this base location (/data/software) by overriding it in your environment file.</p> <p>SOFTWARE_INSTALLS_DIR=</p>"},{"location":"Supporting-Software/DB2/#install-db2","title":"Install DB2","text":"<p>Installing DB2 needs one new values to your environment file (&lt;ENVIRONMENT_NAME&gt;-env.sh).</p> <p>DB2_VERSION</p> <p>With this value, the daffy engine will be able to install the version of DB2.</p> <p>Valid Options:</p> <p>DB2_VERSION: 11.5</p> <pre><code>DB2_VERSION=\"11.5\"\n</code></pre> <p>Once you have the requires values in your environment file and the sofware was placed on the Linux server, you are ready to kick off the install.  Below is the command to kick off the install.  The process should take approx 10 min to install.</p> <pre><code>/data/daffy/db2/build.sh &lt;ENVIRONMENT_NAME&gt;\n</code></pre>"},{"location":"Supporting-Software/DB2/#help","title":"Help","text":"<p>There are a few options that the Daffy DB2 installer has to help with install and post install.  To see the options you can run the following command:</p> <p><pre><code>/data/daffy/db2/build.sh &lt;ENVIRONMENT_NAME&gt; --help\n</code></pre> If you need to test is you have correct info to install DB2 on this host,  You can run the following command</p> <p><pre><code>/data/daffy/db2/build.sh &lt;ENVIRONMENT_NAME&gt; --precheck\n</code></pre> If you need to display connect info for DB2, user name password port, etc.  You can run the following command <pre><code>/data/daffy/db2/build.sh &lt;ENVIRONMENT_NAME&gt; --console\n</code></pre></p>"},{"location":"Supporting-Software/IBM-LDAP/","title":"IBM LDAP","text":"<p>With the Daffy system you can now install IBM LDAP server on a Linux server.  IDS only supports RHEL 8.x. You will need to size the server based on your needs for the Database.   Below are the steps you would need to get the software, install it and check on the status of the install.</p> <p>Important</p> <p>To install LDAP, you need to have DB2 installed locally. Instructions here</p>"},{"location":"Supporting-Software/IBM-LDAP/#obtain-software","title":"Obtain Software","text":"<p>Before you install IBM LDAP you will need to download the IBM LDAP Software and place the binary's on your Linux server.</p> <p>1)   Customer can download the software via the Passport Advantage site.</p> <p>2)  Tech Sellers can download it from IBM Internal DSW Downloads.</p> <p>You will need to search for two part numbers CN487ML and CN4VJML , this is the IBM LDAP Software that will be used during the install.</p> <p>Once you get the software, you will need to upload it to the Linux server where you plan to install it.</p> <p>By default, it will need to be in the following location:</p> <p>/data/software/ldap</p> <p>You can override this base location (/data/software) by overriding it in your environment file.</p> <p>SOFTWARE_INSTALLS_DIR=</p>"},{"location":"Supporting-Software/IBM-LDAP/#install-ldap","title":"Install LDAP","text":"<p>Installing LDAP needs one new values to your environment file (&lt;ENVIRONMENT_NAME&gt;-env.sh).</p> <p>IDS_VERSION=</p> <p>With this value, the daffy engine will be able to install the version of IDS(LDAP).</p> <p>** The IBM LDAP Software is only supported on IBM RHEL.</p> <p>Valid Options:</p> <p>IDS_VERSION              </p> <p>6.4 <pre><code>IDS_VERSION=\"6.4\"\n</code></pre> <pre><code>/data/daffy/ldap/build.sh &lt;ENVIRONMENT_NAME&gt;\n</code></pre></p>"},{"location":"Supporting-Software/IBM-LDAP/#help","title":"Help","text":"<p>There are a few options that the Daffy IBM LDAP installer has to help with install and post install.  To see the options you can run the following command:</p> <p><pre><code>/data/daffy/ldap/build.sh &lt;ENVIRONMENT_NAME&gt; --help\n</code></pre> If you need to test is you have correct info to install IBM LDAP on this host,  You can run the following command</p> <pre><code>/data/daffy/ldap/build.sh &lt;ENVIRONMENT_NAME&gt; --precheck\n</code></pre> <p>If you need to display connect info for IBM LDAP , user name password port, etc.  You can run the following command</p> <pre><code>/data/daffy/ldap/build.sh &lt;ENVIRONMENT_NAME&gt; --console\n</code></pre>"},{"location":"Supporting-Software/Instana/","title":"Instana","text":"<p>With the Daffy scripts you can now install Instana monitoring. The scripts support installing both the Instana backend (server) and the Instana agent on OpenShift clusters.</p> <p>Attention</p> <p>When deploying OpenShift, you may enable the Instana Agent for monitoring. This is assuming you already have a remote Instana Platform Server up and the monitoring agent URL is ready to accept monitoring data.</p>"},{"location":"Supporting-Software/Instana/#obtain-key","title":"Obtain Key","text":"<p>Attention</p> <p>You will be required to provide an Instana license key to fully enable the  Instana monitoring platform (backend). The Instana monitoring agent does not require a license key for installation.</p> <p>Documentation on how to obtain a license key can be found here</p> <p>Seismic Link to how to obtain a Instana License Key</p> <p>If you are looking to gain a license key for Customer Evaluation, you will need access to the Portal. To gain access to the portal you need to request access via AccessHub. Below is documentation on how to request access to the Instana License Portal on AccessHub.</p> <p>AccessHub Access Hub Documentation</p>"},{"location":"Supporting-Software/Instana/#install","title":"Install","text":"<p>INSTANA BACKEND SERVER PLATFORM - REQUIRED ENVIRONMENT VARIABLES</p> <p>To install the backend platform you must have this variable set in your env file.</p> <pre><code>INSTANA_SERVER_INSTALL=true\n</code></pre> <p>INSTANA BACKEND SERVER PLATFORM - OPTIONAL ENVIRONMENT VARIABLES</p> <p>The version of the instana platform will default to the latest version supported by the daffy scripts. You may choose to set it to a specific version, but it must be a version that is supported by the Daffy installer.</p> <p>Currently the supported versions for the Instana Backend Platform is: (245-[1-3])</p> <pre><code>INSTANA_VERSION=245-1\n</code></pre> <p>INSTANA MONITORING - REQUIRED ENVIRONMENT VARIABLES</p> <p>To install the Instana monitoring agent for your OpenShift cluster, you must have this variable in your env file.</p> <pre><code>INSTANA_MONITORING=true\n</code></pre> <p>INSTANA MONITORING - OPTIONAL ENVIRONMENT VARIABLES</p> <p>If you are installing the Instana monitoring agent on the same OpenShift cluster as the Instana platform server, you can use this variable. If you are installing just the Instana agent and sending monitoring data to a remote Instna Platform, this variable must be set to the remote Instna Server Agent URL.</p> <pre><code>INSTANA_AGENT_URL=\"agent.instana.apps.${CLUSTER_NAME}.${BASE_DOMAIN}\"\n</code></pre>"},{"location":"Supporting-Software/Instana/#executing-scripts","title":"Executing Scripts","text":"Instana Platform BackendInstana Agent Deploy Instana Platform Backend Server<pre><code>/data/daffy/instana/build.sh &lt;env-prefix&gt;\n</code></pre> Deploy Instana Monitoring Agent<pre><code>/data/daffy/instana/service.sh &lt;env-prefix&gt;\n</code></pre>"},{"location":"Supporting-Software/Instana/#help","title":"Help","text":"<p>For assistance, please reach out to the slack channel #daffy-user-group.</p> <p>Note</p> <p>Your question may have already been answered, please do a search in this channel before you post a question.</p>"},{"location":"Supporting-Software/Turbonomics/","title":"Turbonomics","text":""},{"location":"Supporting-Software/Turbonomics/#turbonomic","title":"Turbonomic","text":"<p>Attention</p> <p>You will be required to provide a license key to fully enable the   Turbonomic Platform. When the platform installation is complete, you will   be presented with a URL where you will need to configure the administrator  password and provide a valid Turbonomic license key.</p> <p></p> <p>Info</p> <p>The default admin username for the Turbonomic Platform is: administrator and you will have the ability to set the password when you configure the server after install.</p> <p>KubeTurbo is the Turbo Metric Collector designed to send metrics and usage data from the Kubernetes environment to a Turbonomic Platform instance. Installing KubeTurbo will require you to provide the Turbonomic Platform topology processor URL and the administrator password.</p>"},{"location":"Supporting-Software/Turbonomics/#turbonomic-platform","title":"Turbonomic Platform","text":"<p>Turbonomic is an Application Resource Management (ARM) tool used to identify and automate critical actions that proactively deliver the most efficient use of compute, storage and network resources to your apps at every layer of the stack. Continuously, in real time and without human intervention.</p> <p>REQUIRED ENVIRONMENT VARIABLES</p> <pre><code>TURBO_PLATFORM_VERSION=\"8.9.2\"\n</code></pre>"},{"location":"Supporting-Software/Turbonomics/#kubeturbo","title":"Kubeturbo","text":"<p>Kubeturbo is an optional service that you can deploy to OpenShift. This will collect metrics from the OpenShift environment and send it to the Turbo Platform for display in the dashboard. You may configure the Kubeturbo to collect metrics and send data to an external Turbo Plaftorm, by overriding the values below.</p> <p>OPTIONAL ENVIRONMENT VARIABLES</p> <p>Info</p> <p>TURBO_PLATFORM_URL will be the \"topology-processor\" OpenShift ROUTE URL     (If using the Turbo Platform Deployed by Daffy). If the Turbo Platform was deployed without DAFFY, the URL may be the nginx endpoint.</p> <pre><code>TURBO_PLATFORM_URL=\"https://topology-processor-turbo.apps.yourdomain.net\"\n</code></pre> <p>Info</p> <p>TURBO_PLATFORM_USERNAME Default value is: \"administrator\"</p> <pre><code>TURBO_PLATFORM_USERNAME=\"administrator\"\n</code></pre> <p>Info</p> <p>TURBO_KUBE_CLUSTER_NAME is the label of your cluster. This will be defaulted to the cluster name you specified in the env file.</p> <pre><code>TURBO_KUBE_CLUSTER_NAME=\"gamma03\"\n</code></pre>"},{"location":"Supporting-Software/Turbonomics/#executing-scripts","title":"Executing Scripts","text":"Turbo PlatformKubeTurbo Deploy Turbo Platform<pre><code>/data/daffy/turbo/build.sh &lt;env-prefix&gt;\n</code></pre> Deploy KubeTurbo<pre><code>/data/daffy/turbo/service.sh &lt;env-prefix&gt;\n</code></pre>"},{"location":"Team-Notes/","title":"Index","text":"<p>MKDOCS Install &amp; Configuration</p> <p>This site uses MKDocs for the publishing of the \"GitHub Pages\" site</p> <p>You MUST install both MKDocs and the Material Theme.</p> <p>Here is how you setup your machine to edit the documentation hosted on the Github Pages Site. Details of these steps are below.</p> <p>Outline of the steps you will take</p> <ol> <li>Install MKdocs</li> <li>Install the MKdocs Theme (Material)</li> <li>Get a Personal Access Token from GitHub</li> <li>Clone this repository</li> <li>Make your changes (Feel free to reach out to Dave Krier or Kyle Dawson for help on how to use MKDocs)</li> <li>Deploy your changes</li> </ol>"},{"location":"Team-Notes/#install-mkdocs","title":"Install MKDocs","text":"<p>Here is the installation documentation for MKDocs</p> <p>Install the MKDocs theme Install guidance is on the Material theme page.</p> <p>Material Theme - Quick Start</p>"},{"location":"Team-Notes/#material-theme-documentation","title":"Material Theme Documentation","text":"<pre><code>pip3 install mkdocs-material\n</code></pre> <p>You can check the version of Material you currently have installed with this command.</p> <pre><code>python3 -m pip list\n</code></pre> <p>If you have any issues with the pip install, you may need to perform an uninstall and reinstall.</p> <pre><code>pip3 uninstall  mkdocs-material\npip3 install  mkdocs-material\n\npip3 uninstall mkdocs\npip3 install mkdocs\n</code></pre> <p>Link to the Material Theme Documentation</p>"},{"location":"Team-Notes/#you-need-a-personal-access-token-from-github-this-is-super-simple","title":"You need a \"Personal Access Token\" from Github. (This is super simple).","text":"<p>Why do I need this? You need this because you need to be able to push changes back into the github repository.</p> <p>Here is a Github doc page you can look at for help if the steps below do not make sense.</p> <ol> <li>Login to Github.</li> <li>Click on your user icon in the upper right hand corner</li> <li>Select Settings --&gt; Developer Settings --&gt; Personal Access Tokens</li> <li>Generate a new token with only these prevliages</li> <li>Repo:Status</li> <li>Repo-Deployment</li> <li>Public-Repo</li> <li>Take note of the access key (It's the only time you will see it)</li> </ol> <p>Important</p> <p>You will use this Access Token as your password when using the gh-deploy mkdocs command.</p>"},{"location":"Team-Notes/#clone-this-repo-in-github","title":"Clone this repo in Github.","text":"<p>I assume you will all know how to do that!!</p>"},{"location":"Team-Notes/#mkdocs-user-guide-modifying-the-site","title":"MKDocs User Guide - Modifying the site","text":"<p>MKDocs Documentation</p> <p>Deploying your changes.</p> <ol> <li>Navigate to your cloned repo directory in a terminal window.</li> </ol> <p>You must be inside the /daffydoc directory to run the following commands.</p> <ol> <li>Build the files using the build command</li> </ol> <pre><code>mkdocs build\n</code></pre> <ol> <li>Deploy the newly built files to the Github Pages site</li> </ol> <p>Note</p> <p>This is where you will use the access token from Github. Use the access    token instead of your password.</p> <pre><code>mkdocs gh-deploy\n</code></pre>"},{"location":"Team-Notes/#update-mkdocs","title":"Update mkdocs","text":"<pre><code>pip3 install -U mkdocs\n</code></pre>"},{"location":"Tips-and-Tricks/Common-Commands/","title":"Common Commands","text":"<p>This page lists some of the most commonly used Daffy commands for installation, troubleshooting, and cleanup.</p>"},{"location":"Tips-and-Tricks/Common-Commands/#precheck-environment","title":"Precheck Environment","text":"<p>By adding --precheck to any of the following build/service commands in daffy. It will run all prechecks for that step and then exit without preforming the full process</p> <p>Info</p> <p>This will not work on the all-in-one command /data/daffy/build.sh  --precheck <p>Precheck OpenShift Build <pre><code>/data/daffy/ocp/build.sh &lt;env-name&gt; --precheck\n</code></pre> And will work on all Cloud Pak commands as well:</p> <p>Precheck CP4D Build <pre><code>/data/daffy/cp4d/build.sh &lt;env-name&gt; --precheck\n</code></pre> Precheck CP4D Service <pre><code>/data/daffy/cp4d/service.sh &lt;env-name&gt; --precheck\n</code></pre></p>"},{"location":"Tips-and-Tricks/Common-Commands/#delete-environment","title":"Delete Environment","text":"<p>Run the following command to delete all resources created by Daffy, including but not limited to the OpenShift cluster, Cloud Paks, services, VM systems, and tools.</p> <pre><code>/data/daffy/cleanup.sh &lt;env-name&gt;\n</code></pre>"},{"location":"Tips-and-Tricks/Common-Commands/#daffy-versioning","title":"Daffy Versioning","text":""},{"location":"Tips-and-Tricks/Common-Commands/#upgrade","title":"Upgrade","text":"<p>Run the following command to upgrade Daffy to the newest release.</p> <pre><code>cd /data ; /data/daffy/refresh.sh\n</code></pre>"},{"location":"Tips-and-Tricks/Common-Commands/#downgrade","title":"Downgrade","text":"<p>Run the following command to install older version of Daffy. This will output a list of previous releases to select and install.</p> <pre><code>cd /data ; /data/daffy/refresh.sh --list\n</code></pre> Example Output <p></p> <p>Warning</p> <p>If you already downgraded to an older version and want to install another older version, you must first upgrade to the latest release and then downgrade to the other version you want.</p>"},{"location":"Tips-and-Tricks/Common-Commands/#install-older","title":"Install Older","text":"<p>If you want to install an older version from the begining, you can with this command:</p> <pre><code>curl http://get.daffy-installer.com/download-scripts/daffy-init.sh | sudo -E bash -s -- v2023-01-11.tar\n</code></pre> Version name format <p>Just supply the name of the tar version you want to install.</p> <p>Format  v${YEAR}-${MONTH}-${DAY}.tar </p>"},{"location":"Tips-and-Tricks/Common-Commands/#get-version","title":"Get Version","text":"<p>Run the following command to view the current Daffy version.</p> <pre><code>/data/daffy/version.sh\n</code></pre>"},{"location":"Tips-and-Tricks/Common-Commands/#ocp-build-help","title":"OCP Build Help","text":"<p>The OCP build process has several flags, to see all the flags, you can run the --help</p> <pre><code>/data/daffy/ocp/build.sh &lt;env_name&gt; --help\n</code></pre> All Build Flags <pre><code>--precheck                            \"This will just do a quick precheck to see if the environment is ready for a build\"\n--tshirtSize                          \"This will display what the current TShirt size would be with this environment\"\n--createIBMCloudDNSEntries            \"This will create your Public DNS enries in IBM Cloud\"\n--approveVCenterCert                  \"This will download VCenter certs and add to host trust store.\"\n--removeIBMCloudDNSEntries            \"This will remove your Public DNS enries in IBM Cloud\"\n--displayOCPDNSRequirements           \"This will display what DNS records that need to be created\"\n--displayOCPLoadBalanceRules          \"This will display the Load Balnacer rules you will need\"\n--runOpenShiftInstallWaitBoot         \"This will run openshift-install (wait-for boot)\"\n--approveCSR                          \"This will approve all pending CSR request\"\n--runOpenShiftInstallWaitInstall      \"This will run openshift-install (wait-for install)\"\n--updateIngress                       \"This will get new IBM Cert and update the main ingress certs/secret\"\n--createOpenShiftContainerStorage     \"This will create OpenShift Container Storage\"\n--createNFSServer                     \"This will create local NFS Server\"\n--createNFSDisk                       \"This will create local disk for NFS Server\"\n--configureLocalStorge                \"This will create local stroage\"\n--createVMDashboard                   \"This will create the VM Dashboard Web UI\"\n--createImageRegistry                 \"This will create the OpenShift Image Registry\"\n--console                             \"This will display Admin Console Info\"\n--status                              \"This will display cluster info\"\n--installOpenShiftTools               \"This will install oc, kubectl and openshift-install tools\"\n--displayVSpherePermissionsNeeded     \"This will display all permissions needed for VSphere Install\"\n--validateReserverLookupDNS           \"This will validate PTR records for UPI installs\"\n--applyFix                            \"This will apply a given fix to your Daffy env, pass the fix number to command\"\n--deleteAllFailedPods                 \"This will delete all pods in Failed state.\"\n--MACinstallOCPTools                  \"This will install oc and kubectl tools locally on your Mac\"\n--installRHACM                        \"This will install Red Hat Advanced Cluster Management\"\n--CephStatus                          \"This will show the status of ceph using ceph tools inside of OpenShift Storage\"\n--createAdminAccount                  \"This will create local htaccess admin account\"\n--help|--?|?|-?|help|-help            \"This help menu\"\n</code></pre> <p>Examples</p> <p>To show OpenShift Console info, ID/Password and URL <pre><code>/data/daffy/ocp/build.sh &lt;env_name&gt; --console\n</code></pre> To show OpenShift status <pre><code>/data/daffy/ocp/build.sh &lt;env_name&gt; --status\n</code></pre> To show OpenShift Node sizes based on your environment file <pre><code>/data/daffy/ocp/build.sh &lt;env_name&gt; --tshirtSize\n</code></pre></p>"},{"location":"Tips-and-Tricks/Common-Commands/#odf-existing-cluster","title":"ODF Existing Cluster","text":"<p>If you have existing cluster, you can run the follow command to setup OpenShift Data Foundation on your existing cluster.</p> <pre><code>/data/daffy/ocp/build.sh &lt;env_name&gt; --createOpenShiftContainerStorage\n</code></pre>"},{"location":"Tips-and-Tricks/Common-Commands/#daffy-tools","title":"Daffy Tools","text":"<p>Run the following command to install other tools you might need.</p> <pre><code>/data/daffy/tools.sh\n</code></pre> Tools Flags <pre><code>Daffy Tools\n--prepareHost                          This will run the prepareHost for daffy\n--mustGather                           This will run the mustGather for daffy\n--installOC                            This will install the oc command line tool\n--installOpenShiftInstall              This will install the openshift-install command line tool\n--installAWS                           This will install the aws commandline tool\n--installAzure                         This will install the azure commandline tool\n--installGCloud                        This will install the gcloud commandline tool\n--installGOVC                          This will install the govc commandline tool(VMware)\n--installCloudCTL                      This will install the cloudctl for CP4D\n--installCP4DCloudCLI                  This will install the CP4D Cloud CLI utility\n--installCPDCTL                        This will install the Cloud Pak 4 Data (CPDCTL) utility\n--installCP4BACase                     This will install the Cloud Pak 4 Business Automation (case) utility\n</code></pre>"},{"location":"Tips-and-Tricks/Common-Commands/#gather-logs","title":"Gather Logs","text":"<p>Run the following command to package all Daffy logs into a tar file. When troubleshooting, please send the gathered logs at <code>/tmp/daffy/daffy_mustgather.tar.gz</code> to the Daffy team.</p> <pre><code>/data/daffy/tools.sh --mustGather\n</code></pre>"},{"location":"Tips-and-Tricks/Common-Commands/#security-cleanup","title":"Security Cleanup","text":"<p>Run the following command to clear all security (or individual) credentials.</p> <pre><code>/data/daffy/security-cleanup.sh\n</code></pre> Additional Cleanup Flags <pre><code>--all                            This will cleanup all security including RH Pull Secret, SSH key, and IBM Entitlement keys\n--aws                            This will cleanup the aws security information\n--azure                          This will cleanup the azure security information\n--gcp                            This will cleanup the gcp credential information\n--ibm                            This will cleanup the IBM Entitlement Key and ibmcloud sesssion info\n--vsphere                        This will cleanup vsphere information\n--pullSecret                     This will cleanup your RedHat Pull Secret\n--ssh                            This will cleanup your local ssh key\n--ibm-ipi                        This will cleanup IBM API Key for use with IBM-ipi\n--roks                           This will cleanup IBM ROKS keys and login information\n--turbo                          This will cleanup your local ssh key\n--help|--?|?|-?|help|-help       This help menu\n</code></pre>"},{"location":"Tips-and-Tricks/Common-Issues/","title":"Common Issues","text":""},{"location":"Tips-and-Tricks/Common-Issues/#common-issues-tips","title":"Common Issues &amp; Tips","text":"<p>If you run into issues with your install, please take the time to read the output of the script. In most cases, the output will give you a clear indication of what the problem is and in some cases how to fix it.</p> <p>When reading the output during an error, always read messages starting from the TOP of your console output.  Daffy will try to find as many errors as it can, so ALWAYS start at the TOP of your output.</p> <p></p> <p>Most cases, the issue is a simple typo or a missing variable in the environment file. Please double check your environment file for any errors.</p> <p></p> <p>Run the /data/daffy/refresh.sh script to download the latest version of the daffy scripts.</p> <p></p> <p>Run the security-cleanup.sh script to remove all the sensitive data and the next time you run, the scripts will prompt you for the necessary input.</p> <pre><code>/data/daffy/security-cleanup.sh\n</code></pre>"},{"location":"Tips-and-Tricks/Edit-Files/","title":"Edit Files","text":""},{"location":"Tips-and-Tricks/Edit-Files/#editing-files","title":"Editing Files","text":""},{"location":"Tips-and-Tricks/Edit-Files/#basic-vi-commands-you-will-use-to-edit-your-environment-file","title":"Basic vi commands you will use to edit your environment file:","text":"<p>Use the arrows on your keyboard to go to location you want to edit, you can not use your mouse.</p> <p>Esc    starts all modes and Esc  will end all modes. The Esc character is your friend!!!!!!</p> <p>Modes:</p> <p>Esc - X     = deletes one character where you cursor is. Once done Esc</p> <p>Esc - I      =  puts you in insert mode, just start typing what you want to add. Once done Esc</p> <p>Esc - D - D  = delete entire line cursor is on.  Once done Esc</p> <p>Esc - U     = will undo your last action.  Once done Esc</p> <p>Once done, the following will save your file:</p> <p>Esc - : - W - Q - Enter (lower case w and q)  </p> <p>If you messed up the file and you want to exit and not save your file:</p> <p>Esc -  : - Q - ! - Enter (lower case q)    </p> <p>VI Cheat-sheet -</p> <p>https://www.atmos.albany.edu/daes/atmclasses/atm350/vi_cheat_sheet.pdf</p> <p>Nano Cheat-sheet           https://itsfoss.com/nano-editor-guide/</p>"},{"location":"Tips-and-Tricks/Environment-File/","title":"Environment File","text":""},{"location":"Tips-and-Tricks/Environment-File/#your-environment-file-name-must-follow-simple-rules","title":"Your environment file name must follow simple rules:","text":"<ol> <li>All lower case no spaces</li> <li>Alphanumeric  or - (no special characters)</li> <li>Must end in -env.sh</li> <li> <p>Environment name is the prefix for the file name</p> <p>Example:</p> <p>/data/daffy/ocp/build.sh myenv    =====&gt;   /data/daffy/env/myenv-env.sh is the file that daffy would use</p> </li> <li> <p>The # at the beginning of the line will mark line as comment only</p> </li> <li>All Names in file must be UPPER_CASE</li> <li>All true/false values must be lower case</li> </ol>"},{"location":"Tips-and-Tricks/Upgrade/","title":"Daffy Upgrade","text":""},{"location":"Tips-and-Tricks/Upgrade/#updating-daffy","title":"Updating Daffy","text":"<p>There are two ways to update existing daffy. Both process will replace the code of daffy but leave your environment files as is.  It will require you to accept the daffy warranty.</p>"},{"location":"Tips-and-Tricks/Upgrade/#upgrade","title":"Upgrade","text":"<p>1) Run the Refresh script in the daffy home directory</p> <pre><code>/data/daffy/refresh.sh\n</code></pre> Screenshot <p></p>"},{"location":"Tips-and-Tricks/Upgrade/#downgrade","title":"Downgrade","text":"<p>2) If you want to install older version of daffy from existing installed daffy:</p> <p><pre><code>/data/daffy/refresh.sh --list\n</code></pre> This will give you a list of older version of daffy to downgrade to:</p> Screenshot <p></p> <p>Warning</p> <p>If you have downgrade to older version of daffy and want to install another older version, you need to first upgrade to the lasted version then downgrade the other version you want.</p>"},{"location":"Tips-and-Tricks/Upgrade/#install-older","title":"Install Older","text":"<p>If you want to install an older version from the begining, you can with this command:</p> <pre><code>curl http://get.daffy-installer.com/download-scripts/daffy-init.sh | sudo -E bash -s -- v2023-01-11.tar\n</code></pre> Version name format <p>Just supply the name of the tar version you want to install.</p> <p>Format  v${YEAR}-${MONTH}-${DAY}.tar </p>"},{"location":"Tips-and-Tricks/occommands/","title":"OC Commands","text":""},{"location":"Tips-and-Tricks/occommands/#get-login-token","title":"Get Login token","text":"<p>At certain parts of the install of cloud paks, you may need to get a login token and give to daffy.  This will show you how to do this.</p> <p>1) Login to our cluster UI</p> Screenshot <p></p> <p>2) Top right of your screen, select the drop down menu for your user</p> Screenshot <p></p> <p>3) From the menu, select the \"Copy login command\"</p> Screenshot <p></p> <p>4) This will open a new browser tab, click the \"Display Token\" link</p> Screenshot <p> </p> <p>5) It will now show the token command.  copy the oc login section only</p> Screenshot <p> </p> <p>6) Once you copied the oc command, go back to your bastion and past it into terminal console. You will see a successful login after the command.</p> Screenshot <p> </p>"},{"location":"Tips-and-Tricks/occommands/#get-cluster-name","title":"Get Cluster Name","text":"<p>Sometimes you will use existing cluster for daffy, but daffy still needs to match your cluster name in the environment file with your runtime. There is an oc command that will display your cluster name.</p> <pre><code>oc describe infrastructure/cluster | grep \"Infrastructure Name\"\n</code></pre>"},{"location":"Tips-and-Tricks/occommands/#get-worker-nodes","title":"Get Worker Nodes","text":"<p>Daffy will require your environment file to match your runtime environment. One area it checks is the number of worker nodes you have.  They must match.  Here is a command you can run to get the your worker nodes for your running cluster <pre><code>oc get nodes | grep \"worker\"\n</code></pre></p>"},{"location":"Tips-and-Tricks/occommands/#other-helpful-sites","title":"Other Helpful Sites","text":"<p>OpenShif Tips Site</p>"},{"location":"overrides/","title":"Index","text":"<p>Warning</p> <p>There is less error checking on overrides that you add, so make sure you understand the override you are adding and its valid.</p> <p>Daffy allows you to override many settings.  Most common installs do not need to do any overrides.</p> <p>Overrides are simple, just add the override to your own Environment file with the value you want to override.</p>"},{"location":"overrides/daffy/","title":"Daffy","text":""},{"location":"overrides/daffy/#general","title":"General","text":""},{"location":"overrides/daffy/#debug","title":"DEBUG","text":"<p>If set to true, this will tell the daffy system to stop at the beginning of each major section of the process.  It will allow you to view each step before it executes.  You will have to hit any key on your keyboard to continue each step.  If not set in your environment file or set to any other value the true, it will disable debug. This is great option for your first install to slowly go thru each step to understand what the daffy process is doing.</p> <p>Info</p> <p>Must be all lower case true</p> Variable Name Default Value Valid Options DEBUG false true or false <pre><code>DEBUG=\"true\"\n</code></pre>"},{"location":"overrides/ocp/","title":"OpenShift","text":""},{"location":"overrides/ocp/#worker-node-sizing","title":"Worker Node Sizing","text":"<p>Default values are based on TShirt Sizing.</p> <p>To override the default number of workers, just add the value you want to build in your environment file.</p> Variable Name Install Type Default Value Valid Options VM_NUMBER_OF_WORKERS_MIN VM_TSHIRT_SIZE=\"Min\" 3 any Number VM_NUMBER_OF_WORKERS_LARGE VM_TSHIRT_SIZE=\"Large\" 3(+3 if ODF true) any Number <pre><code>#VM_NUMBER_OF_WORKERS_MIN=\"4\"\nVM_NUMBER_OF_WORKERS_LARGE=\"8\"\n</code></pre>"},{"location":"overrides/ocp/#master-node-sizing","title":"Master Node Sizing","text":"<p>Default values are based on TShirt Sizing.</p> <p>To override the default number of masters, just add the value you want to build in your environment file.</p> Variable Name Install Type Default Value Valid Options VM_NUMBER_OF_MASTERS_MIN VM_TSHIRT_SIZE=\"Min\" 3 any Number VM_NUMBER_OF_MASTERS_LARGE VM_TSHIRT_SIZE=\"Large\" 3 any Number <pre><code>#VM_NUMBER_OF_MASTERS_MIN=\"1\"\nVM_NUMBER_OF_MASTERS_LARGE=\"4\"\n</code></pre>"},{"location":"overrides/ocp/#masters-schedulable","title":"Masters Schedulable","text":"<p>This will tell the cluster if you want the master nodes to preform workload.  Should it be considered part of the other worker nodes.</p> Variable Name Default Value Valid Options OCP_MASTER_NODES_SCHEDULABLE false true/false <pre><code>OCP_MASTER_NODES_SCHEDULABLE=\"true\"\n</code></pre>"},{"location":"overrides/ocp/#tshirt-sizing","title":"TShirt Sizing","text":"<p>If you want to see what the current TShirt sizing is or what values you can override in your env file, just run the following command. It will show you current values and the names you can override in your file.</p> <pre><code>/data/daffy/ocp/build.sh &lt;env_name&gt; --tshirtSize\n</code></pre> <p>Example of GCP:</p> <pre><code>Current T-Shirt Sizing Info\n################################################################\nSetting VM T-Shirt Size to Large\nGoogle Cloud Platform:\nBootstrap Node type = n1-standard-2\nGCP_MACHINE_TYPE_BOOTSTRAP_CPU_LARGE=\"2\"\n\nMaster Nodes:\n--------------------------------------------------\nGCP_MACHINE_TYPE_MASTER_LARGE=\"n1-standard-8\"\nVM_NUMBER_OF_MASTERS_LARGE=\"3\"\nGCP_MACHINE_TYPE_MASTER_CPU_LARGE=\"8\"\n\nWorker Nodes:\n--------------------------------------------------\nGCP_MACHINE_TYPE_WORKER_LARGE=\"n1-standard-16\"\nVM_NUMBER_OF_WORKERS_LARGE=\"6\"\nGCP_MACHINE_TYPE_WORKER_CPU_LARGE=\"16\"\n\nOpenShift Storage Cluster Storage(Block):\n--------------------------------------------------\nVM_WORKER_DISK2_LARGE=\"50G\"\n\nOpenShift Storage Cluster Storage(Block):\n--------------------------------------------------\nVM_WORKER_DISK3_LARGE=\"200G\"\n</code></pre>"},{"location":"overrides/cloudpaks/","title":"Index","text":""},{"location":"overrides/cloudpaks/#cloud-pak-overrides","title":"Cloud Pak Overrides","text":"<p>In progress !!!!!</p>"},{"location":"overrides/cloudpaks/cp4ba/","title":"Business Automation","text":""},{"location":"overrides/cloudpaks/cp4ba/#cp4ba-overrides","title":"CP4BA Overrides","text":""},{"location":"overrides/cloudpaks/cp4ba/#storage","title":"Storage","text":"<p>Based on the platform you delopy to, a default storge will be picked. If you want to override any, just add to your environment file based on names below and new stroage class name that is in the cluster. </p> <p>There are 6 storage class variables that are set:</p> Variable Name Storage Type Info CP4BA_AUTO_STORAGE_CLASS_OCP_BLOCK Block Storage This is block storage, NFS is not block storage CP4BA_AUTO_STORAGE_CLASS_FAST_ROKS File Storage CP4BA_AUTO_STORAGE_CLASS_OCP File Storage CP4BA_AUTO_STORAGE_CLASS_OCP_SLOW File Storage CP4BA_AUTO_STORAGE_CLASS_OCP_MEDIUM File Storage CP4BA_AUTO_STORAGE_CLASS_OCP_FAST File Storage"},{"location":"overrides/cloudpaks/cp4ba/#roks","title":"ROKS","text":"<pre><code>CP4BA_AUTO_STORAGE_CLASS_FAST_ROKS=\"ibmc-file-gold-gid\"\nCP4BA_AUTO_STORAGE_CLASS_OCP=\"ibmc-file-gold-gid\"\nCP4BA_AUTO_STORAGE_CLASS_OCP_BLOCK=\"ibmc-block-gold\"\nCP4BA_AUTO_STORAGE_CLASS_OCP_SLOW=\"ibmc-file-bronze-gid\"\nCP4BA_AUTO_STORAGE_CLASS_OCP_MEDIUM=\"ibmc-file-silver-gid\"\nCP4BA_AUTO_STORAGE_CLASS_OCP_FAST=\"ibmc-file-gold-gid\"\n</code></pre>"},{"location":"overrides/cloudpaks/cp4ba/#roks-vpc2","title":"ROKS VPC2","text":"<pre><code>CP4BA_AUTO_STORAGE_CLASS_FAST_ROKS=\"ocs-storagecluster-cephfs\"\nCP4BA_AUTO_STORAGE_CLASS_OCP=\"ocs-storagecluster-cephfs\"\nCP4BA_AUTO_STORAGE_CLASS_OCP_BLOCK=\"ocs-storagecluster-ceph-rbd\"\nCP4BA_AUTO_STORAGE_CLASS_OCP_SLOW=\"ocs-storagecluster-cephfs\"\nCP4BA_AUTO_STORAGE_CLASS_OCP_MEDIUM=\"ocs-storagecluster-cephfs\"\nCP4BA_AUTO_STORAGE_CLASS_OCP_FAST=\"ocs-storagecluster-cephfs\"\n</code></pre>"},{"location":"overrides/cloudpaks/cp4ba/#rosa","title":"ROSA","text":"<pre><code>CP4BA_AUTO_PLATFORM=OCP\nCP4BA_AUTO_STORAGE_CLASS_FAST_ROKS=\"efs-nfs-client\"\nCP4BA_AUTO_STORAGE_CLASS_OCP=\"efs-nfs-client\"\nCP4BA_AUTO_STORAGE_CLASS_OCP_BLOCK=\"gp3\"\nCP4BA_AUTO_STORAGE_CLASS_OCP_SLOW=\"efs-nfs-client\"\nCP4BA_AUTO_STORAGE_CLASS_OCP_MEDIUM=\"efs-nfs-client\"\nCP4BA_AUTO_STORAGE_CLASS_OCP_FAST=\"efs-nfs-client\"\n</code></pre>"},{"location":"overrides/cloudpaks/cp4ba/#all-others","title":"All Others","text":"<pre><code>CP4BA_AUTO_STORAGE_CLASS_OCP=\"ocs-storagecluster-cephfs\"\nCP4BA_AUTO_STORAGE_CLASS_OCP_BLOCK=\"ocs-storagecluster-ceph-rbd\"\nCP4BA_AUTO_STORAGE_CLASS_OCP_SLOW=\"ocs-storagecluster-cephfs\"\nCP4BA_AUTO_STORAGE_CLASS_OCP_MEDIUM=\"ocs-storagecluster-cephfs\"\nCP4BA_AUTO_STORAGE_CLASS_OCP_FAST=\"ocs-storagecluster-cephfs\"\n</code></pre>"},{"location":"overrides/cloudpaks/cp4ba/#namespace","title":"Namespace","text":"<p>A default namespace will be used, but if you want to override it, just update the following variable based on your deployment type and your new name in your environment file</p>"},{"location":"overrides/cloudpaks/cp4ba/#starter","title":"Starter","text":"<pre><code>CP4BA_AUTO_NAMESPACE_STARTER=\"cp4ba-starter\"\n</code></pre>"},{"location":"overrides/cloudpaks/cp4ba/#production","title":"Production","text":"<pre><code>CP4BA_DEPLOYMENT_PRODUCTION_DECISIONS_NAMESPACE=\"cp4ba-decisions\"\nCP4BA_DEPLOYMENT_PRODUCTION_CONTENT_NAMESPACE=\"cp4ba-content\"\nCP4BA_DEPLOYMENT_PRODUCTION_WORKFLOW_RUNTIME_NAMESPACE=\"cp4ba-workflow-runtime\"\n</code></pre>"},{"location":"overrides/cloudpaks/cp4ba/#license","title":"License","text":"<p>By default, daffy will use a non-production license value during deployment. If you wish to change this, add the following varaible and update based on need to your environment file.</p> Variable Name Options CP4BA_DEPLOYMENT_LICENSE non-production or production <pre><code>CP4BA_DEPLOYMENT_LICENSE=\"non-production\"\n</code></pre>"},{"location":"overrides/cloudpaks/cp4ba/#odm-production","title":"ODM Production","text":""},{"location":"overrides/cloudpaks/cp4ba/#ldap-overrides","title":"LDAP Overrides","text":"<p>If you have existing LDAP Server and want to integrate with the ODM production deployment, below are the dafault values you can override to point to a new LDAP Server.  Just add variable name and new value to your environment file. <pre><code>CP4BA_DEPLOYMENT_PRODUCTION_DECISIONS_LDAP_OU=\"${PROJECT_NAME}\"\nCP4BA_DEPLOYMENT_PRODUCTION_DECISIONS_LDAP_TYPE=\"IBM Security Directory Server\"\nCP4BA_DEPLOYMENT_PRODUCTION_DECISIONS_LDAP_SERVER=\"\"\nCP4BA_DEPLOYMENT_PRODUCTION_DECISIONS_LDAP_PORT=\"389\"\nCP4BA_DEPLOYMENT_PRODUCTION_DECISIONS_ADMIN_USER=\"odmAdmin\"\nCP4BA_DEPLOYMENT_PRODUCTION_DECISIONS_LDAP_BIND_SECRET=\"ldap-bind-secret\"\nCP4BA_DEPLOYMENT_PRODUCTION_DECISIONS_LDAP_BASE_DN=\"ou=${PROJECT_NAME},ou=odm,dc=ibm,dc=com\"\nCP4BA_DEPLOYMENT_PRODUCTION_DECISIONS_LDAP_SSL_ENABLED=\"false\"\nCP4BA_DEPLOYMENT_PRODUCTION_DECISIONS_LDAP_SECRET_NAME=\"ldap-ssl-cert\"\nCP4BA_DEPLOYMENT_PRODUCTION_DECISIONS_LDAP_USER_NAME_ATTRIBUTE=\"*:uid\"\nCP4BA_DEPLOYMENT_PRODUCTION_DECISIONS_LDAP_USER_DISPLAY_NAME_ATTR=\"uid\"\nCP4BA_DEPLOYMENT_PRODUCTION_DECISIONS_LDAP_GROUP_BASE_DN=\"ou=Groups,ou=${PROJECT_NAME},ou=odm,dc=ibm,dc=com\"\nCP4BA_DEPLOYMENT_PRODUCTION_DECISIONS_LDAP_GROUP_NAME_ATTRIBUTE=\"*:cn\"\nCP4BA_DEPLOYMENT_PRODUCTION_DECISIONS_LDAP_GROUP_DISPLAY_NAME_ATTR=\"cn\"\nCP4BA_DEPLOYMENT_PRODUCTION_DECISIONS_LDAP_GROUP_MEMBERSHIP_SEARCH_FILTER=\"(|(\\&amp;(objectclass=groupofnames)(member={0}))(\\&amp;amp;(objectclass=groupofuniquenames)(uniquemember={0})))\"\nCP4BA_DEPLOYMENT_PRODUCTION_DECISIONS_LDAP_GROUP_MEMBER_ID_MAP=\"groupofnames:member\"\nCP4BA_DEPLOYMENT_PRODUCTION_DECISIONS_LDAP_TDS_LC_USER_FILTER=\"(\\&amp;amp;(cn=%v)(objectclass=person))\"\nCP4BA_DEPLOYMENT_PRODUCTION_DECISIONS_LDAP_TDS_LC_GROUP_FILTER=\"(\\&amp;amp;(cn=%v)(|(objectclass=groupofnames)(objectclass=groupofuniquenames)(objectclass=groupofurls)))\"\n</code></pre></p>"},{"location":"overrides/cloudpaks/cp4d/","title":"Data","text":""},{"location":"overrides/cloudpaks/cp4d/#cp4d-overrides","title":"CP4D Overrides","text":"<p>In progress !!!!!</p>"},{"location":"overrides/cloudpaks/cp4i/","title":"Integration","text":""},{"location":"overrides/cloudpaks/cp4i/#cp4i-overrides","title":"CP4I Overrides","text":"<p>In progress !!!!!</p>"},{"location":"overrides/cloudpaks/cp4waiops/","title":"Wations AIOps","text":""},{"location":"overrides/cloudpaks/cp4waiops/#cp4waiops-overrides","title":"CP4WAIOps Overrides","text":"<p>In progress !!!!!</p>"},{"location":"overrides/platforms/","title":"Index","text":""},{"location":"overrides/platforms/#platform-overrides","title":"Platform Overrides","text":"<p>In progress !!!!!</p>"},{"location":"overrides/platforms/aws/","title":"AWS","text":""},{"location":"overrides/platforms/aws/#aws-overrides","title":"AWS Overrides","text":"<p>In progress !!!!!</p>"},{"location":"overrides/platforms/azure/","title":"Azure","text":""},{"location":"overrides/platforms/azure/#azure-overrides","title":"Azure Overrides","text":"<p>In progress !!!!!</p>"},{"location":"overrides/platforms/gcp/","title":"Google","text":""},{"location":"overrides/platforms/gcp/#gcp-overrides","title":"GCP Overrides","text":"<p>In progress !!!!!</p>"},{"location":"overrides/platforms/ibm/","title":"IBM","text":""},{"location":"overrides/platforms/ibm/#ibm-overrides","title":"IBM Overrides","text":"<p>In progress !!!!!</p>"},{"location":"overrides/platforms/vsphere/","title":"VSphere","text":""},{"location":"overrides/platforms/vsphere/#vsphere-overrides","title":"VSphere Overrides","text":"<p>In progress !!!!!</p>"},{"location":"overrides/software/db2/","title":"DB2","text":""},{"location":"overrides/software/db2/#db2-overrides","title":"DB2 Overrides","text":"<p>In progress !!!!!</p>"},{"location":"overrides/software/ldap/","title":"LDAP","text":""},{"location":"overrides/software/ldap/#ldap-overrides","title":"LDAP Overrides","text":"<p>In progress !!!!!</p>"},{"location":"overrides/software/turbonomics/","title":"Turbonomics","text":""},{"location":"overrides/software/turbonomics/#turbonomics-overrides","title":"Turbonomics Overrides","text":"<p>In progress !!!!!</p>"}]}